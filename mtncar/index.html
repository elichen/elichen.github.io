<!DOCTYPE html>
<html>
<head>
    <title>Mountain Car</title>
    <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="src/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.11.0"></script>
</head>
<body>
    <div class="container">
        <div class="instructions">
            <h2>Mountain Car</h2>
            <p id="modeText">Watch a pre-trained AI agent solve this classic control problem!</p>
            <p style="font-size: 14px; margin-top: 5px;">Goal: Reach the flag on the right peak</p>
        </div>
        <canvas id="gameCanvas"></canvas>
        <div class="training-info">
            <h3>About This Model</h3>
            <p>
                This agent was trained using <strong>Deep Q-Network (DQN)</strong> with
                <a href="https://stable-baselines3.readthedocs.io/" target="_blank">Stable Baselines3</a>,
                a state-of-the-art reinforcement learning library in Python. The trained model was then converted
                to TensorFlow.js to run directly in your browser.
            </p>

            <h4>Training Details</h4>
            <ul>
                <li><strong>Algorithm:</strong> DQN (Deep Q-Network) with sparse rewards</li>
                <li><strong>Training Duration:</strong> 120,000 timesteps (~40 seconds on CPU)</li>
                <li><strong>Network Architecture:</strong> 2 inputs → [256, 256] hidden → 3 actions</li>
                <li><strong>Best Performance:</strong> Achieved at 60,000 steps (mean reward -148)</li>
                <li><strong>Episode Length:</strong> Solves in exactly 136 steps consistently</li>
                <li><strong>Success Rate:</strong> 100% after convergence</li>
            </ul>

            <h4>How It Learned</h4>
            <p>
                The agent learned through trial and error, discovering the optimal strategy through
                120,000 environment interactions. It receives a small penalty for each time step,
                incentivizing it to reach the goal quickly.
            </p>
            <p>
                Through exploration and learning, it discovered that building momentum by
                rocking back and forth is key to reaching the goal on the right peak.
            </p>

            <h4>Technical Implementation</h4>
            <ul>
                <li><strong>Training Framework:</strong> PyTorch + Stable Baselines3</li>
                <li><strong>Inference Framework:</strong> TensorFlow.js (browser)</li>
                <li><strong>Model Size:</strong> 543 KB (256x256 network weights)</li>
                <li><strong>Conversion Accuracy:</strong> &lt;0.000005 error vs original model</li>
            </ul>

            <p style="margin-top: 20px; font-size: 14px;">
                <strong>View the code:</strong><br>
                <a href="mtncar_env.py" target="_blank">Custom Gym Environment</a> |
                <a href="train_sb3.py" target="_blank">Training Script</a> |
                <a href="convert_to_tfjs.py" target="_blank">Model Converter</a> |
                <a href="src/sb3Model.js" target="_blank">Browser Inference Code</a>
            </p>
        </div>
    </div>
    <script src="src/mountainCar.js"></script>
    <script src="src/mountainCarEnv.js"></script>
    <script src="src/utils.js"></script>
    <script src="src/sb3Model.js"></script>
    <script src="src/game.js"></script>
    <script>
        let game;

        async function init() {
            await tf.setBackend('cpu');
            game = new Game();
        }

        init();
    </script>
</body>
</html> 
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Wf7HjrPRgv",
        "outputId": "9c4b57ac-49bc-4bf8-dcd9-50054542ca59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium==0.29.1 --quiet\n",
        "!pip install numpy==1.26.4 --quiet\n",
        "!pip install tensorflow==2.17.0 --quiet\n",
        "!pip install ml-dtypes>=0.3.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import os, pickle, math"
      ],
      "metadata": {
        "id": "4uYb1uAXPYyA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ObGD(keras.optimizers.Optimizer):\n",
        "    def __init__(self, learning_rate=1.0, gamma=0.99, lamda=0.8, kappa=2.0, name='ObGD'):\n",
        "        \"\"\"Initialize the optimizer.\"\"\"\n",
        "        super().__init__(name=name, learning_rate=learning_rate)\n",
        "        self.gamma = gamma\n",
        "        self.lamda = lamda\n",
        "        self.kappa = kappa\n",
        "        self.state = {}\n",
        "        self.param_groups = [{\n",
        "            'learning_rate': learning_rate,\n",
        "            'gamma': gamma,\n",
        "            'lamda': lamda,\n",
        "            'kappa': kappa\n",
        "        }]\n",
        "\n",
        "    def apply_gradients(self, grads_and_vars, delta, reset=False, name=None, **kwargs):\n",
        "        \"\"\"Mirror of PyTorch's step method\"\"\"\n",
        "        grads_and_vars = [(g, v) for (g, v) in grads_and_vars if g is not None]\n",
        "\n",
        "        def get_var_id(var):\n",
        "            return f\"{var.name}_{var.shape}\"\n",
        "\n",
        "        # First pass: Initialize eligibility traces\n",
        "        for grad, var in grads_and_vars:\n",
        "            var_id = get_var_id(var)\n",
        "            if var_id not in self.state:\n",
        "                self.state[var_id] = {\n",
        "                    \"eligibility_trace\": tf.zeros(var.shape, dtype=var.dtype)\n",
        "                }\n",
        "\n",
        "        # Second pass: Update eligibility traces and compute z_sum\n",
        "        z_sum = 0.0\n",
        "        for group in self.param_groups:\n",
        "            for grad, var in grads_and_vars:\n",
        "                var_id = get_var_id(var)\n",
        "                e = self.state[var_id][\"eligibility_trace\"]\n",
        "\n",
        "                # Update eligibility trace\n",
        "                decay = group['gamma'] * group['lamda']\n",
        "                new_e = tf.multiply(e, decay)\n",
        "                new_e = tf.add(new_e, grad)\n",
        "\n",
        "                self.state[var_id][\"eligibility_trace\"] = new_e\n",
        "                z_sum += tf.reduce_sum(tf.abs(new_e))\n",
        "\n",
        "        # Compute step size\n",
        "        delta_bar = tf.maximum(tf.abs(delta), 1.0)\n",
        "        dot_product = delta_bar * z_sum * group['learning_rate'] * group['kappa']\n",
        "        step_size = tf.where(dot_product > 1,\n",
        "                           group['learning_rate'] / dot_product,\n",
        "                           group['learning_rate'])\n",
        "\n",
        "        # Third pass: Update parameters\n",
        "        for grad, var in grads_and_vars:\n",
        "            var_id = get_var_id(var)\n",
        "            e = self.state[var_id][\"eligibility_trace\"]\n",
        "            var.assign_sub(step_size * delta * e)\n",
        "\n",
        "            if reset:\n",
        "                self.state[var_id][\"eligibility_trace\"] = tf.zeros(var.shape, dtype=var.dtype)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'gamma': self.gamma,\n",
        "            'lamda': self.lamda,\n",
        "            'kappa': self.kappa,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "5mMoo8xBPchi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_init(shape, sparsity, type='uniform'):\n",
        "    if len(shape) == 2:\n",
        "        fan_out, fan_in = shape\n",
        "        num_zeros = int(math.ceil(sparsity * fan_in))\n",
        "\n",
        "        if type == 'uniform':\n",
        "            tensor = tf.random.uniform(shape, -math.sqrt(1.0/fan_in), math.sqrt(1.0/fan_in))\n",
        "        elif type == 'normal':\n",
        "            tensor = tf.random.normal(shape, 0, math.sqrt(1.0/fan_in))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown initialization type\")\n",
        "\n",
        "        mask = tf.ones(shape)\n",
        "        for col_idx in range(fan_out):\n",
        "            # Convert to int32\n",
        "            col_idx = tf.cast(col_idx, tf.int32)\n",
        "            zero_indices = tf.cast(tf.random.shuffle(tf.range(fan_in))[:num_zeros], tf.int32)\n",
        "            updates = tf.zeros(num_zeros)\n",
        "            indices = tf.stack([\n",
        "                tf.repeat(col_idx, num_zeros),\n",
        "                zero_indices\n",
        "            ], axis=1)\n",
        "            mask = tf.tensor_scatter_nd_update(mask, indices, updates)\n",
        "        return tensor * mask\n",
        "\n",
        "    elif len(shape) == 4:\n",
        "        channels_out, channels_in, h, w = shape\n",
        "        fan_in = channels_in * h * w\n",
        "        num_zeros = int(math.ceil(sparsity * fan_in))\n",
        "\n",
        "        if type == 'uniform':\n",
        "            tensor = tf.random.uniform(shape, -math.sqrt(1.0/fan_in), math.sqrt(1.0/fan_in))\n",
        "        elif type == 'normal':\n",
        "            tensor = tf.random.normal(shape, 0, math.sqrt(1.0/fan_in))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown initialization type\")\n",
        "\n",
        "        mask = tf.ones(shape)\n",
        "        for out_channel_idx in range(channels_out):\n",
        "            # Convert to int32\n",
        "            out_channel_idx = tf.cast(out_channel_idx, tf.int32)\n",
        "            zero_indices = tf.cast(tf.random.shuffle(tf.range(fan_in))[:num_zeros], tf.int32)\n",
        "            updates = tf.zeros(num_zeros)\n",
        "            flat_mask = tf.reshape(mask[out_channel_idx], [-1])\n",
        "            flat_mask = tf.tensor_scatter_nd_update(\n",
        "                flat_mask,\n",
        "                tf.expand_dims(zero_indices, 1),\n",
        "                updates\n",
        "            )\n",
        "            mask = tf.tensor_scatter_nd_update(\n",
        "                mask,\n",
        "                tf.expand_dims(out_channel_idx, 0),\n",
        "                [tf.reshape(flat_mask, [channels_in, h, w])]\n",
        "            )\n",
        "        return tensor * mask\n",
        "    else:\n",
        "        raise ValueError(\"Only tensors with 2 or 4 dimensions are supported\")"
      ],
      "metadata": {
        "id": "uyevu0cDQT4P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleMeanStd:\n",
        "    def __init__(self, shape=()):\n",
        "        self.mean = np.zeros(shape, \"float64\")\n",
        "        self.var = np.ones(shape, \"float64\")\n",
        "        self.p = np.ones(shape, \"float64\")\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, x):\n",
        "        if self.count == 0:\n",
        "            self.mean = x\n",
        "            self.p = np.zeros_like(x)\n",
        "        self.mean, self.var, self.p, self.count = self.update_mean_var_count_from_moments(self.mean, self.p, self.count, x*1.0)\n",
        "\n",
        "    def update_mean_var_count_from_moments(self, mean, p, count, sample):\n",
        "        new_count = count + 1\n",
        "        new_mean = mean + (sample - mean) / new_count\n",
        "        p = p + (sample - mean) * (sample - new_mean)\n",
        "        new_var = 1 if new_count < 2 else p / (new_count - 1)\n",
        "        return new_mean, new_var, p, new_count\n",
        "\n",
        "class NormalizeObservation(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env, epsilon: float = 1e-8):\n",
        "        super().__init__(env)\n",
        "        try:\n",
        "            self.num_envs = self.get_wrapper_attr(\"num_envs\")\n",
        "            self.is_vector_env = self.get_wrapper_attr(\"is_vector_env\")\n",
        "        except AttributeError:\n",
        "            self.num_envs = 1\n",
        "            self.is_vector_env = False\n",
        "\n",
        "        if self.is_vector_env:\n",
        "            self.obs_stats = SampleMeanStd(shape=self.single_observation_space.shape)\n",
        "        else:\n",
        "            self.obs_stats = SampleMeanStd(shape=self.observation_space.shape)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        if self.is_vector_env:\n",
        "            obs = self.normalize(obs)\n",
        "        else:\n",
        "            obs = self.normalize(np.array([obs]))[0]\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        if self.is_vector_env:\n",
        "            return self.normalize(obs), info\n",
        "        else:\n",
        "            return self.normalize(np.array([obs]))[0], info\n",
        "\n",
        "    def normalize(self, obs):\n",
        "        self.obs_stats.update(obs)\n",
        "        return (obs - self.obs_stats.mean) / np.sqrt(self.obs_stats.var + self.epsilon)\n",
        "\n",
        "class ScaleReward(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env, gamma: float = 0.99, epsilon: float = 1e-8):\n",
        "        super().__init__(env)\n",
        "        try:\n",
        "            self.num_envs = self.get_wrapper_attr(\"num_envs\")\n",
        "            self.is_vector_env = self.get_wrapper_attr(\"is_vector_env\")\n",
        "        except AttributeError:\n",
        "            self.num_envs = 1\n",
        "            self.is_vector_env = False\n",
        "        self.reward_stats = SampleMeanStd(shape=())\n",
        "        self.reward_trace = np.zeros(self.num_envs)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        if not self.is_vector_env:\n",
        "            rews = np.array([rews])\n",
        "        term = terminateds or truncateds\n",
        "        self.reward_trace = self.reward_trace * self.gamma * (1 - term) + rews\n",
        "        rews = self.normalize(rews)\n",
        "        if not self.is_vector_env:\n",
        "            rews = rews[0]\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def normalize(self, rews):\n",
        "        self.reward_stats.update(self.reward_trace)\n",
        "        return rews / np.sqrt(self.reward_stats.var + self.epsilon)"
      ],
      "metadata": {
        "id": "_LmtQhf-lWFZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddTimeInfo(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__(env)\n",
        "        if self.env.num_envs > 1:\n",
        "            raise ValueError(\"AddTimeInfo only supports single environments\")\n",
        "        self.epi_time = -0.5\n",
        "        if 'dm_control' in env.spec.id:\n",
        "            self.time_limit = 1000\n",
        "        else:\n",
        "            self.time_limit = env.spec.max_episode_steps\n",
        "        self.obs_space_size = self.observation_space.shape[0] + self.env.num_envs\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_space_size,), dtype=np.float32)\n",
        "        if not (isinstance(self.action_space, gym.spaces.Box) or isinstance(self.action_space, gym.spaces.Discrete)):\n",
        "            raise ValueError(\"Unsupported action space\")\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        obs = np.concatenate((obs, np.array([self.epi_time] * self.env.num_envs)))\n",
        "        self.epi_time += 1.0 / self.time_limit\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.epi_time = -0.5\n",
        "        obs = np.concatenate((obs, np.array([self.epi_time])))\n",
        "        return obs, info"
      ],
      "metadata": {
        "id": "LRwhxn-_lYND"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "    slope = (end_e - start_e) / duration\n",
        "    return max(slope * t + start_e, end_e)\n",
        "\n",
        "class StreamQ(keras.Model):\n",
        "    def __init__(self, n_obs=11, n_actions=3, hidden_size=32, lr=1.0, epsilon_target=0.01,\n",
        "                 epsilon_start=1.0, exploration_fraction=0.1, total_steps=1_000_000,\n",
        "                 gamma=0.99, lamda=0.8, kappa_value=2.0):\n",
        "        super(StreamQ, self).__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_target = epsilon_target\n",
        "        self.epsilon = epsilon_start\n",
        "        self.exploration_fraction = exploration_fraction\n",
        "        self.total_steps = total_steps\n",
        "        self.time_step = 0\n",
        "\n",
        "        # Define layers\n",
        "        self.fc1_v = keras.layers.Dense(\n",
        "            hidden_size,\n",
        "            kernel_initializer=lambda shape, dtype: sparse_init(shape, 0.9),\n",
        "            bias_initializer='zeros'\n",
        "        )\n",
        "        self.hidden_v = keras.layers.Dense(\n",
        "            hidden_size,\n",
        "            kernel_initializer=lambda shape, dtype: sparse_init(shape, 0.9),\n",
        "            bias_initializer='zeros'\n",
        "        )\n",
        "        self.fc_v = keras.layers.Dense(\n",
        "            n_actions,\n",
        "            kernel_initializer=lambda shape, dtype: sparse_init(shape, 0.9),\n",
        "            bias_initializer='zeros'\n",
        "        )\n",
        "        self.layer_norm1 = keras.layers.LayerNormalization()\n",
        "        self.layer_norm2 = keras.layers.LayerNormalization()\n",
        "\n",
        "        self.optimizer = ObGD(learning_rate=lr, gamma=gamma, lamda=lamda, kappa=kappa_value)\n",
        "\n",
        "    def call(self, x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "        # Ensure input has batch dimension\n",
        "        if len(x.shape) == 1:\n",
        "            x = tf.expand_dims(x, 0)\n",
        "        x = self.fc1_v(x)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = self.hidden_v(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        return self.fc_v(x)\n",
        "\n",
        "    def sample_action(self, s):\n",
        "        self.time_step += 1\n",
        "        self.epsilon = linear_schedule(\n",
        "            self.epsilon_start,\n",
        "            self.epsilon_target,\n",
        "            self.exploration_fraction * self.total_steps,\n",
        "            self.time_step\n",
        "        )\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            q_values = self.call(s)\n",
        "            greedy_action = tf.argmax(q_values, axis=-1).numpy()[0]\n",
        "            random_action = np.random.randint(0, self.n_actions)\n",
        "            if greedy_action == random_action:\n",
        "                return random_action, False\n",
        "            else:\n",
        "                return random_action, True\n",
        "        else:\n",
        "            q_values = self.call(s)\n",
        "            return tf.argmax(q_values, axis=-1).numpy()[0], False\n",
        "\n",
        "    def update_params(self, s, a, r, s_prime, done, is_nongreedy, overshooting_info=False):\n",
        "        done_mask = 0.0 if done else 1.0\n",
        "        s = tf.convert_to_tensor(s, dtype=tf.float32)\n",
        "        s_prime = tf.convert_to_tensor(s_prime, dtype=tf.float32)\n",
        "        r = tf.convert_to_tensor(r, dtype=tf.float32)\n",
        "        done_mask = tf.convert_to_tensor(done_mask, dtype=tf.float32)\n",
        "\n",
        "        # Ensure inputs have batch dimension\n",
        "        if len(s.shape) == 1:\n",
        "            s = tf.expand_dims(s, 0)\n",
        "        if len(s_prime.shape) == 1:\n",
        "            s_prime = tf.expand_dims(s_prime, 0)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_sa = self.call(s)[0][a]\n",
        "            max_q_s_prime_a_prime = tf.reduce_max(self.call(s_prime)[0])\n",
        "            td_target = r + self.gamma * max_q_s_prime_a_prime * done_mask\n",
        "            delta = td_target - q_sa\n",
        "            loss = -q_sa\n",
        "\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        grads_and_vars = list(zip(grads, self.trainable_variables))\n",
        "\n",
        "        # Log gradient stats occasionally (every 1000 steps)\n",
        "        # if self.time_step % 1000 == 0:\n",
        "        #     grad_norms = []\n",
        "        #     for grad, var in grads_and_vars:\n",
        "        #         if grad is not None:\n",
        "        #             grad_norms.append(tf.norm(grad).numpy())\n",
        "        #     if grad_norms:\n",
        "        #         print(f\"Step {self.time_step}, Gradient Stats - Mean: {np.mean(grad_norms):.6f}, Std: {np.std(grad_norms):.6f}, Max: {np.max(grad_norms):.6f}, Min: {np.min(grad_norms):.6f}\")\n",
        "\n",
        "        self.optimizer.apply_gradients(grads_and_vars, delta.numpy(), reset=(done or is_nongreedy))\n",
        "\n",
        "        if overshooting_info:\n",
        "            max_q_s_prime_a_prime = tf.reduce_max(self.call(s_prime)[0])\n",
        "            td_target = r + self.gamma * max_q_s_prime_a_prime * done_mask\n",
        "            delta_bar = td_target - self.call(s)[0][a]\n",
        "            if tf.sign(delta_bar * delta).numpy() == -1:\n",
        "                print(\"Overshooting Detected!\")"
      ],
      "metadata": {
        "id": "QJLCYrJrlaEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_env_interaction(env_name, seed, lr, gamma, lamda, total_steps, epsilon_target,\n",
        "                         epsilon_start, exploration_fraction, kappa_value, debug,\n",
        "                         overshooting_info, render=False):\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    env = gym.make(env_name, render_mode='human', max_episode_steps=10_000) if render else gym.make(env_name, max_episode_steps=10_000)\n",
        "    env = gym.wrappers.FlattenObservation(env)\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    env = ScaleReward(env, gamma=gamma)\n",
        "    env = NormalizeObservation(env)\n",
        "    env = AddTimeInfo(env)\n",
        "\n",
        "    agent = StreamQ(\n",
        "        n_obs=env.observation_space.shape[0],\n",
        "        n_actions=env.action_space.n,\n",
        "        lr=lr,\n",
        "        gamma=gamma,\n",
        "        lamda=lamda,\n",
        "        epsilon_target=epsilon_target,\n",
        "        epsilon_start=epsilon_start,\n",
        "        exploration_fraction=exploration_fraction,\n",
        "        total_steps=total_steps,\n",
        "        kappa_value=kappa_value\n",
        "    )\n",
        "\n",
        "    if debug:\n",
        "        print(\"seed: {}\".format(seed), \"env: {}\".format(env.spec.id))\n",
        "\n",
        "    returns, term_time_steps = [], []\n",
        "    s, _ = env.reset(seed=seed)\n",
        "    episode_num = 1\n",
        "\n",
        "    for t in range(1, total_steps+1):\n",
        "        a, is_nongreedy = agent.sample_action(s)\n",
        "        s_prime, r, terminated, truncated, info = env.step(a)\n",
        "        agent.update_params(s, a, r, s_prime, terminated or truncated, is_nongreedy, overshooting_info)\n",
        "        s = s_prime\n",
        "\n",
        "        if terminated or truncated:\n",
        "            if debug:\n",
        "                print(\"Episodic Return: {}, Time Step {}, Episode Number {}, Epsilon {}\".format(\n",
        "                    info['episode']['r'][0], t, episode_num, agent.epsilon))\n",
        "            returns.append(info['episode']['r'][0])\n",
        "            term_time_steps.append(t)\n",
        "            terminated, truncated = False, False\n",
        "            s, _ = env.reset()\n",
        "            episode_num += 1\n",
        "\n",
        "    env.close()\n",
        "    save_dir = \"data_stream_q_{}_lr{}_gamma{}_lamda{}\".format(env.spec.id, lr, gamma, lamda)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    with open(os.path.join(save_dir, \"seed_{}.pkl\".format(seed)), \"wb\") as f:\n",
        "        pickle.dump((returns, term_time_steps, env_name), f)"
      ],
      "metadata": {
        "id": "5taiB-OSlbiw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_env_interaction(\n",
        "    env_name='CartPole-v1',\n",
        "    seed=0,\n",
        "    lr=1.0,\n",
        "    gamma=0.99,\n",
        "    lamda=0.8,\n",
        "    total_steps=100_000,\n",
        "    epsilon_target=0.01,\n",
        "    epsilon_start=1.0,\n",
        "    exploration_fraction=0.05,\n",
        "    kappa_value=2.0,\n",
        "    debug=True,\n",
        "    overshooting_info=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b7SCyUChldRN",
        "outputId": "67642c00-b624-4c83-fee0-616fafeedf71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed: 0 env: CartPole-v1\n",
            "Episodic Return: 18.0, Time Step 18, Episode Number 1, Epsilon 0.996436\n",
            "Episodic Return: 46.0, Time Step 64, Episode Number 2, Epsilon 0.987328\n",
            "Episodic Return: 27.0, Time Step 91, Episode Number 3, Epsilon 0.981982\n",
            "Episodic Return: 12.0, Time Step 103, Episode Number 4, Epsilon 0.979606\n",
            "Episodic Return: 10.0, Time Step 113, Episode Number 5, Epsilon 0.977626\n",
            "Episodic Return: 43.0, Time Step 156, Episode Number 6, Epsilon 0.969112\n",
            "Episodic Return: 32.0, Time Step 188, Episode Number 7, Epsilon 0.962776\n",
            "Episodic Return: 18.0, Time Step 206, Episode Number 8, Epsilon 0.959212\n",
            "Episodic Return: 49.0, Time Step 255, Episode Number 9, Epsilon 0.94951\n",
            "Episodic Return: 26.0, Time Step 281, Episode Number 10, Epsilon 0.944362\n",
            "Episodic Return: 30.0, Time Step 311, Episode Number 11, Epsilon 0.938422\n",
            "Episodic Return: 26.0, Time Step 337, Episode Number 12, Epsilon 0.933274\n",
            "Episodic Return: 19.0, Time Step 356, Episode Number 13, Epsilon 0.929512\n",
            "Episodic Return: 16.0, Time Step 372, Episode Number 14, Epsilon 0.9263440000000001\n",
            "Episodic Return: 19.0, Time Step 391, Episode Number 15, Epsilon 0.922582\n",
            "Episodic Return: 40.0, Time Step 431, Episode Number 16, Epsilon 0.914662\n",
            "Episodic Return: 33.0, Time Step 464, Episode Number 17, Epsilon 0.908128\n",
            "Episodic Return: 13.0, Time Step 477, Episode Number 18, Epsilon 0.905554\n",
            "Episodic Return: 14.0, Time Step 491, Episode Number 19, Epsilon 0.902782\n",
            "Episodic Return: 11.0, Time Step 502, Episode Number 20, Epsilon 0.900604\n",
            "Episodic Return: 28.0, Time Step 530, Episode Number 21, Epsilon 0.89506\n",
            "Episodic Return: 54.0, Time Step 584, Episode Number 22, Epsilon 0.884368\n",
            "Episodic Return: 21.0, Time Step 605, Episode Number 23, Epsilon 0.88021\n",
            "Episodic Return: 10.0, Time Step 615, Episode Number 24, Epsilon 0.8782300000000001\n",
            "Episodic Return: 51.0, Time Step 666, Episode Number 25, Epsilon 0.868132\n",
            "Episodic Return: 39.0, Time Step 705, Episode Number 26, Epsilon 0.86041\n",
            "Episodic Return: 18.0, Time Step 723, Episode Number 27, Epsilon 0.856846\n",
            "Episodic Return: 18.0, Time Step 741, Episode Number 28, Epsilon 0.853282\n",
            "Episodic Return: 26.0, Time Step 767, Episode Number 29, Epsilon 0.8481339999999999\n",
            "Episodic Return: 17.0, Time Step 784, Episode Number 30, Epsilon 0.844768\n",
            "Episodic Return: 23.0, Time Step 807, Episode Number 31, Epsilon 0.840214\n",
            "Episodic Return: 54.0, Time Step 861, Episode Number 32, Epsilon 0.829522\n",
            "Episodic Return: 21.0, Time Step 882, Episode Number 33, Epsilon 0.825364\n",
            "Episodic Return: 32.0, Time Step 914, Episode Number 34, Epsilon 0.819028\n",
            "Episodic Return: 16.0, Time Step 930, Episode Number 35, Epsilon 0.81586\n",
            "Episodic Return: 29.0, Time Step 959, Episode Number 36, Epsilon 0.810118\n",
            "Episodic Return: 46.0, Time Step 1005, Episode Number 37, Epsilon 0.80101\n",
            "Episodic Return: 32.0, Time Step 1037, Episode Number 38, Epsilon 0.794674\n",
            "Episodic Return: 19.0, Time Step 1056, Episode Number 39, Epsilon 0.7909120000000001\n",
            "Episodic Return: 19.0, Time Step 1075, Episode Number 40, Epsilon 0.78715\n",
            "Episodic Return: 12.0, Time Step 1087, Episode Number 41, Epsilon 0.784774\n",
            "Episodic Return: 21.0, Time Step 1108, Episode Number 42, Epsilon 0.780616\n",
            "Episodic Return: 33.0, Time Step 1141, Episode Number 43, Epsilon 0.774082\n",
            "Episodic Return: 26.0, Time Step 1167, Episode Number 44, Epsilon 0.768934\n",
            "Episodic Return: 53.0, Time Step 1220, Episode Number 45, Epsilon 0.75844\n",
            "Episodic Return: 16.0, Time Step 1236, Episode Number 46, Epsilon 0.755272\n",
            "Episodic Return: 23.0, Time Step 1259, Episode Number 47, Epsilon 0.750718\n",
            "Episodic Return: 30.0, Time Step 1289, Episode Number 48, Epsilon 0.7447779999999999\n",
            "Episodic Return: 14.0, Time Step 1303, Episode Number 49, Epsilon 0.7420059999999999\n",
            "Episodic Return: 105.0, Time Step 1408, Episode Number 50, Epsilon 0.7212160000000001\n",
            "Episodic Return: 63.0, Time Step 1471, Episode Number 51, Epsilon 0.708742\n",
            "Episodic Return: 63.0, Time Step 1534, Episode Number 52, Epsilon 0.696268\n",
            "Episodic Return: 88.0, Time Step 1622, Episode Number 53, Epsilon 0.678844\n",
            "Episodic Return: 28.0, Time Step 1650, Episode Number 54, Epsilon 0.6733\n",
            "Episodic Return: 13.0, Time Step 1663, Episode Number 55, Epsilon 0.670726\n",
            "Episodic Return: 38.0, Time Step 1701, Episode Number 56, Epsilon 0.6632020000000001\n",
            "Episodic Return: 60.0, Time Step 1761, Episode Number 57, Epsilon 0.651322\n",
            "Episodic Return: 144.0, Time Step 1905, Episode Number 58, Epsilon 0.6228100000000001\n",
            "Episodic Return: 21.0, Time Step 1926, Episode Number 59, Epsilon 0.618652\n",
            "Episodic Return: 32.0, Time Step 1958, Episode Number 60, Epsilon 0.6123160000000001\n",
            "Episodic Return: 24.0, Time Step 1982, Episode Number 61, Epsilon 0.607564\n",
            "Episodic Return: 62.0, Time Step 2044, Episode Number 62, Epsilon 0.595288\n",
            "Episodic Return: 229.0, Time Step 2273, Episode Number 63, Epsilon 0.549946\n",
            "Episodic Return: 162.0, Time Step 2435, Episode Number 64, Epsilon 0.51787\n",
            "Episodic Return: 95.0, Time Step 2530, Episode Number 65, Epsilon 0.49906000000000006\n",
            "Episodic Return: 61.0, Time Step 2591, Episode Number 66, Epsilon 0.486982\n",
            "Episodic Return: 128.0, Time Step 2719, Episode Number 67, Epsilon 0.461638\n",
            "Episodic Return: 15.0, Time Step 2734, Episode Number 68, Epsilon 0.4586680000000001\n",
            "Episodic Return: 269.0, Time Step 3003, Episode Number 69, Epsilon 0.40540600000000004\n",
            "Episodic Return: 40.0, Time Step 3043, Episode Number 70, Epsilon 0.397486\n",
            "Episodic Return: 109.0, Time Step 3152, Episode Number 71, Epsilon 0.375904\n",
            "Episodic Return: 280.0, Time Step 3432, Episode Number 72, Epsilon 0.3204640000000001\n",
            "Episodic Return: 179.0, Time Step 3611, Episode Number 73, Epsilon 0.285022\n",
            "Episodic Return: 157.0, Time Step 3768, Episode Number 74, Epsilon 0.25393600000000005\n",
            "Episodic Return: 189.0, Time Step 3957, Episode Number 75, Epsilon 0.2165140000000001\n",
            "Episodic Return: 195.0, Time Step 4152, Episode Number 76, Epsilon 0.17790400000000006\n",
            "Episodic Return: 218.0, Time Step 4370, Episode Number 77, Epsilon 0.13474000000000008\n",
            "Episodic Return: 213.0, Time Step 4583, Episode Number 78, Epsilon 0.09256600000000004\n",
            "Episodic Return: 531.0, Time Step 5114, Episode Number 79, Epsilon 0.01\n",
            "Episodic Return: 137.0, Time Step 5251, Episode Number 80, Epsilon 0.01\n",
            "Episodic Return: 125.0, Time Step 5376, Episode Number 81, Epsilon 0.01\n",
            "Episodic Return: 119.0, Time Step 5495, Episode Number 82, Epsilon 0.01\n",
            "Episodic Return: 93.0, Time Step 5588, Episode Number 83, Epsilon 0.01\n",
            "Episodic Return: 74.0, Time Step 5662, Episode Number 84, Epsilon 0.01\n",
            "Episodic Return: 58.0, Time Step 5720, Episode Number 85, Epsilon 0.01\n",
            "Episodic Return: 42.0, Time Step 5762, Episode Number 86, Epsilon 0.01\n",
            "Episodic Return: 39.0, Time Step 5801, Episode Number 87, Epsilon 0.01\n",
            "Episodic Return: 35.0, Time Step 5836, Episode Number 88, Epsilon 0.01\n",
            "Episodic Return: 53.0, Time Step 5889, Episode Number 89, Epsilon 0.01\n",
            "Episodic Return: 42.0, Time Step 5931, Episode Number 90, Epsilon 0.01\n",
            "Episodic Return: 47.0, Time Step 5978, Episode Number 91, Epsilon 0.01\n",
            "Episodic Return: 54.0, Time Step 6032, Episode Number 92, Epsilon 0.01\n",
            "Episodic Return: 54.0, Time Step 6086, Episode Number 93, Epsilon 0.01\n",
            "Episodic Return: 42.0, Time Step 6128, Episode Number 94, Epsilon 0.01\n",
            "Episodic Return: 42.0, Time Step 6170, Episode Number 95, Epsilon 0.01\n",
            "Episodic Return: 50.0, Time Step 6220, Episode Number 96, Epsilon 0.01\n",
            "Episodic Return: 48.0, Time Step 6268, Episode Number 97, Epsilon 0.01\n",
            "Episodic Return: 34.0, Time Step 6302, Episode Number 98, Epsilon 0.01\n",
            "Episodic Return: 41.0, Time Step 6343, Episode Number 99, Epsilon 0.01\n",
            "Episodic Return: 47.0, Time Step 6390, Episode Number 100, Epsilon 0.01\n",
            "Episodic Return: 55.0, Time Step 6445, Episode Number 101, Epsilon 0.01\n",
            "Episodic Return: 50.0, Time Step 6495, Episode Number 102, Epsilon 0.01\n",
            "Episodic Return: 52.0, Time Step 6547, Episode Number 103, Epsilon 0.01\n",
            "Episodic Return: 50.0, Time Step 6597, Episode Number 104, Epsilon 0.01\n",
            "Episodic Return: 47.0, Time Step 6644, Episode Number 105, Epsilon 0.01\n",
            "Episodic Return: 49.0, Time Step 6693, Episode Number 106, Epsilon 0.01\n",
            "Episodic Return: 44.0, Time Step 6737, Episode Number 107, Epsilon 0.01\n",
            "Episodic Return: 45.0, Time Step 6782, Episode Number 108, Epsilon 0.01\n",
            "Episodic Return: 45.0, Time Step 6827, Episode Number 109, Epsilon 0.01\n",
            "Episodic Return: 50.0, Time Step 6877, Episode Number 110, Epsilon 0.01\n",
            "Episodic Return: 45.0, Time Step 6922, Episode Number 111, Epsilon 0.01\n",
            "Episodic Return: 42.0, Time Step 6964, Episode Number 112, Epsilon 0.01\n",
            "Episodic Return: 45.0, Time Step 7009, Episode Number 113, Epsilon 0.01\n",
            "Episodic Return: 49.0, Time Step 7058, Episode Number 114, Epsilon 0.01\n",
            "Episodic Return: 49.0, Time Step 7107, Episode Number 115, Epsilon 0.01\n",
            "Episodic Return: 54.0, Time Step 7161, Episode Number 116, Epsilon 0.01\n",
            "Episodic Return: 52.0, Time Step 7213, Episode Number 117, Epsilon 0.01\n",
            "Episodic Return: 51.0, Time Step 7264, Episode Number 118, Epsilon 0.01\n",
            "Episodic Return: 56.0, Time Step 7320, Episode Number 119, Epsilon 0.01\n",
            "Episodic Return: 54.0, Time Step 7374, Episode Number 120, Epsilon 0.01\n",
            "Episodic Return: 46.0, Time Step 7420, Episode Number 121, Epsilon 0.01\n",
            "Episodic Return: 43.0, Time Step 7463, Episode Number 122, Epsilon 0.01\n",
            "Episodic Return: 48.0, Time Step 7511, Episode Number 123, Epsilon 0.01\n",
            "Episodic Return: 48.0, Time Step 7559, Episode Number 124, Epsilon 0.01\n",
            "Episodic Return: 43.0, Time Step 7602, Episode Number 125, Epsilon 0.01\n",
            "Episodic Return: 43.0, Time Step 7645, Episode Number 126, Epsilon 0.01\n",
            "Episodic Return: 48.0, Time Step 7693, Episode Number 127, Epsilon 0.01\n",
            "Episodic Return: 43.0, Time Step 7736, Episode Number 128, Epsilon 0.01\n",
            "Episodic Return: 50.0, Time Step 7786, Episode Number 129, Epsilon 0.01\n",
            "Episodic Return: 43.0, Time Step 7829, Episode Number 130, Epsilon 0.01\n",
            "Episodic Return: 47.0, Time Step 7876, Episode Number 131, Epsilon 0.01\n",
            "Episodic Return: 38.0, Time Step 7914, Episode Number 132, Epsilon 0.01\n",
            "Episodic Return: 10.0, Time Step 7924, Episode Number 133, Epsilon 0.01\n",
            "Episodic Return: 38.0, Time Step 7962, Episode Number 134, Epsilon 0.01\n",
            "Episodic Return: 50.0, Time Step 8012, Episode Number 135, Epsilon 0.01\n",
            "Episodic Return: 56.0, Time Step 8068, Episode Number 136, Epsilon 0.01\n",
            "Episodic Return: 57.0, Time Step 8125, Episode Number 137, Epsilon 0.01\n",
            "Episodic Return: 36.0, Time Step 8161, Episode Number 138, Epsilon 0.01\n",
            "Episodic Return: 23.0, Time Step 8184, Episode Number 139, Epsilon 0.01\n",
            "Episodic Return: 31.0, Time Step 8215, Episode Number 140, Epsilon 0.01\n",
            "Episodic Return: 35.0, Time Step 8250, Episode Number 141, Epsilon 0.01\n",
            "Episodic Return: 29.0, Time Step 8279, Episode Number 142, Epsilon 0.01\n",
            "Episodic Return: 12.0, Time Step 8291, Episode Number 143, Epsilon 0.01\n",
            "Episodic Return: 55.0, Time Step 8346, Episode Number 144, Epsilon 0.01\n",
            "Episodic Return: 45.0, Time Step 8391, Episode Number 145, Epsilon 0.01\n",
            "Episodic Return: 33.0, Time Step 8424, Episode Number 146, Epsilon 0.01\n",
            "Episodic Return: 43.0, Time Step 8467, Episode Number 147, Epsilon 0.01\n",
            "Episodic Return: 29.0, Time Step 8496, Episode Number 148, Epsilon 0.01\n",
            "Episodic Return: 9.0, Time Step 8505, Episode Number 149, Epsilon 0.01\n",
            "Episodic Return: 11.0, Time Step 8516, Episode Number 150, Epsilon 0.01\n",
            "Episodic Return: 44.0, Time Step 8560, Episode Number 151, Epsilon 0.01\n",
            "Episodic Return: 54.0, Time Step 8614, Episode Number 152, Epsilon 0.01\n",
            "Episodic Return: 68.0, Time Step 8682, Episode Number 153, Epsilon 0.01\n",
            "Episodic Return: 72.0, Time Step 8754, Episode Number 154, Epsilon 0.01\n",
            "Episodic Return: 11.0, Time Step 8765, Episode Number 155, Epsilon 0.01\n",
            "Episodic Return: 11.0, Time Step 8776, Episode Number 156, Epsilon 0.01\n",
            "Episodic Return: 16.0, Time Step 8792, Episode Number 157, Epsilon 0.01\n",
            "Episodic Return: 23.0, Time Step 8815, Episode Number 158, Epsilon 0.01\n",
            "Episodic Return: 78.0, Time Step 8893, Episode Number 159, Epsilon 0.01\n",
            "Episodic Return: 94.0, Time Step 8987, Episode Number 160, Epsilon 0.01\n",
            "Episodic Return: 121.0, Time Step 9108, Episode Number 161, Epsilon 0.01\n",
            "Episodic Return: 273.0, Time Step 9381, Episode Number 162, Epsilon 0.01\n",
            "Episodic Return: 386.0, Time Step 9767, Episode Number 163, Epsilon 0.01\n",
            "Episodic Return: 1033.0, Time Step 10800, Episode Number 164, Epsilon 0.01\n",
            "Episodic Return: 109.0, Time Step 10909, Episode Number 165, Epsilon 0.01\n",
            "Episodic Return: 15.0, Time Step 10924, Episode Number 166, Epsilon 0.01\n",
            "Episodic Return: 13.0, Time Step 10937, Episode Number 167, Epsilon 0.01\n",
            "Episodic Return: 10.0, Time Step 10947, Episode Number 168, Epsilon 0.01\n",
            "Episodic Return: 22.0, Time Step 10969, Episode Number 169, Epsilon 0.01\n",
            "Episodic Return: 93.0, Time Step 11062, Episode Number 170, Epsilon 0.01\n",
            "Episodic Return: 154.0, Time Step 11216, Episode Number 171, Epsilon 0.01\n",
            "Episodic Return: 59.0, Time Step 11275, Episode Number 172, Epsilon 0.01\n",
            "Episodic Return: 79.0, Time Step 11354, Episode Number 173, Epsilon 0.01\n",
            "Episodic Return: 139.0, Time Step 11493, Episode Number 174, Epsilon 0.01\n",
            "Episodic Return: 243.0, Time Step 11736, Episode Number 175, Epsilon 0.01\n",
            "Episodic Return: 338.0, Time Step 12074, Episode Number 176, Epsilon 0.01\n",
            "Episodic Return: 607.0, Time Step 12681, Episode Number 177, Epsilon 0.01\n",
            "Episodic Return: 833.0, Time Step 13514, Episode Number 178, Epsilon 0.01\n",
            "Episodic Return: 271.0, Time Step 13785, Episode Number 179, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 23785, Episode Number 180, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 33785, Episode Number 181, Epsilon 0.01\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-dfb4a6ec36f6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m agent_env_interaction(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-37f439c701de>\u001b[0m in \u001b[0;36magent_env_interaction\u001b[0;34m(env_name, seed, lr, gamma, lamda, total_steps, epsilon_target, epsilon_start, exploration_fraction, kappa_value, debug, overshooting_info, render)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_nongreedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_nongreedy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0movershooting_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-44d3324480b8>\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(self, s, a, r, s_prime, done, is_nongreedy, overshooting_info)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mq_sa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_nongreedy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_StridedSliceGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    289\u001b[0m   \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrides_static\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstrides_static\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m   return array_ops.strided_slice_grad(\n\u001b[0m\u001b[1;32m    292\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice_grad\u001b[0;34m(shape, begin, end, strides, dy, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  11145\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11146\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11147\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m  11148\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"StridedSliceGrad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11149\u001b[0m         \u001b[0;34m\"begin_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VupEVAlgN_Sj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
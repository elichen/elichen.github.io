{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Wf7HjrPRgv",
        "outputId": "ef0a0d23-49c1-4bb7-ba38-14c3fab62f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium==0.29.1 --quiet\n",
        "!pip install numpy==1.26.4 --quiet\n",
        "!pip install tensorflow==2.17.0 --quiet\n",
        "!pip install ml-dtypes>=0.3.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import os, pickle, math"
      ],
      "metadata": {
        "id": "4uYb1uAXPYyA"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ObGD(keras.optimizers.Optimizer):\n",
        "    def __init__(self, learning_rate=1.0, gamma=0.99, lamda=0.8, kappa=2.0, name='ObGD'):\n",
        "        \"\"\"Initialize the optimizer.\"\"\"\n",
        "        super().__init__(name=name, learning_rate=learning_rate)\n",
        "        self.gamma = gamma\n",
        "        self.lamda = lamda\n",
        "        self.kappa = kappa\n",
        "        self.state = {}\n",
        "        self.param_groups = [{\n",
        "            'learning_rate': learning_rate,\n",
        "            'gamma': gamma,\n",
        "            'lamda': lamda,\n",
        "            'kappa': kappa\n",
        "        }]\n",
        "\n",
        "    def apply_gradients(self, grads_and_vars, delta, reset=False, name=None, **kwargs):\n",
        "        \"\"\"Mirror of PyTorch's step method\"\"\"\n",
        "        grads_and_vars = [(g, v) for (g, v) in grads_and_vars if g is not None]\n",
        "\n",
        "        def get_var_id(var):\n",
        "            return f\"{var.name}_{var.shape}\"\n",
        "\n",
        "        # First pass: Initialize eligibility traces\n",
        "        for grad, var in grads_and_vars:\n",
        "            var_id = get_var_id(var)\n",
        "            if var_id not in self.state:\n",
        "                self.state[var_id] = {\n",
        "                    \"eligibility_trace\": tf.zeros(var.shape, dtype=var.dtype)\n",
        "                }\n",
        "\n",
        "        # Second pass: Update eligibility traces and compute z_sum\n",
        "        z_sum = 0.0\n",
        "        for group in self.param_groups:\n",
        "            for grad, var in grads_and_vars:\n",
        "                var_id = get_var_id(var)\n",
        "                e = self.state[var_id][\"eligibility_trace\"]\n",
        "\n",
        "                # Update eligibility trace\n",
        "                decay = group['gamma'] * group['lamda']\n",
        "                new_e = tf.multiply(e, decay)\n",
        "                new_e = tf.add(new_e, grad)\n",
        "\n",
        "                self.state[var_id][\"eligibility_trace\"] = new_e\n",
        "                z_sum += tf.reduce_sum(tf.abs(new_e))\n",
        "\n",
        "        # Compute step size\n",
        "        delta_bar = tf.maximum(tf.abs(delta), 1.0)\n",
        "        dot_product = delta_bar * z_sum * group['learning_rate'] * group['kappa']\n",
        "        step_size = tf.where(dot_product > 1,\n",
        "                           group['learning_rate'] / dot_product,\n",
        "                           group['learning_rate'])\n",
        "\n",
        "        # Third pass: Update parameters\n",
        "        for grad, var in grads_and_vars:\n",
        "            var_id = get_var_id(var)\n",
        "            e = self.state[var_id][\"eligibility_trace\"]\n",
        "            var.assign_sub(step_size * delta * e)\n",
        "\n",
        "            if reset:\n",
        "                self.state[var_id][\"eligibility_trace\"] = tf.zeros(var.shape, dtype=var.dtype)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'gamma': self.gamma,\n",
        "            'lamda': self.lamda,\n",
        "            'kappa': self.kappa,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "5mMoo8xBPchi"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_init(shape, sparsity, type='uniform'):\n",
        "    if len(shape) == 2:\n",
        "        fan_out, fan_in = shape\n",
        "        num_zeros = int(math.ceil(sparsity * fan_in))\n",
        "\n",
        "        if type == 'uniform':\n",
        "            tensor = tf.random.uniform(shape, -math.sqrt(1.0/fan_in), math.sqrt(1.0/fan_in))\n",
        "        elif type == 'normal':\n",
        "            tensor = tf.random.normal(shape, 0, math.sqrt(1.0/fan_in))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown initialization type\")\n",
        "\n",
        "        mask = tf.ones(shape)\n",
        "        for col_idx in range(fan_out):\n",
        "            # Convert to int32\n",
        "            col_idx = tf.cast(col_idx, tf.int32)\n",
        "            zero_indices = tf.cast(tf.random.shuffle(tf.range(fan_in))[:num_zeros], tf.int32)\n",
        "            updates = tf.zeros(num_zeros)\n",
        "            indices = tf.stack([\n",
        "                tf.repeat(col_idx, num_zeros),\n",
        "                zero_indices\n",
        "            ], axis=1)\n",
        "            mask = tf.tensor_scatter_nd_update(mask, indices, updates)\n",
        "        return tensor * mask\n",
        "\n",
        "    elif len(shape) == 4:\n",
        "        channels_out, channels_in, h, w = shape\n",
        "        fan_in = channels_in * h * w\n",
        "        num_zeros = int(math.ceil(sparsity * fan_in))\n",
        "\n",
        "        if type == 'uniform':\n",
        "            tensor = tf.random.uniform(shape, -math.sqrt(1.0/fan_in), math.sqrt(1.0/fan_in))\n",
        "        elif type == 'normal':\n",
        "            tensor = tf.random.normal(shape, 0, math.sqrt(1.0/fan_in))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown initialization type\")\n",
        "\n",
        "        mask = tf.ones(shape)\n",
        "        for out_channel_idx in range(channels_out):\n",
        "            # Convert to int32\n",
        "            out_channel_idx = tf.cast(out_channel_idx, tf.int32)\n",
        "            zero_indices = tf.cast(tf.random.shuffle(tf.range(fan_in))[:num_zeros], tf.int32)\n",
        "            updates = tf.zeros(num_zeros)\n",
        "            flat_mask = tf.reshape(mask[out_channel_idx], [-1])\n",
        "            flat_mask = tf.tensor_scatter_nd_update(\n",
        "                flat_mask,\n",
        "                tf.expand_dims(zero_indices, 1),\n",
        "                updates\n",
        "            )\n",
        "            mask = tf.tensor_scatter_nd_update(\n",
        "                mask,\n",
        "                tf.expand_dims(out_channel_idx, 0),\n",
        "                [tf.reshape(flat_mask, [channels_in, h, w])]\n",
        "            )\n",
        "        return tensor * mask\n",
        "    else:\n",
        "        raise ValueError(\"Only tensors with 2 or 4 dimensions are supported\")"
      ],
      "metadata": {
        "id": "uyevu0cDQT4P"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleMeanStd:\n",
        "    def __init__(self, shape=()):\n",
        "        self.mean = np.zeros(shape, \"float64\")\n",
        "        self.var = np.ones(shape, \"float64\")\n",
        "        self.p = np.ones(shape, \"float64\")\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, x):\n",
        "        if self.count == 0:\n",
        "            self.mean = x\n",
        "            self.p = np.zeros_like(x)\n",
        "        self.mean, self.var, self.p, self.count = self.update_mean_var_count_from_moments(self.mean, self.p, self.count, x*1.0)\n",
        "\n",
        "    def update_mean_var_count_from_moments(self, mean, p, count, sample):\n",
        "        new_count = count + 1\n",
        "        new_mean = mean + (sample - mean) / new_count\n",
        "        p = p + (sample - mean) * (sample - new_mean)\n",
        "        new_var = 1 if new_count < 2 else p / (new_count - 1)\n",
        "        return new_mean, new_var, p, new_count\n",
        "\n",
        "class NormalizeObservation(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env, epsilon: float = 1e-8):\n",
        "        super().__init__(env)\n",
        "        try:\n",
        "            self.num_envs = self.get_wrapper_attr(\"num_envs\")\n",
        "            self.is_vector_env = self.get_wrapper_attr(\"is_vector_env\")\n",
        "        except AttributeError:\n",
        "            self.num_envs = 1\n",
        "            self.is_vector_env = False\n",
        "\n",
        "        if self.is_vector_env:\n",
        "            self.obs_stats = SampleMeanStd(shape=self.single_observation_space.shape)\n",
        "        else:\n",
        "            self.obs_stats = SampleMeanStd(shape=self.observation_space.shape)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        if self.is_vector_env:\n",
        "            obs = self.normalize(obs)\n",
        "        else:\n",
        "            obs = self.normalize(np.array([obs]))[0]\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        if self.is_vector_env:\n",
        "            return self.normalize(obs), info\n",
        "        else:\n",
        "            return self.normalize(np.array([obs]))[0], info\n",
        "\n",
        "    def normalize(self, obs):\n",
        "        self.obs_stats.update(obs)\n",
        "        return (obs - self.obs_stats.mean) / np.sqrt(self.obs_stats.var + self.epsilon)\n",
        "\n",
        "class ScaleReward(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env, gamma: float = 0.99, epsilon: float = 1e-8):\n",
        "        super().__init__(env)\n",
        "        try:\n",
        "            self.num_envs = self.get_wrapper_attr(\"num_envs\")\n",
        "            self.is_vector_env = self.get_wrapper_attr(\"is_vector_env\")\n",
        "        except AttributeError:\n",
        "            self.num_envs = 1\n",
        "            self.is_vector_env = False\n",
        "        self.reward_stats = SampleMeanStd(shape=())\n",
        "        self.reward_trace = np.zeros(self.num_envs)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        if not self.is_vector_env:\n",
        "            rews = np.array([rews])\n",
        "        term = terminateds or truncateds\n",
        "        self.reward_trace = self.reward_trace * self.gamma * (1 - term) + rews\n",
        "        rews = self.normalize(rews)\n",
        "        if not self.is_vector_env:\n",
        "            rews = rews[0]\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def normalize(self, rews):\n",
        "        self.reward_stats.update(self.reward_trace)\n",
        "        return rews / np.sqrt(self.reward_stats.var + self.epsilon)"
      ],
      "metadata": {
        "id": "_LmtQhf-lWFZ"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddTimeInfo(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__(env)\n",
        "        if self.env.num_envs > 1:\n",
        "            raise ValueError(\"AddTimeInfo only supports single environments\")\n",
        "        self.epi_time = -0.5\n",
        "        if 'dm_control' in env.spec.id:\n",
        "            self.time_limit = 1000\n",
        "        else:\n",
        "            self.time_limit = env.spec.max_episode_steps\n",
        "        self.obs_space_size = self.observation_space.shape[0] + self.env.num_envs\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_space_size,), dtype=np.float32)\n",
        "        if not (isinstance(self.action_space, gym.spaces.Box) or isinstance(self.action_space, gym.spaces.Discrete)):\n",
        "            raise ValueError(\"Unsupported action space\")\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        obs = np.concatenate((obs, np.array([self.epi_time] * self.env.num_envs)))\n",
        "        self.epi_time += 1.0 / self.time_limit\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.epi_time = -0.5\n",
        "        obs = np.concatenate((obs, np.array([self.epi_time])))\n",
        "        return obs, info"
      ],
      "metadata": {
        "id": "LRwhxn-_lYND"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "    slope = (end_e - start_e) / duration\n",
        "    return max(slope * t + start_e, end_e)\n",
        "\n",
        "class StreamQ(keras.Model):\n",
        "    def __init__(self, n_obs=11, n_actions=3, hidden_size=32, lr=1.0, epsilon_target=0.01,\n",
        "                 epsilon_start=1.0, exploration_fraction=0.1, total_steps=1_000_000,\n",
        "                 gamma=0.99, lamda=0.8, kappa_value=2.0):\n",
        "        super(StreamQ, self).__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_target = epsilon_target\n",
        "        self.epsilon = epsilon_start\n",
        "        self.exploration_fraction = exploration_fraction\n",
        "        self.total_steps = total_steps\n",
        "        self.time_step = 0\n",
        "\n",
        "        # Define layers\n",
        "        self.fc1_v = keras.layers.Dense(\n",
        "            hidden_size,\n",
        "            kernel_initializer=lambda shape, dtype: sparse_init(shape, 0.9),\n",
        "            bias_initializer='zeros'\n",
        "        )\n",
        "        self.hidden_v = keras.layers.Dense(\n",
        "            hidden_size,\n",
        "            kernel_initializer=lambda shape, dtype: sparse_init(shape, 0.9),\n",
        "            bias_initializer='zeros'\n",
        "        )\n",
        "        self.fc_v = keras.layers.Dense(\n",
        "            n_actions,\n",
        "            kernel_initializer=lambda shape, dtype: sparse_init(shape, 0.9),\n",
        "            bias_initializer='zeros'\n",
        "        )\n",
        "        self.layer_norm1 = keras.layers.LayerNormalization()\n",
        "        self.layer_norm2 = keras.layers.LayerNormalization()\n",
        "\n",
        "        self.optimizer = ObGD(learning_rate=lr, gamma=gamma, lamda=lamda, kappa=kappa_value)\n",
        "\n",
        "    def call(self, x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "        # Ensure input has batch dimension\n",
        "        if len(x.shape) == 1:\n",
        "            x = tf.expand_dims(x, 0)\n",
        "        x = self.fc1_v(x)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = self.hidden_v(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        return self.fc_v(x)\n",
        "\n",
        "    def sample_action(self, s):\n",
        "        self.time_step += 1\n",
        "        self.epsilon = linear_schedule(\n",
        "            self.epsilon_start,\n",
        "            self.epsilon_target,\n",
        "            self.exploration_fraction * self.total_steps,\n",
        "            self.time_step\n",
        "        )\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            q_values = self.call(s)\n",
        "            greedy_action = tf.argmax(q_values, axis=-1).numpy()[0]\n",
        "            random_action = np.random.randint(0, self.n_actions)\n",
        "            if greedy_action == random_action:\n",
        "                return random_action, False\n",
        "            else:\n",
        "                return random_action, True\n",
        "        else:\n",
        "            q_values = self.call(s)\n",
        "            return tf.argmax(q_values, axis=-1).numpy()[0], False\n",
        "\n",
        "    def update_params(self, s, a, r, s_prime, done, is_nongreedy, overshooting_info=False):\n",
        "        done_mask = 0.0 if done else 1.0\n",
        "        s = tf.convert_to_tensor(s, dtype=tf.float32)\n",
        "        s_prime = tf.convert_to_tensor(s_prime, dtype=tf.float32)\n",
        "        r = tf.convert_to_tensor(r, dtype=tf.float32)\n",
        "        done_mask = tf.convert_to_tensor(done_mask, dtype=tf.float32)\n",
        "\n",
        "        # Ensure inputs have batch dimension\n",
        "        if len(s.shape) == 1:\n",
        "            s = tf.expand_dims(s, 0)\n",
        "        if len(s_prime.shape) == 1:\n",
        "            s_prime = tf.expand_dims(s_prime, 0)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_sa = self.call(s)[0][a]\n",
        "            max_q_s_prime_a_prime = tf.reduce_max(self.call(s_prime)[0])\n",
        "            td_target = r + self.gamma * max_q_s_prime_a_prime * done_mask\n",
        "            delta = td_target - q_sa\n",
        "            loss = -q_sa\n",
        "\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        grads_and_vars = list(zip(grads, self.trainable_variables))\n",
        "        self.optimizer.apply_gradients(grads_and_vars, delta.numpy(), reset=(done or is_nongreedy))\n",
        "\n",
        "        if overshooting_info:\n",
        "            max_q_s_prime_a_prime = tf.reduce_max(self.call(s_prime)[0])\n",
        "            td_target = r + self.gamma * max_q_s_prime_a_prime * done_mask\n",
        "            delta_bar = td_target - self.call(s)[0][a]\n",
        "            if tf.sign(delta_bar * delta).numpy() == -1:\n",
        "                print(\"Overshooting Detected!\")"
      ],
      "metadata": {
        "id": "QJLCYrJrlaEp"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_env_interaction(env_name, seed, lr, gamma, lamda, total_steps, epsilon_target,\n",
        "                         epsilon_start, exploration_fraction, kappa_value, debug,\n",
        "                         overshooting_info, render=False):\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    env = gym.make(env_name, render_mode='human', max_episode_steps=10_000) if render else gym.make(env_name, max_episode_steps=10_000)\n",
        "    env = gym.wrappers.FlattenObservation(env)\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    env = ScaleReward(env, gamma=gamma)\n",
        "    env = NormalizeObservation(env)\n",
        "    env = AddTimeInfo(env)\n",
        "\n",
        "    agent = StreamQ(\n",
        "        n_obs=env.observation_space.shape[0],\n",
        "        n_actions=env.action_space.n,\n",
        "        lr=lr,\n",
        "        gamma=gamma,\n",
        "        lamda=lamda,\n",
        "        epsilon_target=epsilon_target,\n",
        "        epsilon_start=epsilon_start,\n",
        "        exploration_fraction=exploration_fraction,\n",
        "        total_steps=total_steps,\n",
        "        kappa_value=kappa_value\n",
        "    )\n",
        "\n",
        "    if debug:\n",
        "        print(\"seed: {}\".format(seed), \"env: {}\".format(env.spec.id))\n",
        "\n",
        "    returns, term_time_steps = [], []\n",
        "    s, _ = env.reset(seed=seed)\n",
        "    episode_num = 1\n",
        "\n",
        "    for t in range(1, total_steps+1):\n",
        "        a, is_nongreedy = agent.sample_action(s)\n",
        "        s_prime, r, terminated, truncated, info = env.step(a)\n",
        "        agent.update_params(s, a, r, s_prime, terminated or truncated, is_nongreedy, overshooting_info)\n",
        "        s = s_prime\n",
        "\n",
        "        if terminated or truncated:\n",
        "            if debug:\n",
        "                print(\"Episodic Return: {}, Time Step {}, Episode Number {}, Epsilon {}\".format(\n",
        "                    info['episode']['r'][0], t, episode_num, agent.epsilon))\n",
        "            returns.append(info['episode']['r'][0])\n",
        "            term_time_steps.append(t)\n",
        "            terminated, truncated = False, False\n",
        "            s, _ = env.reset()\n",
        "            episode_num += 1\n",
        "\n",
        "    env.close()\n",
        "    save_dir = \"data_stream_q_{}_lr{}_gamma{}_lamda{}\".format(env.spec.id, lr, gamma, lamda)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    with open(os.path.join(save_dir, \"seed_{}.pkl\".format(seed)), \"wb\") as f:\n",
        "        pickle.dump((returns, term_time_steps, env_name), f)"
      ],
      "metadata": {
        "id": "5taiB-OSlbiw"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_env_interaction(\n",
        "    env_name='CartPole-v1',\n",
        "    seed=0,\n",
        "    lr=1.0,\n",
        "    gamma=0.99,\n",
        "    lamda=0.8,\n",
        "    total_steps=100_000,\n",
        "    epsilon_target=0.01,\n",
        "    epsilon_start=1.0,\n",
        "    exploration_fraction=0.05,\n",
        "    kappa_value=2.0,\n",
        "    debug=True,\n",
        "    overshooting_info=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7SCyUChldRN",
        "outputId": "8512f45d-d07b-4ec6-86e5-3b4d7126f302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed: 0 env: CartPole-v1\n",
            "Episodic Return: 18.0, Time Step 18, Episode Number 1, Epsilon 0.996436\n",
            "Episodic Return: 46.0, Time Step 64, Episode Number 2, Epsilon 0.987328\n",
            "Episodic Return: 27.0, Time Step 91, Episode Number 3, Epsilon 0.981982\n",
            "Episodic Return: 12.0, Time Step 103, Episode Number 4, Epsilon 0.979606\n",
            "Episodic Return: 10.0, Time Step 113, Episode Number 5, Epsilon 0.977626\n",
            "Episodic Return: 43.0, Time Step 156, Episode Number 6, Epsilon 0.969112\n",
            "Episodic Return: 32.0, Time Step 188, Episode Number 7, Epsilon 0.962776\n",
            "Episodic Return: 18.0, Time Step 206, Episode Number 8, Epsilon 0.959212\n",
            "Episodic Return: 49.0, Time Step 255, Episode Number 9, Epsilon 0.94951\n",
            "Episodic Return: 26.0, Time Step 281, Episode Number 10, Epsilon 0.944362\n",
            "Episodic Return: 30.0, Time Step 311, Episode Number 11, Epsilon 0.938422\n",
            "Episodic Return: 26.0, Time Step 337, Episode Number 12, Epsilon 0.933274\n",
            "Episodic Return: 19.0, Time Step 356, Episode Number 13, Epsilon 0.929512\n",
            "Episodic Return: 16.0, Time Step 372, Episode Number 14, Epsilon 0.9263440000000001\n",
            "Episodic Return: 19.0, Time Step 391, Episode Number 15, Epsilon 0.922582\n",
            "Episodic Return: 40.0, Time Step 431, Episode Number 16, Epsilon 0.914662\n",
            "Episodic Return: 33.0, Time Step 464, Episode Number 17, Epsilon 0.908128\n",
            "Episodic Return: 13.0, Time Step 477, Episode Number 18, Epsilon 0.905554\n",
            "Episodic Return: 14.0, Time Step 491, Episode Number 19, Epsilon 0.902782\n",
            "Episodic Return: 11.0, Time Step 502, Episode Number 20, Epsilon 0.900604\n",
            "Episodic Return: 28.0, Time Step 530, Episode Number 21, Epsilon 0.89506\n",
            "Episodic Return: 54.0, Time Step 584, Episode Number 22, Epsilon 0.884368\n",
            "Episodic Return: 21.0, Time Step 605, Episode Number 23, Epsilon 0.88021\n",
            "Episodic Return: 10.0, Time Step 615, Episode Number 24, Epsilon 0.8782300000000001\n",
            "Episodic Return: 51.0, Time Step 666, Episode Number 25, Epsilon 0.868132\n",
            "Episodic Return: 39.0, Time Step 705, Episode Number 26, Epsilon 0.86041\n",
            "Episodic Return: 18.0, Time Step 723, Episode Number 27, Epsilon 0.856846\n",
            "Episodic Return: 18.0, Time Step 741, Episode Number 28, Epsilon 0.853282\n",
            "Episodic Return: 26.0, Time Step 767, Episode Number 29, Epsilon 0.8481339999999999\n",
            "Episodic Return: 17.0, Time Step 784, Episode Number 30, Epsilon 0.844768\n",
            "Episodic Return: 23.0, Time Step 807, Episode Number 31, Epsilon 0.840214\n",
            "Episodic Return: 54.0, Time Step 861, Episode Number 32, Epsilon 0.829522\n",
            "Episodic Return: 21.0, Time Step 882, Episode Number 33, Epsilon 0.825364\n",
            "Episodic Return: 32.0, Time Step 914, Episode Number 34, Epsilon 0.819028\n",
            "Episodic Return: 16.0, Time Step 930, Episode Number 35, Epsilon 0.81586\n",
            "Episodic Return: 29.0, Time Step 959, Episode Number 36, Epsilon 0.810118\n",
            "Episodic Return: 46.0, Time Step 1005, Episode Number 37, Epsilon 0.80101\n",
            "Episodic Return: 32.0, Time Step 1037, Episode Number 38, Epsilon 0.794674\n",
            "Episodic Return: 19.0, Time Step 1056, Episode Number 39, Epsilon 0.7909120000000001\n",
            "Episodic Return: 19.0, Time Step 1075, Episode Number 40, Epsilon 0.78715\n",
            "Episodic Return: 12.0, Time Step 1087, Episode Number 41, Epsilon 0.784774\n",
            "Episodic Return: 21.0, Time Step 1108, Episode Number 42, Epsilon 0.780616\n",
            "Episodic Return: 33.0, Time Step 1141, Episode Number 43, Epsilon 0.774082\n",
            "Episodic Return: 26.0, Time Step 1167, Episode Number 44, Epsilon 0.768934\n",
            "Episodic Return: 53.0, Time Step 1220, Episode Number 45, Epsilon 0.75844\n",
            "Episodic Return: 16.0, Time Step 1236, Episode Number 46, Epsilon 0.755272\n",
            "Episodic Return: 23.0, Time Step 1259, Episode Number 47, Epsilon 0.750718\n",
            "Episodic Return: 30.0, Time Step 1289, Episode Number 48, Epsilon 0.7447779999999999\n",
            "Episodic Return: 14.0, Time Step 1303, Episode Number 49, Epsilon 0.7420059999999999\n",
            "Episodic Return: 105.0, Time Step 1408, Episode Number 50, Epsilon 0.7212160000000001\n",
            "Episodic Return: 63.0, Time Step 1471, Episode Number 51, Epsilon 0.708742\n",
            "Episodic Return: 63.0, Time Step 1534, Episode Number 52, Epsilon 0.696268\n",
            "Episodic Return: 88.0, Time Step 1622, Episode Number 53, Epsilon 0.678844\n",
            "Episodic Return: 28.0, Time Step 1650, Episode Number 54, Epsilon 0.6733\n",
            "Episodic Return: 13.0, Time Step 1663, Episode Number 55, Epsilon 0.670726\n",
            "Episodic Return: 38.0, Time Step 1701, Episode Number 56, Epsilon 0.6632020000000001\n",
            "Episodic Return: 60.0, Time Step 1761, Episode Number 57, Epsilon 0.651322\n",
            "Episodic Return: 144.0, Time Step 1905, Episode Number 58, Epsilon 0.6228100000000001\n",
            "Episodic Return: 21.0, Time Step 1926, Episode Number 59, Epsilon 0.618652\n",
            "Episodic Return: 32.0, Time Step 1958, Episode Number 60, Epsilon 0.6123160000000001\n",
            "Episodic Return: 24.0, Time Step 1982, Episode Number 61, Epsilon 0.607564\n",
            "Episodic Return: 62.0, Time Step 2044, Episode Number 62, Epsilon 0.595288\n",
            "Episodic Return: 229.0, Time Step 2273, Episode Number 63, Epsilon 0.549946\n",
            "Episodic Return: 162.0, Time Step 2435, Episode Number 64, Epsilon 0.51787\n",
            "Episodic Return: 95.0, Time Step 2530, Episode Number 65, Epsilon 0.49906000000000006\n",
            "Episodic Return: 61.0, Time Step 2591, Episode Number 66, Epsilon 0.486982\n",
            "Episodic Return: 128.0, Time Step 2719, Episode Number 67, Epsilon 0.461638\n",
            "Episodic Return: 15.0, Time Step 2734, Episode Number 68, Epsilon 0.4586680000000001\n",
            "Episodic Return: 269.0, Time Step 3003, Episode Number 69, Epsilon 0.40540600000000004\n",
            "Episodic Return: 40.0, Time Step 3043, Episode Number 70, Epsilon 0.397486\n",
            "Episodic Return: 109.0, Time Step 3152, Episode Number 71, Epsilon 0.375904\n",
            "Episodic Return: 280.0, Time Step 3432, Episode Number 72, Epsilon 0.3204640000000001\n",
            "Episodic Return: 179.0, Time Step 3611, Episode Number 73, Epsilon 0.285022\n",
            "Episodic Return: 157.0, Time Step 3768, Episode Number 74, Epsilon 0.25393600000000005\n",
            "Episodic Return: 189.0, Time Step 3957, Episode Number 75, Epsilon 0.2165140000000001\n",
            "Episodic Return: 195.0, Time Step 4152, Episode Number 76, Epsilon 0.17790400000000006\n",
            "Episodic Return: 218.0, Time Step 4370, Episode Number 77, Epsilon 0.13474000000000008\n",
            "Episodic Return: 213.0, Time Step 4583, Episode Number 78, Epsilon 0.09256600000000004\n",
            "Episodic Return: 531.0, Time Step 5114, Episode Number 79, Epsilon 0.01\n",
            "Episodic Return: 137.0, Time Step 5251, Episode Number 80, Epsilon 0.01\n",
            "Episodic Return: 125.0, Time Step 5376, Episode Number 81, Epsilon 0.01\n",
            "Episodic Return: 119.0, Time Step 5495, Episode Number 82, Epsilon 0.01\n",
            "Episodic Return: 93.0, Time Step 5588, Episode Number 83, Epsilon 0.01\n",
            "Episodic Return: 74.0, Time Step 5662, Episode Number 84, Epsilon 0.01\n",
            "Episodic Return: 58.0, Time Step 5720, Episode Number 85, Epsilon 0.01\n",
            "Episodic Return: 42.0, Time Step 5762, Episode Number 86, Epsilon 0.01\n",
            "Episodic Return: 39.0, Time Step 5801, Episode Number 87, Epsilon 0.01\n",
            "Episodic Return: 35.0, Time Step 5836, Episode Number 88, Epsilon 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VupEVAlgN_Sj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
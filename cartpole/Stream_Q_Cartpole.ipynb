{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elichen/elichen.github.io/blob/main/cartpole/Stream_Q_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHneGiUqFkKX"
      },
      "source": [
        "# Stream Q($\\lambda$) algorithm with Cartpole:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krEplx1FFw-5"
      },
      "source": [
        "### 1. Install necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bPOAHpVEPPu8",
        "outputId": "9df36b42-4911-4d3e-e26a-ad8af1bf70a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==0.29.1)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 triton-2.3.0\n",
            "Collecting stable_baselines3==2.3.1\n",
            "  Downloading stable_baselines3-2.3.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (2.3.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3==2.3.1) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3==2.3.1) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3==2.3.1) (12.6.85)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3==2.3.1) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3==2.3.1) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3==2.3.1) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3==2.3.1) (1.3.0)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'stable-baselines3' candidate (version 2.3.1 at https://files.pythonhosted.org/packages/2a/59/e35fc90a805303b9f585342f4a3836b7b81924c760e00c66130cc64641e4/stable_baselines3-2.3.1-py3-none-any.whl (from https://pypi.org/simple/stable-baselines3/) (requires-python:>=3.8))\n",
            "Reason for being yanked: Loading broken with PyTorch 1.13\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading stable_baselines3-2.3.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.2/182.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stable_baselines3\n",
            "Successfully installed stable_baselines3-2.3.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (4.67.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (6.4.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2024.12.14)\n",
            "Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446667 sha256=4330dd25832f1afc8153fb029947df6fd7d5da33443fbc7a1dc1e74c7d5808fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libfontenc1 libglu1-mesa libxfont2 libxkbfile1 x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common\n",
            "Suggested packages:\n",
            "  libgle3 python3-numpy\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 libfontenc1 libglu1-mesa libxfont2 libxkbfile1 python3-opengl x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 12 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 8,639 kB of archives.\n",
            "After this operation, 20.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-opengl all 3.1.5+dfsg-1 [605 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12 [28.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.12 [864 kB]\n",
            "Fetched 8,639 kB in 2s (4,480 kB/s)\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../01-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../02-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../03-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../04-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package python3-opengl.\n",
            "Preparing to unpack .../05-python3-opengl_3.1.5+dfsg-1_all.deb ...\n",
            "Unpacking python3-opengl (3.1.5+dfsg-1) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../06-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../07-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../08-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../09-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../10-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../11-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up python3-opengl (3.1.5+dfsg-1) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium==0.29.1\n",
        "!pip install numpy==1.26.4\n",
        "!pip install torch==2.3.0\n",
        "!pip install stable_baselines3==2.3.1\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y python3-opengl xvfb ffmpeg\n",
        "!pip install pyvirtualdisplay imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5a5deBrGH7j"
      },
      "source": [
        "### 2. Import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NIRasg9NGHlO"
      },
      "outputs": [],
      "source": [
        "import torch, math\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import os, pickle, argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZXVIgUEF8_D"
      },
      "source": [
        "### 3. Implement the ObGD optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbqA3nRKPMPQ",
        "outputId": "620f1433-470b-44e3-ec15-da7200fc3215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class ObGD(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=1.0, gamma=0.99, lamda=0.8, kappa=2.0):\n",
        "        defaults = dict(lr=lr, gamma=gamma, lamda=lamda, kappa=kappa)\n",
        "        super(ObGD, self).__init__(params, defaults)\n",
        "    def step(self, delta, reset=False):\n",
        "        z_sum = 0.0\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state[\"eligibility_trace\"] = torch.zeros_like(p.data)\n",
        "                e = state[\"eligibility_trace\"]\n",
        "                e.mul_(group[\"gamma\"] * group[\"lamda\"]).add_(p.grad, alpha=1.0)\n",
        "                z_sum += e.abs().sum().item()\n",
        "\n",
        "        delta_bar = max(abs(delta), 1.0)\n",
        "        dot_product = delta_bar * z_sum * group[\"lr\"] * group[\"kappa\"]\n",
        "        if dot_product > 1:\n",
        "            step_size = group[\"lr\"] / dot_product\n",
        "        else:\n",
        "            step_size = group[\"lr\"]\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                state = self.state[p]\n",
        "                e = state[\"eligibility_trace\"]\n",
        "                p.data.add_(delta * e, alpha=-step_size)\n",
        "                if reset:\n",
        "                    e.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBknMm7_Gj9U"
      },
      "source": [
        "### 4. Implement sparse initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lj5Pk1MkPKTP"
      },
      "outputs": [],
      "source": [
        "def sparse_init(tensor, sparsity, type='uniform'):\n",
        "\n",
        "    if tensor.ndimension() == 2:\n",
        "        fan_out, fan_in = tensor.shape\n",
        "\n",
        "        num_zeros = int(math.ceil(sparsity * fan_in))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if type == 'uniform':\n",
        "                tensor.uniform_(-math.sqrt(1.0 / fan_in), math.sqrt(1.0 / fan_in))\n",
        "            elif type == 'normal':\n",
        "                tensor.normal_(0, math.sqrt(1.0 / fan_in))\n",
        "            else:\n",
        "                raise ValueError(\"Unknown initialization type\")\n",
        "            for col_idx in range(fan_out):\n",
        "                row_indices = torch.randperm(fan_in)\n",
        "                zero_indices = row_indices[:num_zeros]\n",
        "                tensor[col_idx, zero_indices] = 0\n",
        "        return tensor\n",
        "\n",
        "    elif tensor.ndimension() == 4:\n",
        "        channels_out, channels_in, h, w = tensor.shape\n",
        "        fan_in, fan_out = channels_in*h*w, channels_out*h*w\n",
        "\n",
        "        num_zeros = int(math.ceil(sparsity * fan_in))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if type == 'uniform':\n",
        "                tensor.uniform_(-math.sqrt(1.0 / fan_in), math.sqrt(1.0 / fan_in))\n",
        "            elif type == 'normal':\n",
        "                tensor.normal_(0, math.sqrt(1.0 / fan_in))\n",
        "            else:\n",
        "                raise ValueError(\"Unknown initialization type\")\n",
        "            for out_channel_idx in range(channels_out):\n",
        "                indices = torch.randperm(fan_in)\n",
        "                zero_indices = indices[:num_zeros]\n",
        "                tensor[out_channel_idx].reshape(channels_in*h*w)[zero_indices] = 0\n",
        "        return tensor\n",
        "    else:\n",
        "        raise ValueError(\"Only tensors with 2 or 4 dimensions are supported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFzKufnGqEK"
      },
      "source": [
        "### 5. Implement observation normalization and reward scaling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "na4mSpcMPGxg"
      },
      "outputs": [],
      "source": [
        "class SampleMeanStd:\n",
        "    def __init__(self, shape=()):\n",
        "        self.mean = np.zeros(shape, \"float64\")\n",
        "        self.var = np.ones(shape, \"float64\")\n",
        "        self.p = np.ones(shape, \"float64\")\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, x):\n",
        "        if self.count == 0:\n",
        "            self.mean = x\n",
        "            self.p = np.zeros_like(x)\n",
        "        self.mean, self.var, self.p, self.count = self.update_mean_var_count_from_moments(self.mean, self.p, self.count, x*1.0)\n",
        "\n",
        "    def update_mean_var_count_from_moments(self, mean, p, count, sample):\n",
        "        new_count = count + 1\n",
        "        new_mean = mean + (sample - mean) / new_count\n",
        "        p = p + (sample - mean) * (sample - new_mean)\n",
        "        new_var = 1 if new_count < 2 else p / (new_count - 1)\n",
        "        return new_mean, new_var, p, new_count\n",
        "\n",
        "class NormalizeObservation(gym.Wrapper, gym.utils.RecordConstructorArgs):\n",
        "    def __init__(self, env: gym.Env, epsilon: float = 1e-8):\n",
        "        gym.utils.RecordConstructorArgs.__init__(self, epsilon=epsilon)\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        try:\n",
        "            self.num_envs = self.get_wrapper_attr(\"num_envs\")\n",
        "            self.is_vector_env = self.get_wrapper_attr(\"is_vector_env\")\n",
        "        except AttributeError:\n",
        "            self.num_envs = 1\n",
        "            self.is_vector_env = False\n",
        "\n",
        "        if self.is_vector_env:\n",
        "            self.obs_stats = SampleMeanStd(shape=self.single_observation_space.shape)\n",
        "        else:\n",
        "            self.obs_stats = SampleMeanStd(shape=self.observation_space.shape)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        if self.is_vector_env:\n",
        "            obs = self.normalize(obs)\n",
        "        else:\n",
        "            obs = self.normalize(np.array([obs]))[0]\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        if self.is_vector_env:\n",
        "            return self.normalize(obs), info\n",
        "        else:\n",
        "            return self.normalize(np.array([obs]))[0], info\n",
        "\n",
        "    def normalize(self, obs):\n",
        "        self.obs_stats.update(obs)\n",
        "        return (obs - self.obs_stats.mean) / np.sqrt(self.obs_stats.var + self.epsilon)\n",
        "\n",
        "class ScaleReward(gym.core.Wrapper, gym.utils.RecordConstructorArgs):\n",
        "    def __init__(self, env: gym.Env, gamma: float = 0.99, epsilon: float = 1e-8):\n",
        "        gym.utils.RecordConstructorArgs.__init__(self, gamma=gamma, epsilon=epsilon)\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        try:\n",
        "            self.num_envs = self.get_wrapper_attr(\"num_envs\")\n",
        "            self.is_vector_env = self.get_wrapper_attr(\"is_vector_env\")\n",
        "        except AttributeError:\n",
        "            self.num_envs = 1\n",
        "            self.is_vector_env = False\n",
        "        self.reward_stats = SampleMeanStd(shape=())\n",
        "        self.reward_trace = np.zeros(self.num_envs)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        if not self.is_vector_env:\n",
        "            rews = np.array([rews])\n",
        "        term = terminateds or truncateds\n",
        "        self.reward_trace = self.reward_trace * self.gamma * (1 - term) + rews\n",
        "        rews = self.normalize(rews)\n",
        "        if not self.is_vector_env:\n",
        "            rews = rews[0]\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def normalize(self, rews):\n",
        "        self.reward_stats.update(self.reward_trace)\n",
        "        return rews / np.sqrt(self.reward_stats.var + self.epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx5um9DCG0CN"
      },
      "source": [
        "### 6. Implement the Stream Q($\\lambda$) agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bjwzfOFuE79l"
      },
      "outputs": [],
      "source": [
        "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "    slope = (end_e - start_e) / duration\n",
        "    return max(slope * t + start_e, end_e)\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        sparse_init(m.weight, sparsity=0.9)\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "class StreamQ(nn.Module):\n",
        "    def __init__(self, n_obs=11, n_actions=3, hidden_size=32, lr=1.0, epsilon_target=0.01, epsilon_start=1.0, exploration_fraction=0.1, total_steps=1_000_000, gamma=0.99, lamda=0.8, kappa_value=2.0):\n",
        "        super(StreamQ, self).__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_target = epsilon_target\n",
        "        self.epsilon = epsilon_start\n",
        "        self.exploration_fraction = exploration_fraction\n",
        "        self.total_steps = total_steps\n",
        "        self.time_step = 0\n",
        "        self.fc1_v   = nn.Linear(n_obs, hidden_size)\n",
        "        self.hidden_v  = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc_v  = nn.Linear(hidden_size, n_actions)\n",
        "        self.apply(initialize_weights)\n",
        "        self.optimizer = ObGD(list(self.parameters()), lr=lr, gamma=gamma, lamda=lamda, kappa=kappa_value)\n",
        "\n",
        "    def q(self, x):\n",
        "        x = self.fc1_v(x)\n",
        "        x = F.layer_norm(x, x.size())\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.hidden_v(x)\n",
        "        x = F.layer_norm(x, x.size())\n",
        "        x = F.leaky_relu(x)\n",
        "        return self.fc_v(x)\n",
        "\n",
        "    def sample_action(self, s):\n",
        "        self.time_step += 1\n",
        "        self.epsilon = linear_schedule(self.epsilon_start, self.epsilon_target, self.exploration_fraction * self.total_steps, self.time_step)\n",
        "        if isinstance(s, np.ndarray):\n",
        "            s = torch.tensor(np.array(s), dtype=torch.float)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            q_values = self.q(s)\n",
        "            greedy_action = torch.argmax(q_values, dim=-1).item()\n",
        "            random_action = np.random.randint(0, self.n_actions)\n",
        "            if greedy_action == random_action:\n",
        "                return random_action, False\n",
        "            else:\n",
        "                return random_action, True\n",
        "        else:\n",
        "            q_values = self.q(s)\n",
        "            return torch.argmax(q_values, dim=-1).item(), False\n",
        "\n",
        "    def update_params(self, s, a, r, s_prime, done, is_nongreedy, overshooting_info=False):\n",
        "        done_mask = 0 if done else 1\n",
        "        s, a, r, s_prime, done_mask = torch.tensor(np.array(s), dtype=torch.float), torch.tensor([a], dtype=torch.int).squeeze(0), \\\n",
        "                                         torch.tensor(np.array(r)), torch.tensor(np.array(s_prime), dtype=torch.float), \\\n",
        "                                         torch.tensor(np.array(done_mask), dtype=torch.float)\n",
        "        q_sa = self.q(s)[a]\n",
        "        max_q_s_prime_a_prime = torch.max(self.q(s_prime), dim=-1).values\n",
        "        td_target = r + self.gamma * max_q_s_prime_a_prime * done_mask\n",
        "        delta = td_target - q_sa\n",
        "\n",
        "        q_output = -q_sa\n",
        "        self.optimizer.zero_grad()\n",
        "        q_output.backward()\n",
        "        self.optimizer.step(delta.item(), reset=(done or is_nongreedy))\n",
        "\n",
        "        if overshooting_info:\n",
        "            max_q_s_prime_a_prime = torch.max(self.q(s_prime), dim=-1).values\n",
        "            td_target = r + self.gamma * max_q_s_prime_a_prime * done_mask\n",
        "            delta_bar = td_target - self.q(s)[a]\n",
        "            if torch.sign(delta_bar * delta).item() == -1:\n",
        "                print(\"Overshooting Detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "class AddTimeInfo(gym.core.Wrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__(env)\n",
        "        if self.env.num_envs > 1:\n",
        "            raise ValueError(\"AddTimeInfo only supports single environments\")\n",
        "        self.epi_time = -0.5\n",
        "        if 'dm_control' in env.spec.id:\n",
        "            self.time_limit = 1000\n",
        "        else:\n",
        "            self.time_limit = env.spec.max_episode_steps\n",
        "        self.obs_space_size = self.observation_space.shape[0] + self.env.num_envs\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_space_size,), dtype=np.float32)\n",
        "        if not (isinstance(self.action_space, gym.spaces.Box) or isinstance(self.action_space, gym.spaces.Discrete)):\n",
        "            raise ValueError(\"Unsupported action space\")\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        obs = np.concatenate((obs, np.array([self.epi_time] * self.env.num_envs)))\n",
        "        self.epi_time += 1.0 / self.time_limit\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.epi_time = -0.5\n",
        "        obs = np.concatenate((obs, np.array([self.epi_time])))\n",
        "        return obs, info"
      ],
      "metadata": {
        "id": "l1R1qHcLSLtF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqqI3tOXHDkA"
      },
      "source": [
        "### 7. Define agent-enviroment interaction loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RZMOUleDO6JN"
      },
      "outputs": [],
      "source": [
        "def agent_env_interaction(env_name, seed, lr, gamma, lamda, total_steps, epsilon_target, epsilon_start, exploration_fraction, kappa_value, debug, overshooting_info, render=False):\n",
        "    torch.manual_seed(seed); np.random.seed(seed)\n",
        "    env = gym.make(env_name, render_mode='human', max_episode_steps=10_000) if render else gym.make(env_name, max_episode_steps=10_000)\n",
        "    env = gym.wrappers.FlattenObservation(env)\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    env = ScaleReward(env, gamma=gamma)\n",
        "    env = NormalizeObservation(env)\n",
        "    env = AddTimeInfo(env)\n",
        "    agent = StreamQ(n_obs=env.observation_space.shape[0], n_actions=env.action_space.n, lr=lr, gamma=gamma, lamda=lamda, epsilon_target=epsilon_target, epsilon_start=epsilon_start, exploration_fraction=exploration_fraction, total_steps=total_steps, kappa_value=kappa_value)\n",
        "    if debug:\n",
        "        print(\"seed: {}\".format(seed), \"env: {}\".format(env.spec.id))\n",
        "    returns, term_time_steps = [], []\n",
        "    s, _ = env.reset(seed=seed)\n",
        "    episode_num = 1\n",
        "    for t in range(1, total_steps+1):\n",
        "        a, is_nongreedy = agent.sample_action(s)\n",
        "        s_prime, r, terminated, truncated, info = env.step(a)\n",
        "        agent.update_params(s, a, r, s_prime, terminated or truncated, is_nongreedy, overshooting_info)\n",
        "        s = s_prime\n",
        "        if terminated or truncated:\n",
        "            if debug:\n",
        "                print(\"Episodic Return: {}, Time Step {}, Episode Number {}, Epsilon {}\".format(info['episode']['r'][0], t, episode_num, agent.epsilon))\n",
        "            returns.append(info['episode']['r'][0])\n",
        "            term_time_steps.append(t)\n",
        "            terminated, truncated = False, False\n",
        "            s, _ = env.reset()\n",
        "            episode_num += 1\n",
        "    env.close()\n",
        "    save_dir = \"data_stream_q_{}_lr{}_gamma{}_lamda{}\".format(env.spec.id, lr, gamma, lamda)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    with open(os.path.join(save_dir, \"seed_{}.pkl\".format(seed)), \"wb\") as f:\n",
        "        pickle.dump((returns, term_time_steps, env_name), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMOfOLz4G790"
      },
      "source": [
        "### 8. Set hyperparameters and start the interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-P6-raVKTnH",
        "outputId": "1d9c947f-fa6f-4ec7-9e4b-9a9d787055c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed: 0 env: CartPole-v1\n",
            "Episodic Return: 18.0, Time Step 18, Episode Number 1, Epsilon 0.996436\n",
            "Episodic Return: 37.0, Time Step 55, Episode Number 2, Epsilon 0.98911\n",
            "Episodic Return: 13.0, Time Step 68, Episode Number 3, Epsilon 0.986536\n",
            "Episodic Return: 22.0, Time Step 90, Episode Number 4, Epsilon 0.98218\n",
            "Episodic Return: 38.0, Time Step 128, Episode Number 5, Epsilon 0.974656\n",
            "Episodic Return: 16.0, Time Step 144, Episode Number 6, Epsilon 0.971488\n",
            "Episodic Return: 18.0, Time Step 162, Episode Number 7, Epsilon 0.967924\n",
            "Episodic Return: 15.0, Time Step 177, Episode Number 8, Epsilon 0.964954\n",
            "Episodic Return: 34.0, Time Step 211, Episode Number 9, Epsilon 0.958222\n",
            "Episodic Return: 30.0, Time Step 241, Episode Number 10, Epsilon 0.952282\n",
            "Episodic Return: 19.0, Time Step 260, Episode Number 11, Epsilon 0.94852\n",
            "Episodic Return: 15.0, Time Step 275, Episode Number 12, Epsilon 0.94555\n",
            "Episodic Return: 13.0, Time Step 288, Episode Number 13, Epsilon 0.942976\n",
            "Episodic Return: 13.0, Time Step 301, Episode Number 14, Epsilon 0.940402\n",
            "Episodic Return: 35.0, Time Step 336, Episode Number 15, Epsilon 0.933472\n",
            "Episodic Return: 56.0, Time Step 392, Episode Number 16, Epsilon 0.922384\n",
            "Episodic Return: 34.0, Time Step 426, Episode Number 17, Epsilon 0.915652\n",
            "Episodic Return: 53.0, Time Step 479, Episode Number 18, Epsilon 0.905158\n",
            "Episodic Return: 12.0, Time Step 491, Episode Number 19, Epsilon 0.902782\n",
            "Episodic Return: 11.0, Time Step 502, Episode Number 20, Epsilon 0.900604\n",
            "Episodic Return: 31.0, Time Step 533, Episode Number 21, Epsilon 0.894466\n",
            "Episodic Return: 14.0, Time Step 547, Episode Number 22, Epsilon 0.891694\n",
            "Episodic Return: 34.0, Time Step 581, Episode Number 23, Epsilon 0.884962\n",
            "Episodic Return: 26.0, Time Step 607, Episode Number 24, Epsilon 0.879814\n",
            "Episodic Return: 16.0, Time Step 623, Episode Number 25, Epsilon 0.876646\n",
            "Episodic Return: 43.0, Time Step 666, Episode Number 26, Epsilon 0.868132\n",
            "Episodic Return: 39.0, Time Step 705, Episode Number 27, Epsilon 0.86041\n",
            "Episodic Return: 19.0, Time Step 724, Episode Number 28, Epsilon 0.8566480000000001\n",
            "Episodic Return: 55.0, Time Step 779, Episode Number 29, Epsilon 0.845758\n",
            "Episodic Return: 15.0, Time Step 794, Episode Number 30, Epsilon 0.842788\n",
            "Episodic Return: 11.0, Time Step 805, Episode Number 31, Epsilon 0.84061\n",
            "Episodic Return: 50.0, Time Step 855, Episode Number 32, Epsilon 0.8307100000000001\n",
            "Episodic Return: 21.0, Time Step 876, Episode Number 33, Epsilon 0.826552\n",
            "Episodic Return: 43.0, Time Step 919, Episode Number 34, Epsilon 0.818038\n",
            "Episodic Return: 14.0, Time Step 933, Episode Number 35, Epsilon 0.815266\n",
            "Episodic Return: 26.0, Time Step 959, Episode Number 36, Epsilon 0.810118\n",
            "Episodic Return: 17.0, Time Step 976, Episode Number 37, Epsilon 0.806752\n",
            "Episodic Return: 52.0, Time Step 1028, Episode Number 38, Epsilon 0.796456\n",
            "Episodic Return: 26.0, Time Step 1054, Episode Number 39, Epsilon 0.791308\n",
            "Episodic Return: 22.0, Time Step 1076, Episode Number 40, Epsilon 0.786952\n",
            "Episodic Return: 18.0, Time Step 1094, Episode Number 41, Epsilon 0.783388\n",
            "Episodic Return: 30.0, Time Step 1124, Episode Number 42, Epsilon 0.777448\n",
            "Episodic Return: 50.0, Time Step 1174, Episode Number 43, Epsilon 0.767548\n",
            "Episodic Return: 28.0, Time Step 1202, Episode Number 44, Epsilon 0.762004\n",
            "Episodic Return: 56.0, Time Step 1258, Episode Number 45, Epsilon 0.750916\n",
            "Episodic Return: 28.0, Time Step 1286, Episode Number 46, Epsilon 0.745372\n",
            "Episodic Return: 92.0, Time Step 1378, Episode Number 47, Epsilon 0.727156\n",
            "Episodic Return: 40.0, Time Step 1418, Episode Number 48, Epsilon 0.719236\n",
            "Episodic Return: 21.0, Time Step 1439, Episode Number 49, Epsilon 0.715078\n",
            "Episodic Return: 37.0, Time Step 1476, Episode Number 50, Epsilon 0.7077519999999999\n",
            "Episodic Return: 18.0, Time Step 1494, Episode Number 51, Epsilon 0.704188\n",
            "Episodic Return: 30.0, Time Step 1524, Episode Number 52, Epsilon 0.698248\n",
            "Episodic Return: 39.0, Time Step 1563, Episode Number 53, Epsilon 0.690526\n",
            "Episodic Return: 40.0, Time Step 1603, Episode Number 54, Epsilon 0.682606\n",
            "Episodic Return: 63.0, Time Step 1666, Episode Number 55, Epsilon 0.670132\n",
            "Episodic Return: 32.0, Time Step 1698, Episode Number 56, Epsilon 0.663796\n",
            "Episodic Return: 243.0, Time Step 1941, Episode Number 57, Epsilon 0.6156820000000001\n",
            "Episodic Return: 19.0, Time Step 1960, Episode Number 58, Epsilon 0.61192\n",
            "Episodic Return: 14.0, Time Step 1974, Episode Number 59, Epsilon 0.609148\n",
            "Episodic Return: 27.0, Time Step 2001, Episode Number 60, Epsilon 0.603802\n",
            "Episodic Return: 29.0, Time Step 2030, Episode Number 61, Epsilon 0.59806\n",
            "Episodic Return: 72.0, Time Step 2102, Episode Number 62, Epsilon 0.583804\n",
            "Episodic Return: 28.0, Time Step 2130, Episode Number 63, Epsilon 0.57826\n",
            "Episodic Return: 231.0, Time Step 2361, Episode Number 64, Epsilon 0.532522\n",
            "Episodic Return: 161.0, Time Step 2522, Episode Number 65, Epsilon 0.5006440000000001\n",
            "Episodic Return: 41.0, Time Step 2563, Episode Number 66, Epsilon 0.492526\n",
            "Episodic Return: 95.0, Time Step 2658, Episode Number 67, Epsilon 0.473716\n",
            "Episodic Return: 77.0, Time Step 2735, Episode Number 68, Epsilon 0.45847000000000004\n",
            "Episodic Return: 347.0, Time Step 3082, Episode Number 69, Epsilon 0.389764\n",
            "Episodic Return: 72.0, Time Step 3154, Episode Number 70, Epsilon 0.37550800000000006\n",
            "Episodic Return: 121.0, Time Step 3275, Episode Number 71, Epsilon 0.35155000000000003\n",
            "Episodic Return: 85.0, Time Step 3360, Episode Number 72, Epsilon 0.33472\n",
            "Episodic Return: 12.0, Time Step 3372, Episode Number 73, Epsilon 0.3323440000000001\n",
            "Episodic Return: 70.0, Time Step 3442, Episode Number 74, Epsilon 0.318484\n",
            "Episodic Return: 122.0, Time Step 3564, Episode Number 75, Epsilon 0.29432800000000003\n",
            "Episodic Return: 98.0, Time Step 3662, Episode Number 76, Epsilon 0.27492400000000006\n",
            "Episodic Return: 72.0, Time Step 3734, Episode Number 77, Epsilon 0.260668\n",
            "Episodic Return: 98.0, Time Step 3832, Episode Number 78, Epsilon 0.24126400000000003\n",
            "Episodic Return: 126.0, Time Step 3958, Episode Number 79, Epsilon 0.21631600000000006\n",
            "Episodic Return: 32.0, Time Step 3990, Episode Number 80, Epsilon 0.20998000000000006\n",
            "Episodic Return: 121.0, Time Step 4111, Episode Number 81, Epsilon 0.18602200000000002\n",
            "Episodic Return: 48.0, Time Step 4159, Episode Number 82, Epsilon 0.17651800000000006\n",
            "Episodic Return: 40.0, Time Step 4199, Episode Number 83, Epsilon 0.16859800000000003\n",
            "Episodic Return: 39.0, Time Step 4238, Episode Number 84, Epsilon 0.16087600000000002\n",
            "Episodic Return: 38.0, Time Step 4276, Episode Number 85, Epsilon 0.15335200000000004\n",
            "Episodic Return: 13.0, Time Step 4289, Episode Number 86, Epsilon 0.15077800000000008\n",
            "Episodic Return: 14.0, Time Step 4303, Episode Number 87, Epsilon 0.14800600000000008\n",
            "Episodic Return: 9.0, Time Step 4312, Episode Number 88, Epsilon 0.14622400000000002\n",
            "Episodic Return: 25.0, Time Step 4337, Episode Number 89, Epsilon 0.141274\n",
            "Episodic Return: 12.0, Time Step 4349, Episode Number 90, Epsilon 0.13889800000000008\n",
            "Episodic Return: 9.0, Time Step 4358, Episode Number 91, Epsilon 0.13711600000000002\n",
            "Episodic Return: 24.0, Time Step 4382, Episode Number 92, Epsilon 0.13236400000000004\n",
            "Episodic Return: 35.0, Time Step 4417, Episode Number 93, Epsilon 0.12543400000000005\n",
            "Episodic Return: 39.0, Time Step 4456, Episode Number 94, Epsilon 0.11771200000000004\n",
            "Episodic Return: 16.0, Time Step 4472, Episode Number 95, Epsilon 0.11454400000000009\n",
            "Episodic Return: 37.0, Time Step 4509, Episode Number 96, Epsilon 0.10721800000000004\n",
            "Episodic Return: 59.0, Time Step 4568, Episode Number 97, Epsilon 0.09553600000000007\n",
            "Episodic Return: 63.0, Time Step 4631, Episode Number 98, Epsilon 0.08306200000000008\n",
            "Episodic Return: 50.0, Time Step 4681, Episode Number 99, Epsilon 0.07316200000000006\n",
            "Episodic Return: 94.0, Time Step 4775, Episode Number 100, Epsilon 0.0545500000000001\n",
            "Episodic Return: 101.0, Time Step 4876, Episode Number 101, Epsilon 0.03455200000000003\n",
            "Episodic Return: 98.0, Time Step 4974, Episode Number 102, Epsilon 0.01514800000000005\n",
            "Episodic Return: 111.0, Time Step 5085, Episode Number 103, Epsilon 0.01\n",
            "Episodic Return: 121.0, Time Step 5206, Episode Number 104, Epsilon 0.01\n",
            "Episodic Return: 85.0, Time Step 5291, Episode Number 105, Epsilon 0.01\n",
            "Episodic Return: 19.0, Time Step 5310, Episode Number 106, Epsilon 0.01\n",
            "Episodic Return: 76.0, Time Step 5386, Episode Number 107, Epsilon 0.01\n",
            "Episodic Return: 66.0, Time Step 5452, Episode Number 108, Epsilon 0.01\n",
            "Episodic Return: 15.0, Time Step 5467, Episode Number 109, Epsilon 0.01\n",
            "Episodic Return: 64.0, Time Step 5531, Episode Number 110, Epsilon 0.01\n",
            "Episodic Return: 56.0, Time Step 5587, Episode Number 111, Epsilon 0.01\n",
            "Episodic Return: 56.0, Time Step 5643, Episode Number 112, Epsilon 0.01\n",
            "Episodic Return: 47.0, Time Step 5690, Episode Number 113, Epsilon 0.01\n",
            "Episodic Return: 52.0, Time Step 5742, Episode Number 114, Epsilon 0.01\n",
            "Episodic Return: 71.0, Time Step 5813, Episode Number 115, Epsilon 0.01\n",
            "Episodic Return: 85.0, Time Step 5898, Episode Number 116, Epsilon 0.01\n",
            "Episodic Return: 13.0, Time Step 5911, Episode Number 117, Epsilon 0.01\n",
            "Episodic Return: 27.0, Time Step 5938, Episode Number 118, Epsilon 0.01\n",
            "Episodic Return: 107.0, Time Step 6045, Episode Number 119, Epsilon 0.01\n",
            "Episodic Return: 62.0, Time Step 6107, Episode Number 120, Epsilon 0.01\n",
            "Episodic Return: 110.0, Time Step 6217, Episode Number 121, Epsilon 0.01\n",
            "Episodic Return: 81.0, Time Step 6298, Episode Number 122, Epsilon 0.01\n",
            "Episodic Return: 93.0, Time Step 6391, Episode Number 123, Epsilon 0.01\n",
            "Episodic Return: 92.0, Time Step 6483, Episode Number 124, Epsilon 0.01\n",
            "Episodic Return: 94.0, Time Step 6577, Episode Number 125, Epsilon 0.01\n",
            "Episodic Return: 24.0, Time Step 6601, Episode Number 126, Epsilon 0.01\n",
            "Episodic Return: 98.0, Time Step 6699, Episode Number 127, Epsilon 0.01\n",
            "Episodic Return: 95.0, Time Step 6794, Episode Number 128, Epsilon 0.01\n",
            "Episodic Return: 25.0, Time Step 6819, Episode Number 129, Epsilon 0.01\n",
            "Episodic Return: 109.0, Time Step 6928, Episode Number 130, Epsilon 0.01\n",
            "Episodic Return: 35.0, Time Step 6963, Episode Number 131, Epsilon 0.01\n",
            "Episodic Return: 118.0, Time Step 7081, Episode Number 132, Epsilon 0.01\n",
            "Episodic Return: 117.0, Time Step 7198, Episode Number 133, Epsilon 0.01\n",
            "Episodic Return: 25.0, Time Step 7223, Episode Number 134, Epsilon 0.01\n",
            "Episodic Return: 116.0, Time Step 7339, Episode Number 135, Epsilon 0.01\n",
            "Episodic Return: 118.0, Time Step 7457, Episode Number 136, Epsilon 0.01\n",
            "Episodic Return: 9.0, Time Step 7466, Episode Number 137, Epsilon 0.01\n",
            "Episodic Return: 80.0, Time Step 7546, Episode Number 138, Epsilon 0.01\n",
            "Episodic Return: 82.0, Time Step 7628, Episode Number 139, Epsilon 0.01\n",
            "Episodic Return: 31.0, Time Step 7659, Episode Number 140, Epsilon 0.01\n",
            "Episodic Return: 89.0, Time Step 7748, Episode Number 141, Epsilon 0.01\n",
            "Episodic Return: 95.0, Time Step 7843, Episode Number 142, Epsilon 0.01\n",
            "Episodic Return: 102.0, Time Step 7945, Episode Number 143, Epsilon 0.01\n",
            "Episodic Return: 97.0, Time Step 8042, Episode Number 144, Epsilon 0.01\n",
            "Episodic Return: 13.0, Time Step 8055, Episode Number 145, Epsilon 0.01\n",
            "Episodic Return: 90.0, Time Step 8145, Episode Number 146, Epsilon 0.01\n",
            "Episodic Return: 99.0, Time Step 8244, Episode Number 147, Epsilon 0.01\n",
            "Episodic Return: 110.0, Time Step 8354, Episode Number 148, Epsilon 0.01\n",
            "Episodic Return: 113.0, Time Step 8467, Episode Number 149, Epsilon 0.01\n",
            "Episodic Return: 102.0, Time Step 8569, Episode Number 150, Epsilon 0.01\n",
            "Episodic Return: 96.0, Time Step 8665, Episode Number 151, Epsilon 0.01\n",
            "Episodic Return: 84.0, Time Step 8749, Episode Number 152, Epsilon 0.01\n",
            "Episodic Return: 82.0, Time Step 8831, Episode Number 153, Epsilon 0.01\n",
            "Episodic Return: 81.0, Time Step 8912, Episode Number 154, Epsilon 0.01\n",
            "Episodic Return: 65.0, Time Step 8977, Episode Number 155, Epsilon 0.01\n",
            "Episodic Return: 67.0, Time Step 9044, Episode Number 156, Epsilon 0.01\n",
            "Episodic Return: 85.0, Time Step 9129, Episode Number 157, Epsilon 0.01\n",
            "Episodic Return: 91.0, Time Step 9220, Episode Number 158, Epsilon 0.01\n",
            "Episodic Return: 93.0, Time Step 9313, Episode Number 159, Epsilon 0.01\n",
            "Episodic Return: 99.0, Time Step 9412, Episode Number 160, Epsilon 0.01\n",
            "Episodic Return: 141.0, Time Step 9553, Episode Number 161, Epsilon 0.01\n",
            "Episodic Return: 8201.0, Time Step 17754, Episode Number 162, Epsilon 0.01\n",
            "Episodic Return: 204.0, Time Step 17958, Episode Number 163, Epsilon 0.01\n",
            "Episodic Return: 7417.0, Time Step 25375, Episode Number 164, Epsilon 0.01\n",
            "Episodic Return: 442.0, Time Step 25817, Episode Number 165, Epsilon 0.01\n",
            "Episodic Return: 409.0, Time Step 26226, Episode Number 166, Epsilon 0.01\n",
            "Episodic Return: 412.0, Time Step 26638, Episode Number 167, Epsilon 0.01\n",
            "Episodic Return: 122.0, Time Step 26760, Episode Number 168, Epsilon 0.01\n",
            "Episodic Return: 3757.0, Time Step 30517, Episode Number 169, Epsilon 0.01\n",
            "Episodic Return: 18.0, Time Step 30535, Episode Number 170, Epsilon 0.01\n",
            "Episodic Return: 198.0, Time Step 30733, Episode Number 171, Epsilon 0.01\n",
            "Episodic Return: 120.0, Time Step 30853, Episode Number 172, Epsilon 0.01\n",
            "Episodic Return: 189.0, Time Step 31042, Episode Number 173, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 41042, Episode Number 174, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 51042, Episode Number 175, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 61042, Episode Number 176, Epsilon 0.01\n",
            "Episodic Return: 304.0, Time Step 61346, Episode Number 177, Epsilon 0.01\n",
            "Episodic Return: 168.0, Time Step 61514, Episode Number 178, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 71514, Episode Number 179, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 81514, Episode Number 180, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 91514, Episode Number 181, Epsilon 0.01\n"
          ]
        }
      ],
      "source": [
        "agent_env_interaction(\n",
        "    env_name='CartPole-v1',\n",
        "                      seed=0,\n",
        "                      lr=1.0,\n",
        "                      gamma=0.99,\n",
        "                      lamda=0.8,\n",
        "                      total_steps=100_000,\n",
        "                      epsilon_target=0.01,\n",
        "                      epsilon_start=1.0,\n",
        "                      exploration_fraction=0.05,\n",
        "                      kappa_value=2.0,\n",
        "                      debug=True,\n",
        "                      overshooting_info=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKBMPrNReH7W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elichen/elichen.github.io/blob/main/cartpole/Stream_Q_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHneGiUqFkKX"
      },
      "source": [
        "# Stream Q($\\lambda$) algorithm with Cartpole:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krEplx1FFw-5"
      },
      "source": [
        "### 1. Install necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bPOAHpVEPPu8",
        "outputId": "630912a1-216e-48ba-e7bf-450a1c4ee2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium==0.29.1 --quiet\n",
        "!pip install numpy==1.26.4 --quiet\n",
        "!pip install torch==2.3.0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5a5deBrGH7j"
      },
      "source": [
        "### 2. Import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NIRasg9NGHlO"
      },
      "outputs": [],
      "source": [
        "import torch, math\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import os, pickle, argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZXVIgUEF8_D"
      },
      "source": [
        "### 3. Implement the ObGD optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xbqA3nRKPMPQ"
      },
      "outputs": [],
      "source": [
        "class ObGD(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=1.0, gamma=0.99, lamda=0.8, kappa=2.0):\n",
        "        defaults = dict(lr=lr, gamma=gamma, lamda=lamda, kappa=kappa)\n",
        "        super(ObGD, self).__init__(params, defaults)\n",
        "    def step(self, delta, reset=False):\n",
        "        z_sum = 0.0\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state[\"eligibility_trace\"] = torch.zeros_like(p.data)\n",
        "                e = state[\"eligibility_trace\"]\n",
        "                e.mul_(group[\"gamma\"] * group[\"lamda\"]).add_(p.grad, alpha=1.0)\n",
        "                z_sum += e.abs().sum().item()\n",
        "\n",
        "        delta_bar = max(abs(delta), 1.0)\n",
        "        dot_product = delta_bar * z_sum * group[\"lr\"] * group[\"kappa\"]\n",
        "        if dot_product > 1:\n",
        "            step_size = group[\"lr\"] / dot_product\n",
        "        else:\n",
        "            step_size = group[\"lr\"]\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                state = self.state[p]\n",
        "                e = state[\"eligibility_trace\"]\n",
        "                p.data.add_(delta * e, alpha=-step_size)\n",
        "                if reset:\n",
        "                    e.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBknMm7_Gj9U"
      },
      "source": [
        "### 4. Implement sparse initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lj5Pk1MkPKTP"
      },
      "outputs": [],
      "source": [
        "def sparse_init(tensor, sparsity, type='uniform'):\n",
        "\n",
        "    if tensor.ndimension() == 2:\n",
        "        fan_out, fan_in = tensor.shape\n",
        "\n",
        "        num_zeros = int(math.ceil(sparsity * fan_in))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if type == 'uniform':\n",
        "                tensor.uniform_(-math.sqrt(1.0 / fan_in), math.sqrt(1.0 / fan_in))\n",
        "            elif type == 'normal':\n",
        "                tensor.normal_(0, math.sqrt(1.0 / fan_in))\n",
        "            else:\n",
        "                raise ValueError(\"Unknown initialization type\")\n",
        "            for col_idx in range(fan_out):\n",
        "                row_indices = torch.randperm(fan_in)\n",
        "                zero_indices = row_indices[:num_zeros]\n",
        "                tensor[col_idx, zero_indices] = 0\n",
        "        return tensor\n",
        "\n",
        "    elif tensor.ndimension() == 4:\n",
        "        channels_out, channels_in, h, w = tensor.shape\n",
        "        fan_in, fan_out = channels_in*h*w, channels_out*h*w\n",
        "\n",
        "        num_zeros = int(math.ceil(sparsity * fan_in))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if type == 'uniform':\n",
        "                tensor.uniform_(-math.sqrt(1.0 / fan_in), math.sqrt(1.0 / fan_in))\n",
        "            elif type == 'normal':\n",
        "                tensor.normal_(0, math.sqrt(1.0 / fan_in))\n",
        "            else:\n",
        "                raise ValueError(\"Unknown initialization type\")\n",
        "            for out_channel_idx in range(channels_out):\n",
        "                indices = torch.randperm(fan_in)\n",
        "                zero_indices = indices[:num_zeros]\n",
        "                tensor[out_channel_idx].reshape(channels_in*h*w)[zero_indices] = 0\n",
        "        return tensor\n",
        "    else:\n",
        "        raise ValueError(\"Only tensors with 2 or 4 dimensions are supported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFzKufnGqEK"
      },
      "source": [
        "### 5. Implement observation normalization and reward scaling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "na4mSpcMPGxg"
      },
      "outputs": [],
      "source": [
        "class SampleMeanStd:\n",
        "    def __init__(self, shape=()):\n",
        "        self.mean = np.zeros(shape, \"float64\")\n",
        "        self.var = np.ones(shape, \"float64\")\n",
        "        self.p = np.ones(shape, \"float64\")\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, x):\n",
        "        if self.count == 0:\n",
        "            self.mean = x\n",
        "            self.p = np.zeros_like(x)\n",
        "        self.mean, self.var, self.p, self.count = self.update_mean_var_count_from_moments(self.mean, self.p, self.count, x*1.0)\n",
        "\n",
        "    def update_mean_var_count_from_moments(self, mean, p, count, sample):\n",
        "        new_count = count + 1\n",
        "        new_mean = mean + (sample - mean) / new_count\n",
        "        p = p + (sample - mean) * (sample - new_mean)\n",
        "        new_var = 1 if new_count < 2 else p / (new_count - 1)\n",
        "        return new_mean, new_var, p, new_count\n",
        "\n",
        "class NormalizeObservation(gym.Wrapper, gym.utils.RecordConstructorArgs):\n",
        "    def __init__(self, env: gym.Env, epsilon: float = 1e-8):\n",
        "        gym.utils.RecordConstructorArgs.__init__(self, epsilon=epsilon)\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        try:\n",
        "            self.num_envs = self.get_wrapper_attr(\"num_envs\")\n",
        "            self.is_vector_env = self.get_wrapper_attr(\"is_vector_env\")\n",
        "        except AttributeError:\n",
        "            self.num_envs = 1\n",
        "            self.is_vector_env = False\n",
        "\n",
        "        if self.is_vector_env:\n",
        "            self.obs_stats = SampleMeanStd(shape=self.single_observation_space.shape)\n",
        "        else:\n",
        "            self.obs_stats = SampleMeanStd(shape=self.observation_space.shape)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        if self.is_vector_env:\n",
        "            obs = self.normalize(obs)\n",
        "        else:\n",
        "            obs = self.normalize(np.array([obs]))[0]\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        if self.is_vector_env:\n",
        "            return self.normalize(obs), info\n",
        "        else:\n",
        "            return self.normalize(np.array([obs]))[0], info\n",
        "\n",
        "    def normalize(self, obs):\n",
        "        self.obs_stats.update(obs)\n",
        "        return (obs - self.obs_stats.mean) / np.sqrt(self.obs_stats.var + self.epsilon)\n",
        "\n",
        "class ScaleReward(gym.core.Wrapper, gym.utils.RecordConstructorArgs):\n",
        "    def __init__(self, env: gym.Env, gamma: float = 0.99, epsilon: float = 1e-8):\n",
        "        gym.utils.RecordConstructorArgs.__init__(self, gamma=gamma, epsilon=epsilon)\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        try:\n",
        "            self.num_envs = self.get_wrapper_attr(\"num_envs\")\n",
        "            self.is_vector_env = self.get_wrapper_attr(\"is_vector_env\")\n",
        "        except AttributeError:\n",
        "            self.num_envs = 1\n",
        "            self.is_vector_env = False\n",
        "        self.reward_stats = SampleMeanStd(shape=())\n",
        "        self.reward_trace = np.zeros(self.num_envs)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        if not self.is_vector_env:\n",
        "            rews = np.array([rews])\n",
        "        term = terminateds or truncateds\n",
        "        self.reward_trace = self.reward_trace * self.gamma * (1 - term) + rews\n",
        "        rews = self.normalize(rews)\n",
        "        if not self.is_vector_env:\n",
        "            rews = rews[0]\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def normalize(self, rews):\n",
        "        self.reward_stats.update(self.reward_trace)\n",
        "        return rews / np.sqrt(self.reward_stats.var + self.epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx5um9DCG0CN"
      },
      "source": [
        "### 6. Implement the Stream Q($\\lambda$) agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bjwzfOFuE79l"
      },
      "outputs": [],
      "source": [
        "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "    slope = (end_e - start_e) / duration\n",
        "    return max(slope * t + start_e, end_e)\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        sparse_init(m.weight, sparsity=0.9)\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "class StreamQ(nn.Module):\n",
        "    def __init__(self, n_obs=11, n_actions=3, hidden_size=32, lr=1.0, epsilon_target=0.01, epsilon_start=1.0, exploration_fraction=0.1, total_steps=1_000_000, gamma=0.99, lamda=0.8, kappa_value=2.0):\n",
        "        super(StreamQ, self).__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_target = epsilon_target\n",
        "        self.epsilon = epsilon_start\n",
        "        self.exploration_fraction = exploration_fraction\n",
        "        self.total_steps = total_steps\n",
        "        self.time_step = 0\n",
        "        self.fc1_v   = nn.Linear(n_obs, hidden_size)\n",
        "        self.hidden_v  = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc_v  = nn.Linear(hidden_size, n_actions)\n",
        "        self.apply(initialize_weights)\n",
        "        self.optimizer = ObGD(list(self.parameters()), lr=lr, gamma=gamma, lamda=lamda, kappa=kappa_value)\n",
        "\n",
        "    def q(self, x):\n",
        "        x = self.fc1_v(x)\n",
        "        x = F.layer_norm(x, x.size())\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.hidden_v(x)\n",
        "        x = F.layer_norm(x, x.size())\n",
        "        x = F.leaky_relu(x)\n",
        "        return self.fc_v(x)\n",
        "\n",
        "    def sample_action(self, s):\n",
        "        self.time_step += 1\n",
        "        self.epsilon = linear_schedule(self.epsilon_start, self.epsilon_target, self.exploration_fraction * self.total_steps, self.time_step)\n",
        "        if isinstance(s, np.ndarray):\n",
        "            s = torch.tensor(np.array(s), dtype=torch.float)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            q_values = self.q(s)\n",
        "            greedy_action = torch.argmax(q_values, dim=-1).item()\n",
        "            random_action = np.random.randint(0, self.n_actions)\n",
        "            if greedy_action == random_action:\n",
        "                return random_action, False\n",
        "            else:\n",
        "                return random_action, True\n",
        "        else:\n",
        "            q_values = self.q(s)\n",
        "            return torch.argmax(q_values, dim=-1).item(), False\n",
        "\n",
        "    def update_params(self, s, a, r, s_prime, done, is_nongreedy, overshooting_info=False):\n",
        "        done_mask = 0 if done else 1\n",
        "        s, a, r, s_prime, done_mask = torch.tensor(np.array(s), dtype=torch.float), torch.tensor([a], dtype=torch.int).squeeze(0), \\\n",
        "                                         torch.tensor(np.array(r)), torch.tensor(np.array(s_prime), dtype=torch.float), \\\n",
        "                                         torch.tensor(np.array(done_mask), dtype=torch.float)\n",
        "        q_sa = self.q(s)[a]\n",
        "        max_q_s_prime_a_prime = torch.max(self.q(s_prime), dim=-1).values\n",
        "        td_target = r + self.gamma * max_q_s_prime_a_prime * done_mask\n",
        "        delta = td_target - q_sa\n",
        "\n",
        "        q_output = -q_sa\n",
        "        self.optimizer.zero_grad()\n",
        "        q_output.backward()\n",
        "        self.optimizer.step(delta.item(), reset=(done or is_nongreedy))\n",
        "\n",
        "        if overshooting_info:\n",
        "            max_q_s_prime_a_prime = torch.max(self.q(s_prime), dim=-1).values\n",
        "            td_target = r + self.gamma * max_q_s_prime_a_prime * done_mask\n",
        "            delta_bar = td_target - self.q(s)[a]\n",
        "            if torch.sign(delta_bar * delta).item() == -1:\n",
        "                print(\"Overshooting Detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "class AddTimeInfo(gym.core.Wrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__(env)\n",
        "        if self.env.num_envs > 1:\n",
        "            raise ValueError(\"AddTimeInfo only supports single environments\")\n",
        "        self.epi_time = -0.5\n",
        "        if 'dm_control' in env.spec.id:\n",
        "            self.time_limit = 1000\n",
        "        else:\n",
        "            self.time_limit = env.spec.max_episode_steps\n",
        "        self.obs_space_size = self.observation_space.shape[0] + self.env.num_envs\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_space_size,), dtype=np.float32)\n",
        "        if not (isinstance(self.action_space, gym.spaces.Box) or isinstance(self.action_space, gym.spaces.Discrete)):\n",
        "            raise ValueError(\"Unsupported action space\")\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        obs = np.concatenate((obs, np.array([self.epi_time] * self.env.num_envs)))\n",
        "        self.epi_time += 1.0 / self.time_limit\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.epi_time = -0.5\n",
        "        obs = np.concatenate((obs, np.array([self.epi_time])))\n",
        "        return obs, info"
      ],
      "metadata": {
        "id": "l1R1qHcLSLtF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqqI3tOXHDkA"
      },
      "source": [
        "### 7. Define agent-enviroment interaction loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RZMOUleDO6JN"
      },
      "outputs": [],
      "source": [
        "def agent_env_interaction(env_name, seed, lr, gamma, lamda, total_steps, epsilon_target, epsilon_start, exploration_fraction, kappa_value, debug, overshooting_info, render=False):\n",
        "    torch.manual_seed(seed); np.random.seed(seed)\n",
        "    env = gym.make(env_name, render_mode='human', max_episode_steps=10_000) if render else gym.make(env_name, max_episode_steps=10_000)\n",
        "    env = gym.wrappers.FlattenObservation(env)\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    env = ScaleReward(env, gamma=gamma)\n",
        "    env = NormalizeObservation(env)\n",
        "    env = AddTimeInfo(env)\n",
        "    agent = StreamQ(n_obs=env.observation_space.shape[0], n_actions=env.action_space.n, lr=lr, gamma=gamma, lamda=lamda, epsilon_target=epsilon_target, epsilon_start=epsilon_start, exploration_fraction=exploration_fraction, total_steps=total_steps, kappa_value=kappa_value)\n",
        "    if debug:\n",
        "        print(\"seed: {}\".format(seed), \"env: {}\".format(env.spec.id))\n",
        "    returns, term_time_steps = [], []\n",
        "    s, _ = env.reset(seed=seed)\n",
        "    episode_num = 1\n",
        "    for t in range(1, total_steps+1):\n",
        "        a, is_nongreedy = agent.sample_action(s)\n",
        "        s_prime, r, terminated, truncated, info = env.step(a)\n",
        "        agent.update_params(s, a, r, s_prime, terminated or truncated, is_nongreedy, overshooting_info)\n",
        "        s = s_prime\n",
        "        if terminated or truncated:\n",
        "            if debug:\n",
        "                print(\"Episodic Return: {}, Time Step {}, Episode Number {}, Epsilon {}\".format(info['episode']['r'][0], t, episode_num, agent.epsilon))\n",
        "            returns.append(info['episode']['r'][0])\n",
        "            term_time_steps.append(t)\n",
        "            terminated, truncated = False, False\n",
        "            s, _ = env.reset()\n",
        "            episode_num += 1\n",
        "    env.close()\n",
        "    save_dir = \"data_stream_q_{}_lr{}_gamma{}_lamda{}\".format(env.spec.id, lr, gamma, lamda)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    with open(os.path.join(save_dir, \"seed_{}.pkl\".format(seed)), \"wb\") as f:\n",
        "        pickle.dump((returns, term_time_steps, env_name), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMOfOLz4G790"
      },
      "source": [
        "### 8. Set hyperparameters and start the interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-P6-raVKTnH",
        "outputId": "bc964e2c-3654-432e-b6fc-f522168514c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed: 0 env: CartPole-v1\n",
            "Episodic Return: 18.0, Time Step 18, Episode Number 1, Epsilon 0.996436\n",
            "Episodic Return: 37.0, Time Step 55, Episode Number 2, Epsilon 0.98911\n",
            "Episodic Return: 13.0, Time Step 68, Episode Number 3, Epsilon 0.986536\n",
            "Episodic Return: 22.0, Time Step 90, Episode Number 4, Epsilon 0.98218\n",
            "Episodic Return: 38.0, Time Step 128, Episode Number 5, Epsilon 0.974656\n",
            "Episodic Return: 16.0, Time Step 144, Episode Number 6, Epsilon 0.971488\n",
            "Episodic Return: 18.0, Time Step 162, Episode Number 7, Epsilon 0.967924\n",
            "Episodic Return: 15.0, Time Step 177, Episode Number 8, Epsilon 0.964954\n",
            "Episodic Return: 34.0, Time Step 211, Episode Number 9, Epsilon 0.958222\n",
            "Episodic Return: 30.0, Time Step 241, Episode Number 10, Epsilon 0.952282\n",
            "Episodic Return: 19.0, Time Step 260, Episode Number 11, Epsilon 0.94852\n",
            "Episodic Return: 15.0, Time Step 275, Episode Number 12, Epsilon 0.94555\n",
            "Episodic Return: 13.0, Time Step 288, Episode Number 13, Epsilon 0.942976\n",
            "Episodic Return: 13.0, Time Step 301, Episode Number 14, Epsilon 0.940402\n",
            "Episodic Return: 35.0, Time Step 336, Episode Number 15, Epsilon 0.933472\n",
            "Episodic Return: 56.0, Time Step 392, Episode Number 16, Epsilon 0.922384\n",
            "Episodic Return: 34.0, Time Step 426, Episode Number 17, Epsilon 0.915652\n",
            "Episodic Return: 53.0, Time Step 479, Episode Number 18, Epsilon 0.905158\n",
            "Episodic Return: 12.0, Time Step 491, Episode Number 19, Epsilon 0.902782\n",
            "Episodic Return: 11.0, Time Step 502, Episode Number 20, Epsilon 0.900604\n",
            "Episodic Return: 31.0, Time Step 533, Episode Number 21, Epsilon 0.894466\n",
            "Episodic Return: 14.0, Time Step 547, Episode Number 22, Epsilon 0.891694\n",
            "Episodic Return: 34.0, Time Step 581, Episode Number 23, Epsilon 0.884962\n",
            "Episodic Return: 26.0, Time Step 607, Episode Number 24, Epsilon 0.879814\n",
            "Episodic Return: 16.0, Time Step 623, Episode Number 25, Epsilon 0.876646\n",
            "Episodic Return: 43.0, Time Step 666, Episode Number 26, Epsilon 0.868132\n",
            "Episodic Return: 39.0, Time Step 705, Episode Number 27, Epsilon 0.86041\n",
            "Episodic Return: 19.0, Time Step 724, Episode Number 28, Epsilon 0.8566480000000001\n",
            "Episodic Return: 55.0, Time Step 779, Episode Number 29, Epsilon 0.845758\n",
            "Episodic Return: 15.0, Time Step 794, Episode Number 30, Epsilon 0.842788\n",
            "Episodic Return: 11.0, Time Step 805, Episode Number 31, Epsilon 0.84061\n",
            "Episodic Return: 50.0, Time Step 855, Episode Number 32, Epsilon 0.8307100000000001\n",
            "Episodic Return: 21.0, Time Step 876, Episode Number 33, Epsilon 0.826552\n",
            "Episodic Return: 43.0, Time Step 919, Episode Number 34, Epsilon 0.818038\n",
            "Episodic Return: 14.0, Time Step 933, Episode Number 35, Epsilon 0.815266\n",
            "Episodic Return: 26.0, Time Step 959, Episode Number 36, Epsilon 0.810118\n",
            "Episodic Return: 17.0, Time Step 976, Episode Number 37, Epsilon 0.806752\n",
            "Episodic Return: 52.0, Time Step 1028, Episode Number 38, Epsilon 0.796456\n",
            "Episodic Return: 26.0, Time Step 1054, Episode Number 39, Epsilon 0.791308\n",
            "Episodic Return: 22.0, Time Step 1076, Episode Number 40, Epsilon 0.786952\n",
            "Episodic Return: 18.0, Time Step 1094, Episode Number 41, Epsilon 0.783388\n",
            "Episodic Return: 30.0, Time Step 1124, Episode Number 42, Epsilon 0.777448\n",
            "Episodic Return: 50.0, Time Step 1174, Episode Number 43, Epsilon 0.767548\n",
            "Episodic Return: 28.0, Time Step 1202, Episode Number 44, Epsilon 0.762004\n",
            "Episodic Return: 56.0, Time Step 1258, Episode Number 45, Epsilon 0.750916\n",
            "Episodic Return: 28.0, Time Step 1286, Episode Number 46, Epsilon 0.745372\n",
            "Episodic Return: 92.0, Time Step 1378, Episode Number 47, Epsilon 0.727156\n",
            "Episodic Return: 40.0, Time Step 1418, Episode Number 48, Epsilon 0.719236\n",
            "Episodic Return: 21.0, Time Step 1439, Episode Number 49, Epsilon 0.715078\n",
            "Episodic Return: 37.0, Time Step 1476, Episode Number 50, Epsilon 0.7077519999999999\n",
            "Episodic Return: 18.0, Time Step 1494, Episode Number 51, Epsilon 0.704188\n",
            "Episodic Return: 30.0, Time Step 1524, Episode Number 52, Epsilon 0.698248\n",
            "Episodic Return: 39.0, Time Step 1563, Episode Number 53, Epsilon 0.690526\n",
            "Episodic Return: 40.0, Time Step 1603, Episode Number 54, Epsilon 0.682606\n",
            "Episodic Return: 63.0, Time Step 1666, Episode Number 55, Epsilon 0.670132\n",
            "Episodic Return: 32.0, Time Step 1698, Episode Number 56, Epsilon 0.663796\n",
            "Episodic Return: 243.0, Time Step 1941, Episode Number 57, Epsilon 0.6156820000000001\n",
            "Episodic Return: 19.0, Time Step 1960, Episode Number 58, Epsilon 0.61192\n",
            "Episodic Return: 14.0, Time Step 1974, Episode Number 59, Epsilon 0.609148\n",
            "Episodic Return: 27.0, Time Step 2001, Episode Number 60, Epsilon 0.603802\n",
            "Episodic Return: 29.0, Time Step 2030, Episode Number 61, Epsilon 0.59806\n",
            "Episodic Return: 72.0, Time Step 2102, Episode Number 62, Epsilon 0.583804\n",
            "Episodic Return: 28.0, Time Step 2130, Episode Number 63, Epsilon 0.57826\n",
            "Episodic Return: 231.0, Time Step 2361, Episode Number 64, Epsilon 0.532522\n",
            "Episodic Return: 161.0, Time Step 2522, Episode Number 65, Epsilon 0.5006440000000001\n",
            "Episodic Return: 41.0, Time Step 2563, Episode Number 66, Epsilon 0.492526\n",
            "Episodic Return: 95.0, Time Step 2658, Episode Number 67, Epsilon 0.473716\n",
            "Episodic Return: 77.0, Time Step 2735, Episode Number 68, Epsilon 0.45847000000000004\n",
            "Episodic Return: 347.0, Time Step 3082, Episode Number 69, Epsilon 0.389764\n",
            "Episodic Return: 72.0, Time Step 3154, Episode Number 70, Epsilon 0.37550800000000006\n",
            "Episodic Return: 121.0, Time Step 3275, Episode Number 71, Epsilon 0.35155000000000003\n",
            "Episodic Return: 85.0, Time Step 3360, Episode Number 72, Epsilon 0.33472\n",
            "Episodic Return: 12.0, Time Step 3372, Episode Number 73, Epsilon 0.3323440000000001\n",
            "Episodic Return: 70.0, Time Step 3442, Episode Number 74, Epsilon 0.318484\n",
            "Episodic Return: 122.0, Time Step 3564, Episode Number 75, Epsilon 0.29432800000000003\n",
            "Episodic Return: 98.0, Time Step 3662, Episode Number 76, Epsilon 0.27492400000000006\n",
            "Episodic Return: 72.0, Time Step 3734, Episode Number 77, Epsilon 0.260668\n",
            "Episodic Return: 98.0, Time Step 3832, Episode Number 78, Epsilon 0.24126400000000003\n",
            "Episodic Return: 126.0, Time Step 3958, Episode Number 79, Epsilon 0.21631600000000006\n",
            "Episodic Return: 32.0, Time Step 3990, Episode Number 80, Epsilon 0.20998000000000006\n",
            "Episodic Return: 121.0, Time Step 4111, Episode Number 81, Epsilon 0.18602200000000002\n",
            "Episodic Return: 48.0, Time Step 4159, Episode Number 82, Epsilon 0.17651800000000006\n",
            "Episodic Return: 40.0, Time Step 4199, Episode Number 83, Epsilon 0.16859800000000003\n",
            "Episodic Return: 39.0, Time Step 4238, Episode Number 84, Epsilon 0.16087600000000002\n",
            "Episodic Return: 38.0, Time Step 4276, Episode Number 85, Epsilon 0.15335200000000004\n",
            "Episodic Return: 13.0, Time Step 4289, Episode Number 86, Epsilon 0.15077800000000008\n",
            "Episodic Return: 14.0, Time Step 4303, Episode Number 87, Epsilon 0.14800600000000008\n",
            "Episodic Return: 9.0, Time Step 4312, Episode Number 88, Epsilon 0.14622400000000002\n",
            "Episodic Return: 25.0, Time Step 4337, Episode Number 89, Epsilon 0.141274\n",
            "Episodic Return: 12.0, Time Step 4349, Episode Number 90, Epsilon 0.13889800000000008\n",
            "Episodic Return: 9.0, Time Step 4358, Episode Number 91, Epsilon 0.13711600000000002\n",
            "Episodic Return: 24.0, Time Step 4382, Episode Number 92, Epsilon 0.13236400000000004\n",
            "Episodic Return: 35.0, Time Step 4417, Episode Number 93, Epsilon 0.12543400000000005\n",
            "Episodic Return: 39.0, Time Step 4456, Episode Number 94, Epsilon 0.11771200000000004\n",
            "Episodic Return: 16.0, Time Step 4472, Episode Number 95, Epsilon 0.11454400000000009\n",
            "Episodic Return: 37.0, Time Step 4509, Episode Number 96, Epsilon 0.10721800000000004\n",
            "Episodic Return: 59.0, Time Step 4568, Episode Number 97, Epsilon 0.09553600000000007\n",
            "Episodic Return: 63.0, Time Step 4631, Episode Number 98, Epsilon 0.08306200000000008\n",
            "Episodic Return: 50.0, Time Step 4681, Episode Number 99, Epsilon 0.07316200000000006\n",
            "Episodic Return: 94.0, Time Step 4775, Episode Number 100, Epsilon 0.0545500000000001\n",
            "Episodic Return: 101.0, Time Step 4876, Episode Number 101, Epsilon 0.03455200000000003\n",
            "Episodic Return: 98.0, Time Step 4974, Episode Number 102, Epsilon 0.01514800000000005\n",
            "Episodic Return: 111.0, Time Step 5085, Episode Number 103, Epsilon 0.01\n",
            "Episodic Return: 121.0, Time Step 5206, Episode Number 104, Epsilon 0.01\n",
            "Episodic Return: 85.0, Time Step 5291, Episode Number 105, Epsilon 0.01\n",
            "Episodic Return: 19.0, Time Step 5310, Episode Number 106, Epsilon 0.01\n",
            "Episodic Return: 76.0, Time Step 5386, Episode Number 107, Epsilon 0.01\n",
            "Episodic Return: 66.0, Time Step 5452, Episode Number 108, Epsilon 0.01\n",
            "Episodic Return: 15.0, Time Step 5467, Episode Number 109, Epsilon 0.01\n",
            "Episodic Return: 64.0, Time Step 5531, Episode Number 110, Epsilon 0.01\n",
            "Episodic Return: 56.0, Time Step 5587, Episode Number 111, Epsilon 0.01\n",
            "Episodic Return: 56.0, Time Step 5643, Episode Number 112, Epsilon 0.01\n",
            "Episodic Return: 47.0, Time Step 5690, Episode Number 113, Epsilon 0.01\n",
            "Episodic Return: 52.0, Time Step 5742, Episode Number 114, Epsilon 0.01\n",
            "Episodic Return: 71.0, Time Step 5813, Episode Number 115, Epsilon 0.01\n",
            "Episodic Return: 85.0, Time Step 5898, Episode Number 116, Epsilon 0.01\n",
            "Episodic Return: 13.0, Time Step 5911, Episode Number 117, Epsilon 0.01\n",
            "Episodic Return: 27.0, Time Step 5938, Episode Number 118, Epsilon 0.01\n",
            "Episodic Return: 107.0, Time Step 6045, Episode Number 119, Epsilon 0.01\n",
            "Episodic Return: 62.0, Time Step 6107, Episode Number 120, Epsilon 0.01\n",
            "Episodic Return: 110.0, Time Step 6217, Episode Number 121, Epsilon 0.01\n",
            "Episodic Return: 81.0, Time Step 6298, Episode Number 122, Epsilon 0.01\n",
            "Episodic Return: 93.0, Time Step 6391, Episode Number 123, Epsilon 0.01\n",
            "Episodic Return: 92.0, Time Step 6483, Episode Number 124, Epsilon 0.01\n",
            "Episodic Return: 94.0, Time Step 6577, Episode Number 125, Epsilon 0.01\n",
            "Episodic Return: 24.0, Time Step 6601, Episode Number 126, Epsilon 0.01\n",
            "Episodic Return: 98.0, Time Step 6699, Episode Number 127, Epsilon 0.01\n",
            "Episodic Return: 95.0, Time Step 6794, Episode Number 128, Epsilon 0.01\n",
            "Episodic Return: 25.0, Time Step 6819, Episode Number 129, Epsilon 0.01\n",
            "Episodic Return: 109.0, Time Step 6928, Episode Number 130, Epsilon 0.01\n",
            "Episodic Return: 35.0, Time Step 6963, Episode Number 131, Epsilon 0.01\n",
            "Episodic Return: 118.0, Time Step 7081, Episode Number 132, Epsilon 0.01\n",
            "Episodic Return: 117.0, Time Step 7198, Episode Number 133, Epsilon 0.01\n",
            "Episodic Return: 25.0, Time Step 7223, Episode Number 134, Epsilon 0.01\n",
            "Episodic Return: 116.0, Time Step 7339, Episode Number 135, Epsilon 0.01\n",
            "Episodic Return: 118.0, Time Step 7457, Episode Number 136, Epsilon 0.01\n",
            "Episodic Return: 9.0, Time Step 7466, Episode Number 137, Epsilon 0.01\n",
            "Episodic Return: 80.0, Time Step 7546, Episode Number 138, Epsilon 0.01\n",
            "Episodic Return: 82.0, Time Step 7628, Episode Number 139, Epsilon 0.01\n",
            "Episodic Return: 31.0, Time Step 7659, Episode Number 140, Epsilon 0.01\n",
            "Episodic Return: 89.0, Time Step 7748, Episode Number 141, Epsilon 0.01\n",
            "Episodic Return: 95.0, Time Step 7843, Episode Number 142, Epsilon 0.01\n",
            "Episodic Return: 102.0, Time Step 7945, Episode Number 143, Epsilon 0.01\n",
            "Episodic Return: 97.0, Time Step 8042, Episode Number 144, Epsilon 0.01\n",
            "Episodic Return: 13.0, Time Step 8055, Episode Number 145, Epsilon 0.01\n",
            "Episodic Return: 90.0, Time Step 8145, Episode Number 146, Epsilon 0.01\n",
            "Episodic Return: 99.0, Time Step 8244, Episode Number 147, Epsilon 0.01\n",
            "Episodic Return: 110.0, Time Step 8354, Episode Number 148, Epsilon 0.01\n",
            "Episodic Return: 113.0, Time Step 8467, Episode Number 149, Epsilon 0.01\n",
            "Episodic Return: 102.0, Time Step 8569, Episode Number 150, Epsilon 0.01\n",
            "Episodic Return: 96.0, Time Step 8665, Episode Number 151, Epsilon 0.01\n",
            "Episodic Return: 84.0, Time Step 8749, Episode Number 152, Epsilon 0.01\n",
            "Episodic Return: 82.0, Time Step 8831, Episode Number 153, Epsilon 0.01\n",
            "Episodic Return: 81.0, Time Step 8912, Episode Number 154, Epsilon 0.01\n",
            "Episodic Return: 65.0, Time Step 8977, Episode Number 155, Epsilon 0.01\n",
            "Episodic Return: 67.0, Time Step 9044, Episode Number 156, Epsilon 0.01\n",
            "Episodic Return: 85.0, Time Step 9129, Episode Number 157, Epsilon 0.01\n",
            "Episodic Return: 91.0, Time Step 9220, Episode Number 158, Epsilon 0.01\n",
            "Episodic Return: 93.0, Time Step 9313, Episode Number 159, Epsilon 0.01\n",
            "Episodic Return: 99.0, Time Step 9412, Episode Number 160, Epsilon 0.01\n",
            "Episodic Return: 141.0, Time Step 9553, Episode Number 161, Epsilon 0.01\n",
            "Episodic Return: 8201.0, Time Step 17754, Episode Number 162, Epsilon 0.01\n",
            "Episodic Return: 204.0, Time Step 17958, Episode Number 163, Epsilon 0.01\n",
            "Episodic Return: 7417.0, Time Step 25375, Episode Number 164, Epsilon 0.01\n",
            "Episodic Return: 442.0, Time Step 25817, Episode Number 165, Epsilon 0.01\n",
            "Episodic Return: 409.0, Time Step 26226, Episode Number 166, Epsilon 0.01\n",
            "Episodic Return: 412.0, Time Step 26638, Episode Number 167, Epsilon 0.01\n",
            "Episodic Return: 122.0, Time Step 26760, Episode Number 168, Epsilon 0.01\n",
            "Episodic Return: 3757.0, Time Step 30517, Episode Number 169, Epsilon 0.01\n",
            "Episodic Return: 18.0, Time Step 30535, Episode Number 170, Epsilon 0.01\n",
            "Episodic Return: 198.0, Time Step 30733, Episode Number 171, Epsilon 0.01\n",
            "Episodic Return: 120.0, Time Step 30853, Episode Number 172, Epsilon 0.01\n",
            "Episodic Return: 189.0, Time Step 31042, Episode Number 173, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 41042, Episode Number 174, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 51042, Episode Number 175, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 61042, Episode Number 176, Epsilon 0.01\n",
            "Episodic Return: 304.0, Time Step 61346, Episode Number 177, Epsilon 0.01\n",
            "Episodic Return: 168.0, Time Step 61514, Episode Number 178, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 71514, Episode Number 179, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 81514, Episode Number 180, Epsilon 0.01\n"
          ]
        }
      ],
      "source": [
        "agent_env_interaction(\n",
        "    env_name='CartPole-v1',\n",
        "                      seed=0,\n",
        "                      lr=1.0,\n",
        "                      gamma=0.99,\n",
        "                      lamda=0.8,\n",
        "                      total_steps=100_000,\n",
        "                      epsilon_target=0.01,\n",
        "                      epsilon_start=1.0,\n",
        "                      exploration_fraction=0.05,\n",
        "                      kappa_value=2.0,\n",
        "                      debug=True,\n",
        "                      overshooting_info=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKBMPrNReH7W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
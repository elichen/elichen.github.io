{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elichen/elichen.github.io/blob/main/cartpole/Stream_Q_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHneGiUqFkKX"
      },
      "source": [
        "# Stream Q($\\lambda$) algorithm with Cartpole:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krEplx1FFw-5"
      },
      "source": [
        "### 1. Install necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bPOAHpVEPPu8",
        "outputId": "9df36b42-4911-4d3e-e26a-ad8af1bf70a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==0.29.1)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 triton-2.3.0\n",
            "Collecting stable_baselines3==2.3.1\n",
            "  Downloading stable_baselines3-2.3.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (2.3.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3==2.3.1) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3==2.3.1) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3==2.3.1) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3==2.3.1) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3==2.3.1) (12.6.85)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3==2.3.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3==2.3.1) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3==2.3.1) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3==2.3.1) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3==2.3.1) (1.3.0)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'stable-baselines3' candidate (version 2.3.1 at https://files.pythonhosted.org/packages/2a/59/e35fc90a805303b9f585342f4a3836b7b81924c760e00c66130cc64641e4/stable_baselines3-2.3.1-py3-none-any.whl (from https://pypi.org/simple/stable-baselines3/) (requires-python:>=3.8))\n",
            "Reason for being yanked: Loading broken with PyTorch 1.13\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading stable_baselines3-2.3.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.2/182.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stable_baselines3\n",
            "Successfully installed stable_baselines3-2.3.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (4.67.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (6.4.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2024.12.14)\n",
            "Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446667 sha256=4330dd25832f1afc8153fb029947df6fd7d5da33443fbc7a1dc1e74c7d5808fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libfontenc1 libglu1-mesa libxfont2 libxkbfile1 x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common\n",
            "Suggested packages:\n",
            "  libgle3 python3-numpy\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 libfontenc1 libglu1-mesa libxfont2 libxkbfile1 python3-opengl x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 12 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 8,639 kB of archives.\n",
            "After this operation, 20.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-opengl all 3.1.5+dfsg-1 [605 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12 [28.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.12 [864 kB]\n",
            "Fetched 8,639 kB in 2s (4,480 kB/s)\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../01-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../02-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../03-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../04-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package python3-opengl.\n",
            "Preparing to unpack .../05-python3-opengl_3.1.5+dfsg-1_all.deb ...\n",
            "Unpacking python3-opengl (3.1.5+dfsg-1) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../06-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../07-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../08-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../09-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../10-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../11-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up python3-opengl (3.1.5+dfsg-1) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium==0.29.1\n",
        "!pip install numpy==1.26.4\n",
        "!pip install torch==2.3.0\n",
        "!pip install stable_baselines3==2.3.1\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y python3-opengl xvfb ffmpeg\n",
        "!pip install pyvirtualdisplay imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5a5deBrGH7j"
      },
      "source": [
        "### 2. Import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NIRasg9NGHlO"
      },
      "outputs": [],
      "source": [
        "import torch, math\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import os, pickle, argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZXVIgUEF8_D"
      },
      "source": [
        "### 3. Implement the ObGD optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbqA3nRKPMPQ",
        "outputId": "620f1433-470b-44e3-ec15-da7200fc3215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class ObGD(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=1.0, gamma=0.99, lamda=0.8, kappa=2.0):\n",
        "        defaults = dict(lr=lr, gamma=gamma, lamda=lamda, kappa=kappa)\n",
        "        super(ObGD, self).__init__(params, defaults)\n",
        "    def step(self, delta, reset=False):\n",
        "        z_sum = 0.0\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state[\"eligibility_trace\"] = torch.zeros_like(p.data)\n",
        "                e = state[\"eligibility_trace\"]\n",
        "                e.mul_(group[\"gamma\"] * group[\"lamda\"]).add_(p.grad, alpha=1.0)\n",
        "                z_sum += e.abs().sum().item()\n",
        "\n",
        "        delta_bar = max(abs(delta), 1.0)\n",
        "        dot_product = delta_bar * z_sum * group[\"lr\"] * group[\"kappa\"]\n",
        "        if dot_product > 1:\n",
        "            step_size = group[\"lr\"] / dot_product\n",
        "        else:\n",
        "            step_size = group[\"lr\"]\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                state = self.state[p]\n",
        "                e = state[\"eligibility_trace\"]\n",
        "                p.data.add_(delta * e, alpha=-step_size)\n",
        "                if reset:\n",
        "                    e.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBknMm7_Gj9U"
      },
      "source": [
        "### 4. Implement sparse initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lj5Pk1MkPKTP"
      },
      "outputs": [],
      "source": [
        "def sparse_init(tensor, sparsity, type='uniform'):\n",
        "\n",
        "    if tensor.ndimension() == 2:\n",
        "        fan_out, fan_in = tensor.shape\n",
        "\n",
        "        num_zeros = int(math.ceil(sparsity * fan_in))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if type == 'uniform':\n",
        "                tensor.uniform_(-math.sqrt(1.0 / fan_in), math.sqrt(1.0 / fan_in))\n",
        "            elif type == 'normal':\n",
        "                tensor.normal_(0, math.sqrt(1.0 / fan_in))\n",
        "            else:\n",
        "                raise ValueError(\"Unknown initialization type\")\n",
        "            for col_idx in range(fan_out):\n",
        "                row_indices = torch.randperm(fan_in)\n",
        "                zero_indices = row_indices[:num_zeros]\n",
        "                tensor[col_idx, zero_indices] = 0\n",
        "        return tensor\n",
        "\n",
        "    elif tensor.ndimension() == 4:\n",
        "        channels_out, channels_in, h, w = tensor.shape\n",
        "        fan_in, fan_out = channels_in*h*w, channels_out*h*w\n",
        "\n",
        "        num_zeros = int(math.ceil(sparsity * fan_in))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if type == 'uniform':\n",
        "                tensor.uniform_(-math.sqrt(1.0 / fan_in), math.sqrt(1.0 / fan_in))\n",
        "            elif type == 'normal':\n",
        "                tensor.normal_(0, math.sqrt(1.0 / fan_in))\n",
        "            else:\n",
        "                raise ValueError(\"Unknown initialization type\")\n",
        "            for out_channel_idx in range(channels_out):\n",
        "                indices = torch.randperm(fan_in)\n",
        "                zero_indices = indices[:num_zeros]\n",
        "                tensor[out_channel_idx].reshape(channels_in*h*w)[zero_indices] = 0\n",
        "        return tensor\n",
        "    else:\n",
        "        raise ValueError(\"Only tensors with 2 or 4 dimensions are supported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFzKufnGqEK"
      },
      "source": [
        "### 5. Implement observation normalization and reward scaling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "na4mSpcMPGxg"
      },
      "outputs": [],
      "source": [
        "class SampleMeanStd:\n",
        "    def __init__(self, shape=()):\n",
        "        self.mean = np.zeros(shape, \"float64\")\n",
        "        self.var = np.ones(shape, \"float64\")\n",
        "        self.p = np.ones(shape, \"float64\")\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, x):\n",
        "        if self.count == 0:\n",
        "            self.mean = x\n",
        "            self.p = np.zeros_like(x)\n",
        "        self.mean, self.var, self.p, self.count = self.update_mean_var_count_from_moments(self.mean, self.p, self.count, x*1.0)\n",
        "\n",
        "    def update_mean_var_count_from_moments(self, mean, p, count, sample):\n",
        "        new_count = count + 1\n",
        "        new_mean = mean + (sample - mean) / new_count\n",
        "        p = p + (sample - mean) * (sample - new_mean)\n",
        "        new_var = 1 if new_count < 2 else p / (new_count - 1)\n",
        "        return new_mean, new_var, p, new_count\n",
        "\n",
        "class NormalizeObservation(gym.Wrapper, gym.utils.RecordConstructorArgs):\n",
        "    def __init__(self, env: gym.Env, epsilon: float = 1e-8):\n",
        "        gym.utils.RecordConstructorArgs.__init__(self, epsilon=epsilon)\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        try:\n",
        "            self.num_envs = self.get_wrapper_attr(\"num_envs\")\n",
        "            self.is_vector_env = self.get_wrapper_attr(\"is_vector_env\")\n",
        "        except AttributeError:\n",
        "            self.num_envs = 1\n",
        "            self.is_vector_env = False\n",
        "\n",
        "        if self.is_vector_env:\n",
        "            self.obs_stats = SampleMeanStd(shape=self.single_observation_space.shape)\n",
        "        else:\n",
        "            self.obs_stats = SampleMeanStd(shape=self.observation_space.shape)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        if self.is_vector_env:\n",
        "            obs = self.normalize(obs)\n",
        "        else:\n",
        "            obs = self.normalize(np.array([obs]))[0]\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        if self.is_vector_env:\n",
        "            return self.normalize(obs), info\n",
        "        else:\n",
        "            return self.normalize(np.array([obs]))[0], info\n",
        "\n",
        "    def normalize(self, obs):\n",
        "        self.obs_stats.update(obs)\n",
        "        return (obs - self.obs_stats.mean) / np.sqrt(self.obs_stats.var + self.epsilon)\n",
        "\n",
        "class ScaleReward(gym.core.Wrapper, gym.utils.RecordConstructorArgs):\n",
        "    def __init__(self, env: gym.Env, gamma: float = 0.99, epsilon: float = 1e-8):\n",
        "        gym.utils.RecordConstructorArgs.__init__(self, gamma=gamma, epsilon=epsilon)\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        try:\n",
        "            self.num_envs = self.get_wrapper_attr(\"num_envs\")\n",
        "            self.is_vector_env = self.get_wrapper_attr(\"is_vector_env\")\n",
        "        except AttributeError:\n",
        "            self.num_envs = 1\n",
        "            self.is_vector_env = False\n",
        "        self.reward_stats = SampleMeanStd(shape=())\n",
        "        self.reward_trace = np.zeros(self.num_envs)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        if not self.is_vector_env:\n",
        "            rews = np.array([rews])\n",
        "        term = terminateds or truncateds\n",
        "        self.reward_trace = self.reward_trace * self.gamma * (1 - term) + rews\n",
        "        rews = self.normalize(rews)\n",
        "        if not self.is_vector_env:\n",
        "            rews = rews[0]\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def normalize(self, rews):\n",
        "        self.reward_stats.update(self.reward_trace)\n",
        "        return rews / np.sqrt(self.reward_stats.var + self.epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx5um9DCG0CN"
      },
      "source": [
        "### 6. Implement the Stream Q($\\lambda$) agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bjwzfOFuE79l"
      },
      "outputs": [],
      "source": [
        "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "    slope = (end_e - start_e) / duration\n",
        "    return max(slope * t + start_e, end_e)\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        sparse_init(m.weight, sparsity=0.9)\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "class StreamQ(nn.Module):\n",
        "    def __init__(self, n_obs=11, n_actions=3, hidden_size=32, lr=1.0, epsilon_target=0.01, epsilon_start=1.0, exploration_fraction=0.1, total_steps=1_000_000, gamma=0.99, lamda=0.8, kappa_value=2.0):\n",
        "        super(StreamQ, self).__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_target = epsilon_target\n",
        "        self.epsilon = epsilon_start\n",
        "        self.exploration_fraction = exploration_fraction\n",
        "        self.total_steps = total_steps\n",
        "        self.time_step = 0\n",
        "        self.fc1_v   = nn.Linear(n_obs, hidden_size)\n",
        "        self.hidden_v  = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc_v  = nn.Linear(hidden_size, n_actions)\n",
        "        self.apply(initialize_weights)\n",
        "        self.optimizer = ObGD(list(self.parameters()), lr=lr, gamma=gamma, lamda=lamda, kappa=kappa_value)\n",
        "\n",
        "    def q(self, x):\n",
        "        x = self.fc1_v(x)\n",
        "        x = F.layer_norm(x, x.size())\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.hidden_v(x)\n",
        "        x = F.layer_norm(x, x.size())\n",
        "        x = F.leaky_relu(x)\n",
        "        return self.fc_v(x)\n",
        "\n",
        "    def sample_action(self, s):\n",
        "        self.time_step += 1\n",
        "        self.epsilon = linear_schedule(self.epsilon_start, self.epsilon_target, self.exploration_fraction * self.total_steps, self.time_step)\n",
        "        if isinstance(s, np.ndarray):\n",
        "            s = torch.tensor(np.array(s), dtype=torch.float)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            q_values = self.q(s)\n",
        "            greedy_action = torch.argmax(q_values, dim=-1).item()\n",
        "            random_action = np.random.randint(0, self.n_actions)\n",
        "            if greedy_action == random_action:\n",
        "                return random_action, False\n",
        "            else:\n",
        "                return random_action, True\n",
        "        else:\n",
        "            q_values = self.q(s)\n",
        "            return torch.argmax(q_values, dim=-1).item(), False\n",
        "\n",
        "    def update_params(self, s, a, r, s_prime, done, is_nongreedy, overshooting_info=False):\n",
        "        done_mask = 0 if done else 1\n",
        "        s, a, r, s_prime, done_mask = torch.tensor(np.array(s), dtype=torch.float), torch.tensor([a], dtype=torch.int).squeeze(0), \\\n",
        "                                         torch.tensor(np.array(r)), torch.tensor(np.array(s_prime), dtype=torch.float), \\\n",
        "                                         torch.tensor(np.array(done_mask), dtype=torch.float)\n",
        "        q_sa = self.q(s)[a]\n",
        "        max_q_s_prime_a_prime = torch.max(self.q(s_prime), dim=-1).values\n",
        "        td_target = r + self.gamma * max_q_s_prime_a_prime * done_mask\n",
        "        delta = td_target - q_sa\n",
        "\n",
        "        q_output = -q_sa\n",
        "        self.optimizer.zero_grad()\n",
        "        q_output.backward()\n",
        "        self.optimizer.step(delta.item(), reset=(done or is_nongreedy))\n",
        "\n",
        "        if overshooting_info:\n",
        "            max_q_s_prime_a_prime = torch.max(self.q(s_prime), dim=-1).values\n",
        "            td_target = r + self.gamma * max_q_s_prime_a_prime * done_mask\n",
        "            delta_bar = td_target - self.q(s)[a]\n",
        "            if torch.sign(delta_bar * delta).item() == -1:\n",
        "                print(\"Overshooting Detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "class AddTimeInfo(gym.core.Wrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__(env)\n",
        "        if self.env.num_envs > 1:\n",
        "            raise ValueError(\"AddTimeInfo only supports single environments\")\n",
        "        self.epi_time = -0.5\n",
        "        if 'dm_control' in env.spec.id:\n",
        "            self.time_limit = 1000\n",
        "        else:\n",
        "            self.time_limit = env.spec.max_episode_steps\n",
        "        self.obs_space_size = self.observation_space.shape[0] + self.env.num_envs\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_space_size,), dtype=np.float32)\n",
        "        if not (isinstance(self.action_space, gym.spaces.Box) or isinstance(self.action_space, gym.spaces.Discrete)):\n",
        "            raise ValueError(\"Unsupported action space\")\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
        "        obs = np.concatenate((obs, np.array([self.epi_time] * self.env.num_envs)))\n",
        "        self.epi_time += 1.0 / self.time_limit\n",
        "        return obs, rews, terminateds, truncateds, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.epi_time = -0.5\n",
        "        obs = np.concatenate((obs, np.array([self.epi_time])))\n",
        "        return obs, info"
      ],
      "metadata": {
        "id": "l1R1qHcLSLtF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqqI3tOXHDkA"
      },
      "source": [
        "### 7. Define agent-enviroment interaction loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RZMOUleDO6JN"
      },
      "outputs": [],
      "source": [
        "def agent_env_interaction(env_name, seed, lr, gamma, lamda, total_steps, epsilon_target, epsilon_start, exploration_fraction, kappa_value, debug, overshooting_info, render=False):\n",
        "    torch.manual_seed(seed); np.random.seed(seed)\n",
        "    env = gym.make(env_name, render_mode='human', max_episode_steps=10_000) if render else gym.make(env_name, max_episode_steps=10_000)\n",
        "    env = gym.wrappers.FlattenObservation(env)\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    env = ScaleReward(env, gamma=gamma)\n",
        "    env = NormalizeObservation(env)\n",
        "    env = AddTimeInfo(env)\n",
        "    agent = StreamQ(n_obs=env.observation_space.shape[0], n_actions=env.action_space.n, lr=lr, gamma=gamma, lamda=lamda, epsilon_target=epsilon_target, epsilon_start=epsilon_start, exploration_fraction=exploration_fraction, total_steps=total_steps, kappa_value=kappa_value)\n",
        "    if debug:\n",
        "        print(\"seed: {}\".format(seed), \"env: {}\".format(env.spec.id))\n",
        "    returns, term_time_steps = [], []\n",
        "    s, _ = env.reset(seed=seed)\n",
        "    episode_num = 1\n",
        "    for t in range(1, total_steps+1):\n",
        "        a, is_nongreedy = agent.sample_action(s)\n",
        "        s_prime, r, terminated, truncated, info = env.step(a)\n",
        "        agent.update_params(s, a, r, s_prime, terminated or truncated, is_nongreedy, overshooting_info)\n",
        "        s = s_prime\n",
        "        if terminated or truncated:\n",
        "            if debug:\n",
        "                print(\"Episodic Return: {}, Time Step {}, Episode Number {}, Epsilon {}\".format(info['episode']['r'][0], t, episode_num, agent.epsilon))\n",
        "            returns.append(info['episode']['r'][0])\n",
        "            term_time_steps.append(t)\n",
        "            terminated, truncated = False, False\n",
        "            s, _ = env.reset()\n",
        "            episode_num += 1\n",
        "    env.close()\n",
        "    save_dir = \"data_stream_q_{}_lr{}_gamma{}_lamda{}\".format(env.spec.id, lr, gamma, lamda)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    with open(os.path.join(save_dir, \"seed_{}.pkl\".format(seed)), \"wb\") as f:\n",
        "        pickle.dump((returns, term_time_steps, env_name), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMOfOLz4G790"
      },
      "source": [
        "### 8. Set hyperparameters and start the interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-P6-raVKTnH",
        "outputId": "17d28d0e-0b95-4b22-def7-827425c4d6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed: 0 env: CartPole-v1\n",
            "Episodic Return: 18.0, Time Step 18, Episode Number 1, Epsilon 0.9992872\n",
            "Episodic Return: 49.0, Time Step 67, Episode Number 2, Epsilon 0.9973468\n",
            "Episodic Return: 12.0, Time Step 79, Episode Number 3, Epsilon 0.9968716\n",
            "Episodic Return: 23.0, Time Step 102, Episode Number 4, Epsilon 0.9959608\n",
            "Episodic Return: 16.0, Time Step 118, Episode Number 5, Epsilon 0.9953272\n",
            "Episodic Return: 27.0, Time Step 145, Episode Number 6, Epsilon 0.994258\n",
            "Episodic Return: 15.0, Time Step 160, Episode Number 7, Epsilon 0.993664\n",
            "Episodic Return: 27.0, Time Step 187, Episode Number 8, Epsilon 0.9925948\n",
            "Episodic Return: 9.0, Time Step 196, Episode Number 9, Epsilon 0.9922384\n",
            "Episodic Return: 33.0, Time Step 229, Episode Number 10, Epsilon 0.9909316\n",
            "Episodic Return: 30.0, Time Step 259, Episode Number 11, Epsilon 0.9897436\n",
            "Episodic Return: 20.0, Time Step 279, Episode Number 12, Epsilon 0.9889516\n",
            "Episodic Return: 39.0, Time Step 318, Episode Number 13, Epsilon 0.9874072\n",
            "Episodic Return: 15.0, Time Step 333, Episode Number 14, Epsilon 0.9868132\n",
            "Episodic Return: 21.0, Time Step 354, Episode Number 15, Epsilon 0.9859816\n",
            "Episodic Return: 16.0, Time Step 370, Episode Number 16, Epsilon 0.985348\n",
            "Episodic Return: 20.0, Time Step 390, Episode Number 17, Epsilon 0.984556\n",
            "Episodic Return: 25.0, Time Step 415, Episode Number 18, Epsilon 0.983566\n",
            "Episodic Return: 15.0, Time Step 430, Episode Number 19, Epsilon 0.982972\n",
            "Episodic Return: 24.0, Time Step 454, Episode Number 20, Epsilon 0.9820216\n",
            "Episodic Return: 19.0, Time Step 473, Episode Number 21, Epsilon 0.9812692\n",
            "Episodic Return: 17.0, Time Step 490, Episode Number 22, Epsilon 0.980596\n",
            "Episodic Return: 56.0, Time Step 546, Episode Number 23, Epsilon 0.9783784\n",
            "Episodic Return: 19.0, Time Step 565, Episode Number 24, Epsilon 0.977626\n",
            "Episodic Return: 26.0, Time Step 591, Episode Number 25, Epsilon 0.9765964\n",
            "Episodic Return: 38.0, Time Step 629, Episode Number 26, Epsilon 0.9750916\n",
            "Episodic Return: 15.0, Time Step 644, Episode Number 27, Epsilon 0.9744976\n",
            "Episodic Return: 28.0, Time Step 672, Episode Number 28, Epsilon 0.9733887999999999\n",
            "Episodic Return: 34.0, Time Step 706, Episode Number 29, Epsilon 0.9720424\n",
            "Episodic Return: 51.0, Time Step 757, Episode Number 30, Epsilon 0.9700228\n",
            "Episodic Return: 22.0, Time Step 779, Episode Number 31, Epsilon 0.9691516\n",
            "Episodic Return: 46.0, Time Step 825, Episode Number 32, Epsilon 0.96733\n",
            "Episodic Return: 28.0, Time Step 853, Episode Number 33, Epsilon 0.9662212\n",
            "Episodic Return: 16.0, Time Step 869, Episode Number 34, Epsilon 0.9655876\n",
            "Episodic Return: 17.0, Time Step 886, Episode Number 35, Epsilon 0.9649144\n",
            "Episodic Return: 46.0, Time Step 932, Episode Number 36, Epsilon 0.9630928\n",
            "Episodic Return: 11.0, Time Step 943, Episode Number 37, Epsilon 0.9626572\n",
            "Episodic Return: 20.0, Time Step 963, Episode Number 38, Epsilon 0.9618652\n",
            "Episodic Return: 15.0, Time Step 978, Episode Number 39, Epsilon 0.9612712\n",
            "Episodic Return: 16.0, Time Step 994, Episode Number 40, Epsilon 0.9606376\n",
            "Episodic Return: 48.0, Time Step 1042, Episode Number 41, Epsilon 0.9587368000000001\n",
            "Episodic Return: 10.0, Time Step 1052, Episode Number 42, Epsilon 0.9583408\n",
            "Episodic Return: 27.0, Time Step 1079, Episode Number 43, Epsilon 0.9572716\n",
            "Episodic Return: 17.0, Time Step 1096, Episode Number 44, Epsilon 0.9565984\n",
            "Episodic Return: 30.0, Time Step 1126, Episode Number 45, Epsilon 0.9554104\n",
            "Episodic Return: 43.0, Time Step 1169, Episode Number 46, Epsilon 0.9537076\n",
            "Episodic Return: 28.0, Time Step 1197, Episode Number 47, Epsilon 0.9525988\n",
            "Episodic Return: 20.0, Time Step 1217, Episode Number 48, Epsilon 0.9518068\n",
            "Episodic Return: 18.0, Time Step 1235, Episode Number 49, Epsilon 0.951094\n",
            "Episodic Return: 32.0, Time Step 1267, Episode Number 50, Epsilon 0.9498268\n",
            "Episodic Return: 16.0, Time Step 1283, Episode Number 51, Epsilon 0.9491932\n",
            "Episodic Return: 22.0, Time Step 1305, Episode Number 52, Epsilon 0.948322\n",
            "Episodic Return: 15.0, Time Step 1320, Episode Number 53, Epsilon 0.947728\n",
            "Episodic Return: 12.0, Time Step 1332, Episode Number 54, Epsilon 0.9472528\n",
            "Episodic Return: 15.0, Time Step 1347, Episode Number 55, Epsilon 0.9466588\n",
            "Episodic Return: 15.0, Time Step 1362, Episode Number 56, Epsilon 0.9460648\n",
            "Episodic Return: 10.0, Time Step 1372, Episode Number 57, Epsilon 0.9456688\n",
            "Episodic Return: 15.0, Time Step 1387, Episode Number 58, Epsilon 0.9450748\n",
            "Episodic Return: 21.0, Time Step 1408, Episode Number 59, Epsilon 0.9442432\n",
            "Episodic Return: 16.0, Time Step 1424, Episode Number 60, Epsilon 0.9436096\n",
            "Episodic Return: 25.0, Time Step 1449, Episode Number 61, Epsilon 0.9426196\n",
            "Episodic Return: 21.0, Time Step 1470, Episode Number 62, Epsilon 0.941788\n",
            "Episodic Return: 17.0, Time Step 1487, Episode Number 63, Epsilon 0.9411148\n",
            "Episodic Return: 18.0, Time Step 1505, Episode Number 64, Epsilon 0.940402\n",
            "Episodic Return: 25.0, Time Step 1530, Episode Number 65, Epsilon 0.939412\n",
            "Episodic Return: 44.0, Time Step 1574, Episode Number 66, Epsilon 0.9376696\n",
            "Episodic Return: 32.0, Time Step 1606, Episode Number 67, Epsilon 0.9364024\n",
            "Episodic Return: 31.0, Time Step 1637, Episode Number 68, Epsilon 0.9351748\n",
            "Episodic Return: 12.0, Time Step 1649, Episode Number 69, Epsilon 0.9346996\n",
            "Episodic Return: 13.0, Time Step 1662, Episode Number 70, Epsilon 0.9341848\n",
            "Episodic Return: 20.0, Time Step 1682, Episode Number 71, Epsilon 0.9333928\n",
            "Episodic Return: 35.0, Time Step 1717, Episode Number 72, Epsilon 0.9320068\n",
            "Episodic Return: 46.0, Time Step 1763, Episode Number 73, Epsilon 0.9301852\n",
            "Episodic Return: 12.0, Time Step 1775, Episode Number 74, Epsilon 0.92971\n",
            "Episodic Return: 28.0, Time Step 1803, Episode Number 75, Epsilon 0.9286012\n",
            "Episodic Return: 14.0, Time Step 1817, Episode Number 76, Epsilon 0.9280468\n",
            "Episodic Return: 21.0, Time Step 1838, Episode Number 77, Epsilon 0.9272152\n",
            "Episodic Return: 28.0, Time Step 1866, Episode Number 78, Epsilon 0.9261064\n",
            "Episodic Return: 29.0, Time Step 1895, Episode Number 79, Epsilon 0.924958\n",
            "Episodic Return: 23.0, Time Step 1918, Episode Number 80, Epsilon 0.9240472\n",
            "Episodic Return: 15.0, Time Step 1933, Episode Number 81, Epsilon 0.9234532\n",
            "Episodic Return: 21.0, Time Step 1954, Episode Number 82, Epsilon 0.9226216\n",
            "Episodic Return: 26.0, Time Step 1980, Episode Number 83, Epsilon 0.921592\n",
            "Episodic Return: 54.0, Time Step 2034, Episode Number 84, Epsilon 0.9194536\n",
            "Episodic Return: 30.0, Time Step 2064, Episode Number 85, Epsilon 0.9182656\n",
            "Episodic Return: 15.0, Time Step 2079, Episode Number 86, Epsilon 0.9176716\n",
            "Episodic Return: 17.0, Time Step 2096, Episode Number 87, Epsilon 0.9169984\n",
            "Episodic Return: 12.0, Time Step 2108, Episode Number 88, Epsilon 0.9165232\n",
            "Episodic Return: 12.0, Time Step 2120, Episode Number 89, Epsilon 0.916048\n",
            "Episodic Return: 12.0, Time Step 2132, Episode Number 90, Epsilon 0.9155728\n",
            "Episodic Return: 77.0, Time Step 2209, Episode Number 91, Epsilon 0.9125236\n",
            "Episodic Return: 10.0, Time Step 2219, Episode Number 92, Epsilon 0.9121276\n",
            "Episodic Return: 51.0, Time Step 2270, Episode Number 93, Epsilon 0.910108\n",
            "Episodic Return: 15.0, Time Step 2285, Episode Number 94, Epsilon 0.909514\n",
            "Episodic Return: 12.0, Time Step 2297, Episode Number 95, Epsilon 0.9090388\n",
            "Episodic Return: 14.0, Time Step 2311, Episode Number 96, Epsilon 0.9084844\n",
            "Episodic Return: 54.0, Time Step 2365, Episode Number 97, Epsilon 0.906346\n",
            "Episodic Return: 52.0, Time Step 2417, Episode Number 98, Epsilon 0.9042868\n",
            "Episodic Return: 27.0, Time Step 2444, Episode Number 99, Epsilon 0.9032176\n",
            "Episodic Return: 17.0, Time Step 2461, Episode Number 100, Epsilon 0.9025444\n",
            "Episodic Return: 32.0, Time Step 2493, Episode Number 101, Epsilon 0.9012772\n",
            "Episodic Return: 42.0, Time Step 2535, Episode Number 102, Epsilon 0.899614\n",
            "Episodic Return: 12.0, Time Step 2547, Episode Number 103, Epsilon 0.8991388\n",
            "Episodic Return: 23.0, Time Step 2570, Episode Number 104, Epsilon 0.898228\n",
            "Episodic Return: 35.0, Time Step 2605, Episode Number 105, Epsilon 0.896842\n",
            "Episodic Return: 23.0, Time Step 2628, Episode Number 106, Epsilon 0.8959312\n",
            "Episodic Return: 34.0, Time Step 2662, Episode Number 107, Epsilon 0.8945848\n",
            "Episodic Return: 14.0, Time Step 2676, Episode Number 108, Epsilon 0.8940304\n",
            "Episodic Return: 16.0, Time Step 2692, Episode Number 109, Epsilon 0.8933968\n",
            "Episodic Return: 21.0, Time Step 2713, Episode Number 110, Epsilon 0.8925652\n",
            "Episodic Return: 16.0, Time Step 2729, Episode Number 111, Epsilon 0.8919316\n",
            "Episodic Return: 14.0, Time Step 2743, Episode Number 112, Epsilon 0.8913772\n",
            "Episodic Return: 24.0, Time Step 2767, Episode Number 113, Epsilon 0.8904268\n",
            "Episodic Return: 30.0, Time Step 2797, Episode Number 114, Epsilon 0.8892388\n",
            "Episodic Return: 24.0, Time Step 2821, Episode Number 115, Epsilon 0.8882884\n",
            "Episodic Return: 33.0, Time Step 2854, Episode Number 116, Epsilon 0.8869816\n",
            "Episodic Return: 33.0, Time Step 2887, Episode Number 117, Epsilon 0.8856748\n",
            "Episodic Return: 19.0, Time Step 2906, Episode Number 118, Epsilon 0.8849224\n",
            "Episodic Return: 15.0, Time Step 2921, Episode Number 119, Epsilon 0.8843284\n",
            "Episodic Return: 17.0, Time Step 2938, Episode Number 120, Epsilon 0.8836552\n",
            "Episodic Return: 22.0, Time Step 2960, Episode Number 121, Epsilon 0.882784\n",
            "Episodic Return: 11.0, Time Step 2971, Episode Number 122, Epsilon 0.8823484\n",
            "Episodic Return: 51.0, Time Step 3022, Episode Number 123, Epsilon 0.8803288\n",
            "Episodic Return: 36.0, Time Step 3058, Episode Number 124, Epsilon 0.8789032\n",
            "Episodic Return: 43.0, Time Step 3101, Episode Number 125, Epsilon 0.8772004\n",
            "Episodic Return: 48.0, Time Step 3149, Episode Number 126, Epsilon 0.8752996\n",
            "Episodic Return: 32.0, Time Step 3181, Episode Number 127, Epsilon 0.8740323999999999\n",
            "Episodic Return: 15.0, Time Step 3196, Episode Number 128, Epsilon 0.8734384\n",
            "Episodic Return: 55.0, Time Step 3251, Episode Number 129, Epsilon 0.8712603999999999\n",
            "Episodic Return: 18.0, Time Step 3269, Episode Number 130, Epsilon 0.8705476\n",
            "Episodic Return: 42.0, Time Step 3311, Episode Number 131, Epsilon 0.8688844\n",
            "Episodic Return: 36.0, Time Step 3347, Episode Number 132, Epsilon 0.8674588\n",
            "Episodic Return: 11.0, Time Step 3358, Episode Number 133, Epsilon 0.8670232\n",
            "Episodic Return: 19.0, Time Step 3377, Episode Number 134, Epsilon 0.8662708\n",
            "Episodic Return: 58.0, Time Step 3435, Episode Number 135, Epsilon 0.863974\n",
            "Episodic Return: 20.0, Time Step 3455, Episode Number 136, Epsilon 0.863182\n",
            "Episodic Return: 41.0, Time Step 3496, Episode Number 137, Epsilon 0.8615584000000001\n",
            "Episodic Return: 16.0, Time Step 3512, Episode Number 138, Epsilon 0.8609248\n",
            "Episodic Return: 10.0, Time Step 3522, Episode Number 139, Epsilon 0.8605288\n",
            "Episodic Return: 51.0, Time Step 3573, Episode Number 140, Epsilon 0.8585092\n",
            "Episodic Return: 26.0, Time Step 3599, Episode Number 141, Epsilon 0.8574796\n",
            "Episodic Return: 36.0, Time Step 3635, Episode Number 142, Epsilon 0.856054\n",
            "Episodic Return: 15.0, Time Step 3650, Episode Number 143, Epsilon 0.85546\n",
            "Episodic Return: 17.0, Time Step 3667, Episode Number 144, Epsilon 0.8547868\n",
            "Episodic Return: 40.0, Time Step 3707, Episode Number 145, Epsilon 0.8532028\n",
            "Episodic Return: 9.0, Time Step 3716, Episode Number 146, Epsilon 0.8528464\n",
            "Episodic Return: 15.0, Time Step 3731, Episode Number 147, Epsilon 0.8522524\n",
            "Episodic Return: 46.0, Time Step 3777, Episode Number 148, Epsilon 0.8504308\n",
            "Episodic Return: 47.0, Time Step 3824, Episode Number 149, Epsilon 0.8485696\n",
            "Episodic Return: 37.0, Time Step 3861, Episode Number 150, Epsilon 0.8471044\n",
            "Episodic Return: 64.0, Time Step 3925, Episode Number 151, Epsilon 0.84457\n",
            "Episodic Return: 19.0, Time Step 3944, Episode Number 152, Epsilon 0.8438176\n",
            "Episodic Return: 57.0, Time Step 4001, Episode Number 153, Epsilon 0.8415604\n",
            "Episodic Return: 11.0, Time Step 4012, Episode Number 154, Epsilon 0.8411248\n",
            "Episodic Return: 22.0, Time Step 4034, Episode Number 155, Epsilon 0.8402536\n",
            "Episodic Return: 21.0, Time Step 4055, Episode Number 156, Epsilon 0.839422\n",
            "Episodic Return: 14.0, Time Step 4069, Episode Number 157, Epsilon 0.8388675999999999\n",
            "Episodic Return: 14.0, Time Step 4083, Episode Number 158, Epsilon 0.8383132\n",
            "Episodic Return: 11.0, Time Step 4094, Episode Number 159, Epsilon 0.8378776\n",
            "Episodic Return: 21.0, Time Step 4115, Episode Number 160, Epsilon 0.837046\n",
            "Episodic Return: 38.0, Time Step 4153, Episode Number 161, Epsilon 0.8355412\n",
            "Episodic Return: 33.0, Time Step 4186, Episode Number 162, Epsilon 0.8342343999999999\n",
            "Episodic Return: 42.0, Time Step 4228, Episode Number 163, Epsilon 0.8325712000000001\n",
            "Episodic Return: 16.0, Time Step 4244, Episode Number 164, Epsilon 0.8319376\n",
            "Episodic Return: 21.0, Time Step 4265, Episode Number 165, Epsilon 0.831106\n",
            "Episodic Return: 68.0, Time Step 4333, Episode Number 166, Epsilon 0.8284132\n",
            "Episodic Return: 27.0, Time Step 4360, Episode Number 167, Epsilon 0.827344\n",
            "Episodic Return: 51.0, Time Step 4411, Episode Number 168, Epsilon 0.8253244\n",
            "Episodic Return: 20.0, Time Step 4431, Episode Number 169, Epsilon 0.8245324\n",
            "Episodic Return: 16.0, Time Step 4447, Episode Number 170, Epsilon 0.8238988\n",
            "Episodic Return: 15.0, Time Step 4462, Episode Number 171, Epsilon 0.8233048000000001\n",
            "Episodic Return: 16.0, Time Step 4478, Episode Number 172, Epsilon 0.8226712\n",
            "Episodic Return: 89.0, Time Step 4567, Episode Number 173, Epsilon 0.8191468\n",
            "Episodic Return: 57.0, Time Step 4624, Episode Number 174, Epsilon 0.8168896\n",
            "Episodic Return: 17.0, Time Step 4641, Episode Number 175, Epsilon 0.8162164000000001\n",
            "Episodic Return: 46.0, Time Step 4687, Episode Number 176, Epsilon 0.8143948\n",
            "Episodic Return: 19.0, Time Step 4706, Episode Number 177, Epsilon 0.8136424\n",
            "Episodic Return: 19.0, Time Step 4725, Episode Number 178, Epsilon 0.81289\n",
            "Episodic Return: 42.0, Time Step 4767, Episode Number 179, Epsilon 0.8112268\n",
            "Episodic Return: 43.0, Time Step 4810, Episode Number 180, Epsilon 0.809524\n",
            "Episodic Return: 22.0, Time Step 4832, Episode Number 181, Epsilon 0.8086528\n",
            "Episodic Return: 49.0, Time Step 4881, Episode Number 182, Epsilon 0.8067124\n",
            "Episodic Return: 44.0, Time Step 4925, Episode Number 183, Epsilon 0.80497\n",
            "Episodic Return: 19.0, Time Step 4944, Episode Number 184, Epsilon 0.8042176\n",
            "Episodic Return: 18.0, Time Step 4962, Episode Number 185, Epsilon 0.8035048\n",
            "Episodic Return: 25.0, Time Step 4987, Episode Number 186, Epsilon 0.8025148\n",
            "Episodic Return: 53.0, Time Step 5040, Episode Number 187, Epsilon 0.800416\n",
            "Episodic Return: 53.0, Time Step 5093, Episode Number 188, Epsilon 0.7983172000000001\n",
            "Episodic Return: 13.0, Time Step 5106, Episode Number 189, Epsilon 0.7978024\n",
            "Episodic Return: 76.0, Time Step 5182, Episode Number 190, Epsilon 0.7947928\n",
            "Episodic Return: 16.0, Time Step 5198, Episode Number 191, Epsilon 0.7941592\n",
            "Episodic Return: 48.0, Time Step 5246, Episode Number 192, Epsilon 0.7922584\n",
            "Episodic Return: 24.0, Time Step 5270, Episode Number 193, Epsilon 0.791308\n",
            "Episodic Return: 17.0, Time Step 5287, Episode Number 194, Epsilon 0.7906348\n",
            "Episodic Return: 15.0, Time Step 5302, Episode Number 195, Epsilon 0.7900408\n",
            "Episodic Return: 51.0, Time Step 5353, Episode Number 196, Epsilon 0.7880212\n",
            "Episodic Return: 22.0, Time Step 5375, Episode Number 197, Epsilon 0.78715\n",
            "Episodic Return: 61.0, Time Step 5436, Episode Number 198, Epsilon 0.7847344\n",
            "Episodic Return: 81.0, Time Step 5517, Episode Number 199, Epsilon 0.7815268\n",
            "Episodic Return: 56.0, Time Step 5573, Episode Number 200, Epsilon 0.7793092\n",
            "Episodic Return: 43.0, Time Step 5616, Episode Number 201, Epsilon 0.7776064\n",
            "Episodic Return: 21.0, Time Step 5637, Episode Number 202, Epsilon 0.7767748\n",
            "Episodic Return: 72.0, Time Step 5709, Episode Number 203, Epsilon 0.7739236\n",
            "Episodic Return: 56.0, Time Step 5765, Episode Number 204, Epsilon 0.771706\n",
            "Episodic Return: 93.0, Time Step 5858, Episode Number 205, Epsilon 0.7680232\n",
            "Episodic Return: 14.0, Time Step 5872, Episode Number 206, Epsilon 0.7674688000000001\n",
            "Episodic Return: 13.0, Time Step 5885, Episode Number 207, Epsilon 0.766954\n",
            "Episodic Return: 16.0, Time Step 5901, Episode Number 208, Epsilon 0.7663204\n",
            "Episodic Return: 14.0, Time Step 5915, Episode Number 209, Epsilon 0.765766\n",
            "Episodic Return: 22.0, Time Step 5937, Episode Number 210, Epsilon 0.7648948\n",
            "Episodic Return: 22.0, Time Step 5959, Episode Number 211, Epsilon 0.7640236\n",
            "Episodic Return: 57.0, Time Step 6016, Episode Number 212, Epsilon 0.7617664\n",
            "Episodic Return: 23.0, Time Step 6039, Episode Number 213, Epsilon 0.7608556\n",
            "Episodic Return: 11.0, Time Step 6050, Episode Number 214, Epsilon 0.76042\n",
            "Episodic Return: 21.0, Time Step 6071, Episode Number 215, Epsilon 0.7595883999999999\n",
            "Episodic Return: 18.0, Time Step 6089, Episode Number 216, Epsilon 0.7588756\n",
            "Episodic Return: 15.0, Time Step 6104, Episode Number 217, Epsilon 0.7582816\n",
            "Episodic Return: 21.0, Time Step 6125, Episode Number 218, Epsilon 0.75745\n",
            "Episodic Return: 14.0, Time Step 6139, Episode Number 219, Epsilon 0.7568956\n",
            "Episodic Return: 34.0, Time Step 6173, Episode Number 220, Epsilon 0.7555492\n",
            "Episodic Return: 27.0, Time Step 6200, Episode Number 221, Epsilon 0.75448\n",
            "Episodic Return: 45.0, Time Step 6245, Episode Number 222, Epsilon 0.752698\n",
            "Episodic Return: 20.0, Time Step 6265, Episode Number 223, Epsilon 0.751906\n",
            "Episodic Return: 21.0, Time Step 6286, Episode Number 224, Epsilon 0.7510744\n",
            "Episodic Return: 53.0, Time Step 6339, Episode Number 225, Epsilon 0.7489756000000001\n",
            "Episodic Return: 32.0, Time Step 6371, Episode Number 226, Epsilon 0.7477084\n",
            "Episodic Return: 23.0, Time Step 6394, Episode Number 227, Epsilon 0.7467976000000001\n",
            "Episodic Return: 41.0, Time Step 6435, Episode Number 228, Epsilon 0.745174\n",
            "Episodic Return: 17.0, Time Step 6452, Episode Number 229, Epsilon 0.7445008\n",
            "Episodic Return: 35.0, Time Step 6487, Episode Number 230, Epsilon 0.7431148000000001\n",
            "Episodic Return: 30.0, Time Step 6517, Episode Number 231, Epsilon 0.7419268\n",
            "Episodic Return: 59.0, Time Step 6576, Episode Number 232, Epsilon 0.7395904\n",
            "Episodic Return: 15.0, Time Step 6591, Episode Number 233, Epsilon 0.7389964\n",
            "Episodic Return: 25.0, Time Step 6616, Episode Number 234, Epsilon 0.7380064\n",
            "Episodic Return: 95.0, Time Step 6711, Episode Number 235, Epsilon 0.7342444\n",
            "Episodic Return: 50.0, Time Step 6761, Episode Number 236, Epsilon 0.7322644\n",
            "Episodic Return: 12.0, Time Step 6773, Episode Number 237, Epsilon 0.7317891999999999\n",
            "Episodic Return: 10.0, Time Step 6783, Episode Number 238, Epsilon 0.7313932000000001\n",
            "Episodic Return: 19.0, Time Step 6802, Episode Number 239, Epsilon 0.7306408\n",
            "Episodic Return: 17.0, Time Step 6819, Episode Number 240, Epsilon 0.7299675999999999\n",
            "Episodic Return: 40.0, Time Step 6859, Episode Number 241, Epsilon 0.7283836\n",
            "Episodic Return: 143.0, Time Step 7002, Episode Number 242, Epsilon 0.7227208\n",
            "Episodic Return: 13.0, Time Step 7015, Episode Number 243, Epsilon 0.722206\n",
            "Episodic Return: 31.0, Time Step 7046, Episode Number 244, Epsilon 0.7209784\n",
            "Episodic Return: 24.0, Time Step 7070, Episode Number 245, Epsilon 0.720028\n",
            "Episodic Return: 47.0, Time Step 7117, Episode Number 246, Epsilon 0.7181668\n",
            "Episodic Return: 47.0, Time Step 7164, Episode Number 247, Epsilon 0.7163056\n",
            "Episodic Return: 31.0, Time Step 7195, Episode Number 248, Epsilon 0.715078\n",
            "Episodic Return: 16.0, Time Step 7211, Episode Number 249, Epsilon 0.7144444\n",
            "Episodic Return: 51.0, Time Step 7262, Episode Number 250, Epsilon 0.7124248\n",
            "Episodic Return: 40.0, Time Step 7302, Episode Number 251, Epsilon 0.7108407999999999\n",
            "Episodic Return: 76.0, Time Step 7378, Episode Number 252, Epsilon 0.7078312\n",
            "Episodic Return: 60.0, Time Step 7438, Episode Number 253, Epsilon 0.7054552000000001\n",
            "Episodic Return: 29.0, Time Step 7467, Episode Number 254, Epsilon 0.7043068\n",
            "Episodic Return: 51.0, Time Step 7518, Episode Number 255, Epsilon 0.7022872\n",
            "Episodic Return: 78.0, Time Step 7596, Episode Number 256, Epsilon 0.6991984\n",
            "Episodic Return: 25.0, Time Step 7621, Episode Number 257, Epsilon 0.6982084\n",
            "Episodic Return: 117.0, Time Step 7738, Episode Number 258, Epsilon 0.6935752\n",
            "Episodic Return: 25.0, Time Step 7763, Episode Number 259, Epsilon 0.6925852\n",
            "Episodic Return: 64.0, Time Step 7827, Episode Number 260, Epsilon 0.6900508000000001\n",
            "Episodic Return: 148.0, Time Step 7975, Episode Number 261, Epsilon 0.6841900000000001\n",
            "Episodic Return: 111.0, Time Step 8086, Episode Number 262, Epsilon 0.6797944\n",
            "Episodic Return: 37.0, Time Step 8123, Episode Number 263, Epsilon 0.6783292000000001\n",
            "Episodic Return: 49.0, Time Step 8172, Episode Number 264, Epsilon 0.6763888\n",
            "Episodic Return: 46.0, Time Step 8218, Episode Number 265, Epsilon 0.6745672\n",
            "Episodic Return: 88.0, Time Step 8306, Episode Number 266, Epsilon 0.6710824\n",
            "Episodic Return: 55.0, Time Step 8361, Episode Number 267, Epsilon 0.6689044\n",
            "Episodic Return: 22.0, Time Step 8383, Episode Number 268, Epsilon 0.6680332\n",
            "Episodic Return: 33.0, Time Step 8416, Episode Number 269, Epsilon 0.6667263999999999\n",
            "Episodic Return: 64.0, Time Step 8480, Episode Number 270, Epsilon 0.664192\n",
            "Episodic Return: 16.0, Time Step 8496, Episode Number 271, Epsilon 0.6635584\n",
            "Episodic Return: 62.0, Time Step 8558, Episode Number 272, Epsilon 0.6611032\n",
            "Episodic Return: 26.0, Time Step 8584, Episode Number 273, Epsilon 0.6600736\n",
            "Episodic Return: 104.0, Time Step 8688, Episode Number 274, Epsilon 0.6559552\n",
            "Episodic Return: 33.0, Time Step 8721, Episode Number 275, Epsilon 0.6546484\n",
            "Episodic Return: 53.0, Time Step 8774, Episode Number 276, Epsilon 0.6525496\n",
            "Episodic Return: 112.0, Time Step 8886, Episode Number 277, Epsilon 0.6481144\n",
            "Episodic Return: 32.0, Time Step 8918, Episode Number 278, Epsilon 0.6468472000000001\n",
            "Episodic Return: 35.0, Time Step 8953, Episode Number 279, Epsilon 0.6454612\n",
            "Episodic Return: 59.0, Time Step 9012, Episode Number 280, Epsilon 0.6431248\n",
            "Episodic Return: 49.0, Time Step 9061, Episode Number 281, Epsilon 0.6411844\n",
            "Episodic Return: 104.0, Time Step 9165, Episode Number 282, Epsilon 0.637066\n",
            "Episodic Return: 31.0, Time Step 9196, Episode Number 283, Epsilon 0.6358383999999999\n",
            "Episodic Return: 64.0, Time Step 9260, Episode Number 284, Epsilon 0.633304\n",
            "Episodic Return: 21.0, Time Step 9281, Episode Number 285, Epsilon 0.6324723999999999\n",
            "Episodic Return: 75.0, Time Step 9356, Episode Number 286, Epsilon 0.6295024\n",
            "Episodic Return: 58.0, Time Step 9414, Episode Number 287, Epsilon 0.6272055999999999\n",
            "Episodic Return: 105.0, Time Step 9519, Episode Number 288, Epsilon 0.6230476\n",
            "Episodic Return: 22.0, Time Step 9541, Episode Number 289, Epsilon 0.6221764000000001\n",
            "Episodic Return: 120.0, Time Step 9661, Episode Number 290, Epsilon 0.6174244\n",
            "Episodic Return: 143.0, Time Step 9804, Episode Number 291, Epsilon 0.6117616\n",
            "Episodic Return: 124.0, Time Step 9928, Episode Number 292, Epsilon 0.6068511999999999\n",
            "Episodic Return: 31.0, Time Step 9959, Episode Number 293, Epsilon 0.6056235999999999\n",
            "Episodic Return: 20.0, Time Step 9979, Episode Number 294, Epsilon 0.6048316\n",
            "Episodic Return: 10.0, Time Step 9989, Episode Number 295, Epsilon 0.6044356\n",
            "Episodic Return: 18.0, Time Step 10007, Episode Number 296, Epsilon 0.6037228\n",
            "Episodic Return: 156.0, Time Step 10163, Episode Number 297, Epsilon 0.5975452\n",
            "Episodic Return: 106.0, Time Step 10269, Episode Number 298, Epsilon 0.5933476\n",
            "Episodic Return: 23.0, Time Step 10292, Episode Number 299, Epsilon 0.5924368\n",
            "Episodic Return: 16.0, Time Step 10308, Episode Number 300, Epsilon 0.5918032\n",
            "Episodic Return: 65.0, Time Step 10373, Episode Number 301, Epsilon 0.5892292\n",
            "Episodic Return: 30.0, Time Step 10403, Episode Number 302, Epsilon 0.5880411999999999\n",
            "Episodic Return: 36.0, Time Step 10439, Episode Number 303, Epsilon 0.5866156\n",
            "Episodic Return: 12.0, Time Step 10451, Episode Number 304, Epsilon 0.5861404\n",
            "Episodic Return: 277.0, Time Step 10728, Episode Number 305, Epsilon 0.5751712\n",
            "Episodic Return: 17.0, Time Step 10745, Episode Number 306, Epsilon 0.574498\n",
            "Episodic Return: 33.0, Time Step 10778, Episode Number 307, Epsilon 0.5731912\n",
            "Episodic Return: 76.0, Time Step 10854, Episode Number 308, Epsilon 0.5701816\n",
            "Episodic Return: 12.0, Time Step 10866, Episode Number 309, Epsilon 0.5697064000000001\n",
            "Episodic Return: 47.0, Time Step 10913, Episode Number 310, Epsilon 0.5678452\n",
            "Episodic Return: 97.0, Time Step 11010, Episode Number 311, Epsilon 0.564004\n",
            "Episodic Return: 38.0, Time Step 11048, Episode Number 312, Epsilon 0.5624992\n",
            "Episodic Return: 40.0, Time Step 11088, Episode Number 313, Epsilon 0.5609152\n",
            "Episodic Return: 15.0, Time Step 11103, Episode Number 314, Epsilon 0.5603212\n",
            "Episodic Return: 55.0, Time Step 11158, Episode Number 315, Epsilon 0.5581432\n",
            "Episodic Return: 117.0, Time Step 11275, Episode Number 316, Epsilon 0.55351\n",
            "Episodic Return: 32.0, Time Step 11307, Episode Number 317, Epsilon 0.5522427999999999\n",
            "Episodic Return: 75.0, Time Step 11382, Episode Number 318, Epsilon 0.5492728\n",
            "Episodic Return: 48.0, Time Step 11430, Episode Number 319, Epsilon 0.547372\n",
            "Episodic Return: 56.0, Time Step 11486, Episode Number 320, Epsilon 0.5451543999999999\n",
            "Episodic Return: 15.0, Time Step 11501, Episode Number 321, Epsilon 0.5445604\n",
            "Episodic Return: 96.0, Time Step 11597, Episode Number 322, Epsilon 0.5407588\n",
            "Episodic Return: 89.0, Time Step 11686, Episode Number 323, Epsilon 0.5372344\n",
            "Episodic Return: 23.0, Time Step 11709, Episode Number 324, Epsilon 0.5363236\n",
            "Episodic Return: 260.0, Time Step 11969, Episode Number 325, Epsilon 0.5260275999999999\n",
            "Episodic Return: 76.0, Time Step 12045, Episode Number 326, Epsilon 0.523018\n",
            "Episodic Return: 73.0, Time Step 12118, Episode Number 327, Epsilon 0.5201272\n",
            "Episodic Return: 15.0, Time Step 12133, Episode Number 328, Epsilon 0.5195331999999999\n",
            "Episodic Return: 71.0, Time Step 12204, Episode Number 329, Epsilon 0.5167216\n",
            "Episodic Return: 117.0, Time Step 12321, Episode Number 330, Epsilon 0.5120884\n",
            "Episodic Return: 57.0, Time Step 12378, Episode Number 331, Epsilon 0.5098312\n",
            "Episodic Return: 44.0, Time Step 12422, Episode Number 332, Epsilon 0.5080888\n",
            "Episodic Return: 31.0, Time Step 12453, Episode Number 333, Epsilon 0.5068612\n",
            "Episodic Return: 22.0, Time Step 12475, Episode Number 334, Epsilon 0.5059899999999999\n",
            "Episodic Return: 22.0, Time Step 12497, Episode Number 335, Epsilon 0.5051188\n",
            "Episodic Return: 96.0, Time Step 12593, Episode Number 336, Epsilon 0.5013172\n",
            "Episodic Return: 53.0, Time Step 12646, Episode Number 337, Epsilon 0.49921839999999995\n",
            "Episodic Return: 12.0, Time Step 12658, Episode Number 338, Epsilon 0.49874319999999994\n",
            "Episodic Return: 81.0, Time Step 12739, Episode Number 339, Epsilon 0.49553559999999996\n",
            "Episodic Return: 91.0, Time Step 12830, Episode Number 340, Epsilon 0.49193200000000004\n",
            "Episodic Return: 138.0, Time Step 12968, Episode Number 341, Epsilon 0.4864672\n",
            "Episodic Return: 31.0, Time Step 12999, Episode Number 342, Epsilon 0.4852396\n",
            "Episodic Return: 80.0, Time Step 13079, Episode Number 343, Epsilon 0.48207160000000004\n",
            "Episodic Return: 78.0, Time Step 13157, Episode Number 344, Epsilon 0.47898280000000004\n",
            "Episodic Return: 147.0, Time Step 13304, Episode Number 345, Epsilon 0.47316159999999996\n",
            "Episodic Return: 14.0, Time Step 13318, Episode Number 346, Epsilon 0.4726072\n",
            "Episodic Return: 36.0, Time Step 13354, Episode Number 347, Epsilon 0.4711816\n",
            "Episodic Return: 29.0, Time Step 13383, Episode Number 348, Epsilon 0.47003320000000004\n",
            "Episodic Return: 21.0, Time Step 13404, Episode Number 349, Epsilon 0.4692016\n",
            "Episodic Return: 18.0, Time Step 13422, Episode Number 350, Epsilon 0.46848880000000004\n",
            "Episodic Return: 16.0, Time Step 13438, Episode Number 351, Epsilon 0.4678552\n",
            "Episodic Return: 96.0, Time Step 13534, Episode Number 352, Epsilon 0.46405359999999996\n",
            "Episodic Return: 39.0, Time Step 13573, Episode Number 353, Epsilon 0.46250919999999995\n",
            "Episodic Return: 16.0, Time Step 13589, Episode Number 354, Epsilon 0.46187559999999994\n",
            "Episodic Return: 13.0, Time Step 13602, Episode Number 355, Epsilon 0.4613608\n",
            "Episodic Return: 29.0, Time Step 13631, Episode Number 356, Epsilon 0.46021239999999997\n",
            "Episodic Return: 103.0, Time Step 13734, Episode Number 357, Epsilon 0.45613360000000003\n",
            "Episodic Return: 117.0, Time Step 13851, Episode Number 358, Epsilon 0.4515004\n",
            "Episodic Return: 111.0, Time Step 13962, Episode Number 359, Epsilon 0.44710479999999997\n",
            "Episodic Return: 57.0, Time Step 14019, Episode Number 360, Epsilon 0.4448476\n",
            "Episodic Return: 106.0, Time Step 14125, Episode Number 361, Epsilon 0.44065\n",
            "Episodic Return: 67.0, Time Step 14192, Episode Number 362, Epsilon 0.43799679999999996\n",
            "Episodic Return: 98.0, Time Step 14290, Episode Number 363, Epsilon 0.43411599999999995\n",
            "Episodic Return: 115.0, Time Step 14405, Episode Number 364, Epsilon 0.429562\n",
            "Episodic Return: 113.0, Time Step 14518, Episode Number 365, Epsilon 0.4250872\n",
            "Episodic Return: 27.0, Time Step 14545, Episode Number 366, Epsilon 0.424018\n",
            "Episodic Return: 120.0, Time Step 14665, Episode Number 367, Epsilon 0.419266\n",
            "Episodic Return: 120.0, Time Step 14785, Episode Number 368, Epsilon 0.41451400000000005\n",
            "Episodic Return: 89.0, Time Step 14874, Episode Number 369, Epsilon 0.41098959999999995\n",
            "Episodic Return: 37.0, Time Step 14911, Episode Number 370, Epsilon 0.4095244\n",
            "Episodic Return: 130.0, Time Step 15041, Episode Number 371, Epsilon 0.40437639999999997\n",
            "Episodic Return: 1660.0, Time Step 16701, Episode Number 372, Epsilon 0.33864039999999995\n",
            "Episodic Return: 71.0, Time Step 16772, Episode Number 373, Epsilon 0.33582880000000004\n",
            "Episodic Return: 192.0, Time Step 16964, Episode Number 374, Epsilon 0.3282256\n",
            "Episodic Return: 178.0, Time Step 17142, Episode Number 375, Epsilon 0.32117680000000004\n",
            "Episodic Return: 138.0, Time Step 17280, Episode Number 376, Epsilon 0.315712\n",
            "Episodic Return: 182.0, Time Step 17462, Episode Number 377, Epsilon 0.3085048\n",
            "Episodic Return: 360.0, Time Step 17822, Episode Number 378, Epsilon 0.2942488\n",
            "Episodic Return: 320.0, Time Step 18142, Episode Number 379, Epsilon 0.28157679999999996\n",
            "Episodic Return: 2694.0, Time Step 20836, Episode Number 380, Epsilon 0.1748944\n",
            "Episodic Return: 176.0, Time Step 21012, Episode Number 381, Epsilon 0.16792479999999999\n",
            "Episodic Return: 176.0, Time Step 21188, Episode Number 382, Epsilon 0.16095519999999996\n",
            "Episodic Return: 209.0, Time Step 21397, Episode Number 383, Epsilon 0.1526788\n",
            "Episodic Return: 570.0, Time Step 21967, Episode Number 384, Epsilon 0.13010679999999997\n",
            "Episodic Return: 228.0, Time Step 22195, Episode Number 385, Epsilon 0.12107800000000002\n",
            "Episodic Return: 271.0, Time Step 22466, Episode Number 386, Epsilon 0.11034639999999996\n",
            "Episodic Return: 773.0, Time Step 23239, Episode Number 387, Epsilon 0.07973560000000002\n",
            "Episodic Return: 151.0, Time Step 23390, Episode Number 388, Epsilon 0.07375600000000004\n",
            "Episodic Return: 266.0, Time Step 23656, Episode Number 389, Epsilon 0.06322240000000001\n",
            "Episodic Return: 816.0, Time Step 24472, Episode Number 390, Epsilon 0.03090879999999996\n",
            "Episodic Return: 164.0, Time Step 24636, Episode Number 391, Epsilon 0.024414399999999947\n",
            "Episodic Return: 174.0, Time Step 24810, Episode Number 392, Epsilon 0.017523999999999984\n",
            "Episodic Return: 873.0, Time Step 25683, Episode Number 393, Epsilon 0.01\n",
            "Episodic Return: 217.0, Time Step 25900, Episode Number 394, Epsilon 0.01\n",
            "Episodic Return: 138.0, Time Step 26038, Episode Number 395, Epsilon 0.01\n",
            "Episodic Return: 196.0, Time Step 26234, Episode Number 396, Epsilon 0.01\n",
            "Episodic Return: 925.0, Time Step 27159, Episode Number 397, Epsilon 0.01\n",
            "Episodic Return: 1054.0, Time Step 28213, Episode Number 398, Epsilon 0.01\n",
            "Episodic Return: 264.0, Time Step 28477, Episode Number 399, Epsilon 0.01\n",
            "Episodic Return: 234.0, Time Step 28711, Episode Number 400, Epsilon 0.01\n",
            "Episodic Return: 597.0, Time Step 29308, Episode Number 401, Epsilon 0.01\n",
            "Episodic Return: 307.0, Time Step 29615, Episode Number 402, Epsilon 0.01\n",
            "Episodic Return: 2000.0, Time Step 31615, Episode Number 403, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 41615, Episode Number 404, Epsilon 0.01\n",
            "Episodic Return: 1752.0, Time Step 43367, Episode Number 405, Epsilon 0.01\n",
            "Episodic Return: 188.0, Time Step 43555, Episode Number 406, Epsilon 0.01\n",
            "Episodic Return: 277.0, Time Step 43832, Episode Number 407, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 53832, Episode Number 408, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 63832, Episode Number 409, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 73832, Episode Number 410, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 83832, Episode Number 411, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 93832, Episode Number 412, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 103832, Episode Number 413, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 113832, Episode Number 414, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 123832, Episode Number 415, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 133832, Episode Number 416, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 143832, Episode Number 417, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 153832, Episode Number 418, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 163832, Episode Number 419, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 173832, Episode Number 420, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 183832, Episode Number 421, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 193832, Episode Number 422, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 203832, Episode Number 423, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 213832, Episode Number 424, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 223832, Episode Number 425, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 233832, Episode Number 426, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 243832, Episode Number 427, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 253832, Episode Number 428, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 263832, Episode Number 429, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 273832, Episode Number 430, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 283832, Episode Number 431, Epsilon 0.01\n",
            "Episodic Return: 10000.0, Time Step 293832, Episode Number 432, Epsilon 0.01\n"
          ]
        }
      ],
      "source": [
        "agent_env_interaction(\n",
        "    env_name='CartPole-v1',\n",
        "                      seed=0,\n",
        "                      lr=1.0,\n",
        "                      gamma=0.99,\n",
        "                      lamda=0.8,\n",
        "                      total_steps=500_000,\n",
        "                      epsilon_target=0.01,\n",
        "                      epsilon_start=1.0,\n",
        "                      exploration_fraction=0.05,\n",
        "                      kappa_value=2.0,\n",
        "                      debug=True,\n",
        "                      overshooting_info=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzyPm2yctrJK"
      },
      "source": [
        "## 10. Load agents and continue the interaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XraeCqDesf9Y"
      },
      "source": [
        "Let's save the actor and critic networks along with the observations mean/std and reward trace std. We also save the frames to visualize the behavior next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJfg7AxKsaR3"
      },
      "outputs": [],
      "source": [
        "frames = agent_env_interaction(env_name='PongNoFrameskip-v4',\n",
        "                              seed=0,\n",
        "                              lr=1.0,\n",
        "                              gamma=0.99,\n",
        "                              lamda=0.8,\n",
        "                              total_steps=2000,\n",
        "                              epsilon_target=0.01,\n",
        "                              epsilon_start=0.01,\n",
        "                              exploration_fraction=0.05,\n",
        "                              kappa_value=2.0,\n",
        "                              debug=True,\n",
        "                              save_agent=False,\n",
        "                              update_agent=False,\n",
        "                              load_agent=\"stream_q_PongNoFrameskip-v4\",\n",
        "                              render=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPq8PTtqNKFY"
      },
      "source": [
        "### 10. See how the behavior looks like:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfbV5ruJzC0Q"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "# Save as a GIF\n",
        "gif_path = '/content/PongNoFrameskip_v4.gif'\n",
        "imageio.mimsave(gif_path, frames, format='GIF', fps=30)\n",
        "# Display the GIF\n",
        "from IPython.display import Image\n",
        "Image(gif_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
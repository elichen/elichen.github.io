<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Stick Balancing RL</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <div id="app">
        <h1>Stick Balancing Reinforcement Learning</h1>
        <canvas id="stickCanvas" width="800" height="400"></canvas>
        <div id="controls">
            <button id="toggleMode">Switch to Testing</button>
        </div>
        <div id="stats">
            <p>Mode: <span id="currentMode">Training</span></p>
            <p>Episode: <span id="episodeCount">0</span></p>
        </div>
        <canvas id="metricsChart" width="800" height="400"></canvas> <!-- Added metrics chart -->

        <div id="explanation"> <!-- Added explanation section -->
            <h2>Learning Algorithm</h2>
            <p>
                The reinforcement learning agent uses a Policy Gradient method to directly learn the optimal policy for balancing the stick. Unlike value-based methods (like DQN), policy gradients learn by directly adjusting the probabilities of taking actions in each state. The network outputs action probabilities and samples actions from this distribution during training, providing natural exploration without needing an explicit exploration parameter.
            </p>
            
            <h2>Metrics Explanation</h2>
            <p>
                <strong>Episode Reward:</strong> Represents the cumulative reward obtained in a single episode. A higher reward indicates better performance in balancing the stick. The reward structure encourages keeping the pole upright (+1 for each step, scaled by pole angle) while heavily penalizing failures (-10).
            </p>
            
            <h2>Learning Process</h2>
            <p>
                The agent learns by:
                <br>1. Sampling actions based on their predicted probabilities
                <br>2. Collecting entire episodes of experience
                <br>3. Computing returns (discounted sum of rewards) for each action
                <br>4. Adjusting action probabilities: increasing probabilities of actions that led to good returns, and decreasing probabilities of actions that led to poor returns
                <br><br>
                This process is similar to learning to ride a bike - try actions with certain probabilities, do successful ones more often, and unsuccessful ones less often, until converging to good actions.
            </p>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"> </script>
    <script src="environment.js"></script>
    <script src="agent.js"></script>
    <script src="visualization.js"></script>
    <script src="main.js"></script>
</body>
</html>
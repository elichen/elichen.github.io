<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Snake AI - Reinforcement Learning</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.11.0/dist/tf.min.js"></script>
</head>
<body>
    <div class="container">
        <h1>Snake AI</h1>
        <p class="subtitle">Trained with Proximal Policy Optimization</p>

        <div class="canvas-wrapper">
            <canvas id="gameCanvas" width="600" height="600"></canvas>
        </div>

        <div class="stats">
            <div class="stat-item">
                <span class="stat-value" id="score">0</span>
                <span class="stat-label">Current Score</span>
            </div>
            <div class="stat-item">
                <span class="stat-value" id="episode">1</span>
                <span class="stat-label">Episode</span>
            </div>
            <div class="stat-item">
                <span class="stat-value" id="foodEaten">0</span>
                <span class="stat-label">Food Eaten</span>
            </div>
        </div>

        <p id="modelStatus"></p>

        <div class="description">
            <p>This agent learned to play Snake through 37.1 million timesteps of reinforcement learning on a 20×20 grid. The model achieves a best score of 71 cells (17.8% grid fill) and an average of 32 cells across continuous play.</p>

            <p>Training was conducted directly on the target grid size without curriculum learning, using 8 parallel environments and PPO with entropy regularization to encourage exploration.</p>
        </div>

        <div class="metrics">
            <div class="metric-card">
                <div class="label">Best Performance</div>
                <div class="value">71 cells</div>
            </div>
            <div class="metric-card">
                <div class="label">Training Steps</div>
                <div class="value">37.1M</div>
            </div>
            <div class="metric-card">
                <div class="label">Grid Fill</div>
                <div class="value">17.8%</div>
            </div>
            <div class="metric-card">
                <div class="label">Architecture</div>
                <div class="value">MLP</div>
            </div>
        </div>

        <div class="technical-details">
            <h3>Architecture</h3>
            <ul>
                <li>Algorithm: Proximal Policy Optimization (PPO)</li>
                <li>State space: 24 features (direction, food location, danger detection, connectivity)</li>
                <li>Network: 2-layer MLP (64→64→4) with 6,020 parameters</li>
                <li>Training: 8 parallel environments, batch size 256, learning rate 1e-4</li>
                <li>Model size: 24KB (TensorFlow.js)</li>
            </ul>
        </div>

        <div class="description">
            <p>The agent demonstrates learned strategies including food-seeking behavior, obstacle avoidance, and basic space management. Performance continues to improve with extended training, approaching the theoretical ceiling for feature-based representations.</p>
        </div>
    </div>

    <script src="snake.js"></script>
    <script src="tf-model-pretrained.js"></script>
    <script src="agent-pretrained.js"></script>
    <script src="script-pretrained.js"></script>
</body>
</html>
